# å¤§æ¨¡å‹æœåŠ¡é›†æˆè®¾è®¡

## ğŸ¯ æ ¸å¿ƒä»·å€¼

å°†GPT-4ã€Claudeç­‰å…ˆè¿›å¤§æ¨¡å‹æœåŠ¡é›†æˆåˆ°ç¯å¢ƒéŸ³æ··åˆç³»ç»Ÿä¸­ï¼Œæä¾›è¶…å¼ºçš„åœºæ™¯ç†è§£ã€ä¸Šä¸‹æ–‡åˆ†æå’Œåˆ›æ„éŸ³æ•ˆå»ºè®®èƒ½åŠ›ã€‚

## ğŸ§  å¤§æ¨¡å‹åº”ç”¨åœºæ™¯

### 1. æ™ºèƒ½åœºæ™¯åˆ†æå¢å¼º
```python
class LLMSceneAnalyzer:
    def __init__(self):
        self.openai_client = OpenAI()  # æˆ–å…¶ä»–å¤§æ¨¡å‹æœåŠ¡
        self.local_fallback = OllamaClient()  # æœ¬åœ°æ¨¡å‹å…œåº•
    
    async def analyze_scene_with_llm(self, text_segments: List[str]) -> EnhancedSceneInfo:
        """ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ·±åº¦åœºæ™¯åˆ†æ"""
        
        prompt = self._build_scene_analysis_prompt(text_segments)
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": SCENE_ANALYSIS_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"}
            )
            
            scene_data = json.loads(response.choices[0].message.content)
            return self._parse_llm_scene_response(scene_data)
            
        except Exception as e:
            # é™çº§åˆ°æœ¬åœ°æ¨¡å‹
            return await self.local_fallback.analyze_scene(text_segments[0])
```

### 2. ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æƒ…èŠ‚åˆ†æ
```python
CONTEXT_ANALYSIS_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éŸ³é¢‘åˆ¶ä½œä¸“å®¶ã€‚è¯·åˆ†æè¿™æ®µå°è¯´æ–‡æœ¬çš„æƒ…èŠ‚å‘å±•å’Œæƒ…æ„Ÿå˜åŒ–ï¼š

æ–‡æœ¬å†…å®¹ï¼š
{text_content}

è¯·ä»ä»¥ä¸‹ç»´åº¦åˆ†æï¼š
1. æƒ…èŠ‚ç´§å¼ åº¦ (1-10çº§)
2. æƒ…æ„ŸåŸºè°ƒ (å¹³é™/ç´§å¼ /æµªæ¼«/ææ€–/æ¿€åŠ¨ç­‰)
3. åœºæ™¯è½¬æ¢èŠ‚ç‚¹ (æ˜¯å¦æœ‰åœºæ™¯åˆ‡æ¢)
4. ç¯å¢ƒéŸ³å¼ºåº¦å»ºè®® (èƒŒæ™¯éŸ³åº”è¯¥å¤šå¼º)
5. ç‰¹æ®ŠéŸ³æ•ˆå»ºè®® (éœ€è¦ä»€ä¹ˆç‰¹æ®ŠéŸ³æ•ˆ)

è¿”å›JSONæ ¼å¼ç»“æœã€‚
"""

async def analyze_plot_context(self, full_text: str, current_segment: str) -> PlotContext:
    """åˆ†æå½“å‰æ®µè½åœ¨æ•´ä¸ªæƒ…èŠ‚ä¸­çš„ä¸Šä¸‹æ–‡"""
    
    prompt = CONTEXT_ANALYSIS_PROMPT.format(
        text_content=f"å…¨æ–‡èƒŒæ™¯ï¼š{full_text[:500]}...\n\nå½“å‰æ®µè½ï¼š{current_segment}"
    )
    
    response = await self.llm_client.generate(prompt)
    return PlotContext.from_llm_response(response)
```

### 3. åˆ›æ„éŸ³æ•ˆç»„åˆå»ºè®®
```python
CREATIVE_SOUND_PROMPT = """
ä½œä¸ºéŸ³æ•ˆè®¾è®¡å¸ˆï¼Œä¸ºè¿™ä¸ªåœºæ™¯è®¾è®¡åˆ›æ„éŸ³æ•ˆç»„åˆï¼š

åœºæ™¯æè¿°ï¼š{scene_description}
åŸºç¡€ç¯å¢ƒï¼š{base_environment}
æƒ…èŠ‚ä¸Šä¸‹æ–‡ï¼š{plot_context}

è¯·æä¾›ï¼š
1. ä¸»ç¯å¢ƒéŸ³ (æŒç»­æ€§èƒŒæ™¯éŸ³)
2. æ°›å›´éŸ³å±‚ (æƒ…ç»ªæ¸²æŸ“)
3. ç»†èŠ‚éŸ³æ•ˆ (å¢å¼ºçœŸå®æ„Ÿ)
4. åŠ¨æ€å˜åŒ–å»ºè®® (éŸ³é‡/å¼ºåº¦å˜åŒ–)

è¦æ±‚éŸ³æ•ˆå…·æœ‰ç”µå½±çº§çš„ä¸“ä¸šæ€§å’Œåˆ›æ„æ€§ã€‚
"""

async def suggest_creative_soundscape(self, scene_info: SceneInfo) -> CreativeSoundscape:
    """AIåˆ›æ„éŸ³æ•ˆè®¾è®¡"""
    
    prompt = CREATIVE_SOUND_PROMPT.format(
        scene_description=scene_info.description,
        base_environment=scene_info.environment,
        plot_context=scene_info.plot_context
    )
    
    response = await self.llm_client.generate(prompt)
    return self._parse_creative_suggestions(response)
```

## ğŸ­ é«˜çº§åº”ç”¨åœºæ™¯

### 1. è§’è‰²å£°éŸ³æƒ…ç»ªåˆ†æ
```python
EMOTION_ANALYSIS_PROMPT = """
åˆ†æè¿™æ®µå¯¹è¯ä¸­è§’è‰²çš„æƒ…ç»ªçŠ¶æ€ï¼Œç”¨äºè°ƒèŠ‚ç¯å¢ƒéŸ³ï¼š

å¯¹è¯å†…å®¹ï¼š"{dialogue}"
è§’è‰²ï¼š{character_name}
ä¸Šä¸‹æ–‡ï¼š{context}

åˆ†æï¼š
1. æƒ…ç»ªå¼ºåº¦ (1-10)
2. æƒ…ç»ªç±»å‹ (æ„¤æ€’/æ‚²ä¼¤/å–œæ‚¦/ææƒ§/å¹³é™)
3. è¯­æ°”ç‰¹å¾ (æ€¥ä¿ƒ/ç¼“æ…¢/é¢¤æŠ–/åšå®š)
4. ç¯å¢ƒéŸ³é…åˆå»ºè®® (åº”è¯¥å¦‚ä½•è°ƒèŠ‚èƒŒæ™¯éŸ³æ¥é…åˆè¿™ç§æƒ…ç»ª)

å½“è§’è‰²æƒ…ç»ªæ¿€åŠ¨æ—¶ï¼Œç¯å¢ƒéŸ³åº”è¯¥å¦‚ä½•å“åº”ï¼Ÿ
"""

async def analyze_character_emotion(self, dialogue: str, character: str) -> EmotionAnalysis:
    """åˆ†æè§’è‰²æƒ…ç»ªï¼ŒæŒ‡å¯¼ç¯å¢ƒéŸ³è°ƒèŠ‚"""
    
    emotion_data = await self.llm_client.analyze_emotion(dialogue, character)
    
    # æ ¹æ®æƒ…ç»ªè°ƒèŠ‚ç¯å¢ƒéŸ³å‚æ•°
    if emotion_data.intensity > 7:  # é«˜å¼ºåº¦æƒ…ç»ª
        env_adjustment = {
            "volume_change": -0.2,  # ç¯å¢ƒéŸ³é™ä½
            "reverb_increase": 0.1,  # å¢åŠ æ··å“çªå‡ºæƒ…ç»ª
            "frequency_filter": "high_pass"  # çªå‡ºä¸­é«˜é¢‘ï¼Œå¢å¼ºç´§å¼ æ„Ÿ
        }
    
    return EmotionAnalysis(emotion_data, env_adjustment)
```

### 2. æ™ºèƒ½éŸ³æ•ˆæ˜ å°„
```python
SOUND_MAPPING_PROMPT = """
ä½œä¸ºéŸ³æ•ˆåº“ç®¡ç†ä¸“å®¶ï¼Œä¸ºè¿™ä¸ªåœºæ™¯åŒ¹é…æœ€åˆé€‚çš„éŸ³æ•ˆæ–‡ä»¶ï¼š

åœºæ™¯ä¿¡æ¯ï¼š
- åœ°ç‚¹: {location}
- å¤©æ°”: {weather}  
- æ—¶é—´: {time}
- æ°›å›´: {atmosphere}
- æƒ…èŠ‚å¼ åŠ›: {tension_level}

å¯ç”¨éŸ³æ•ˆåº“ï¼š
{available_sounds}

è¯·é€‰æ‹©ï¼š
1. æœ€åŒ¹é…çš„3ä¸ªéŸ³æ•ˆæ–‡ä»¶
2. æ¯ä¸ªéŸ³æ•ˆçš„é€‚ç”¨æ—¶æœº
3. éŸ³é‡å»ºè®® (0-1)
4. æ˜¯å¦éœ€è¦æ··åˆæ’­æ”¾
5. ç‰¹æ®Šå¤„ç†å»ºè®® (æ·¡å…¥æ·¡å‡ºã€å¾ªç¯ç­‰)

ä¼˜å…ˆè€ƒè™‘éŸ³æ•ˆçš„ç»„åˆæ•ˆæœå’Œåˆ›æ„æ€§ã€‚
"""

async def intelligent_sound_mapping(self, scene: SceneInfo, sound_library: List[SoundFile]) -> SoundMapping:
    """æ™ºèƒ½éŸ³æ•ˆåŒ¹é…"""
    
    available_sounds = "\n".join([f"- {s.name}: {s.description}" for s in sound_library])
    
    mapping = await self.llm_client.map_sounds(scene, available_sounds)
    
    return SoundMapping(
        primary_sounds=mapping.primary_sounds,
        secondary_sounds=mapping.secondary_sounds,
        mixing_instructions=mapping.mixing_instructions
    )
```

### 3. åŠ¨æ€æƒ…èŠ‚è·Ÿè¸ª
```python
class PlotTracker:
    def __init__(self):
        self.plot_history = []
        self.character_states = {}
        self.environment_continuity = {}
    
    async def track_plot_development(self, new_segment: str) -> PlotUpdate:
        """è·Ÿè¸ªæƒ…èŠ‚å‘å±•ï¼Œä¿æŒéŸ³æ•ˆè¿ç»­æ€§"""
        
        plot_analysis_prompt = f"""
        åŸºäºä¹‹å‰çš„æƒ…èŠ‚å‘å±•ï¼š
        {self._summarize_plot_history()}
        
        å½“å‰æ–°æ®µè½ï¼š
        {new_segment}
        
        åˆ†æï¼š
        1. æƒ…èŠ‚æ˜¯å¦æœ‰é‡å¤§è½¬æŠ˜ï¼Ÿ
        2. è§’è‰²çŠ¶æ€æ˜¯å¦å‘ç”Ÿå˜åŒ–ï¼Ÿ
        3. ç¯å¢ƒæ˜¯å¦éœ€è¦è°ƒæ•´ï¼Ÿ
        4. éŸ³æ•ˆæ˜¯å¦éœ€è¦é€æ¸è¿‡æ¸¡è¿˜æ˜¯çªç„¶å˜åŒ–ï¼Ÿ
        5. è¿™ä¸ªæ®µè½å¯¹æ•´ä½“æƒ…èŠ‚çš„é‡è¦æ€§ (1-10)
        
        è€ƒè™‘éŸ³æ•ˆçš„è¿ç»­æ€§å’Œå˜åŒ–çš„åˆç†æ€§ã€‚
        """
        
        plot_update = await self.llm_client.analyze_plot_development(plot_analysis_prompt)
        
        # æ›´æ–°å†…éƒ¨çŠ¶æ€
        self.plot_history.append(plot_update)
        self._update_character_states(plot_update)
        self._update_environment_continuity(plot_update)
        
        return plot_update
```

## ğŸ”§ æŠ€æœ¯å®ç°æ¶æ„

### 1. æ··åˆAIç­–ç•¥
```python
class HybridAIAnalyzer:
    def __init__(self):
        # å¤§æ¨¡å‹æœåŠ¡ (ä¸»åŠ›)
        self.openai_client = OpenAI()
        self.claude_client = AnthropicClient()
        
        # æœ¬åœ°æ¨¡å‹ (å…œåº•)
        self.local_ollama = OllamaClient()
        self.local_transformer = LocalTransformerModel()
        
        # ç¼“å­˜æœåŠ¡
        self.redis_cache = RedisCache()
    
    async def analyze_with_fallback(self, text: str, analysis_type: str) -> AnalysisResult:
        """å¤šçº§é™çº§åˆ†æç­–ç•¥"""
        
        cache_key = f"analysis:{analysis_type}:{hash(text)}"
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cached_result = await self.redis_cache.get(cache_key)
        if cached_result:
            return cached_result
        
        # 2. å°è¯•ä¸»åŠ›å¤§æ¨¡å‹ (GPT-4)
        try:
            result = await self._analyze_with_openai(text, analysis_type)
            await self.redis_cache.set(cache_key, result, ttl=3600)
            return result
        except Exception as e:
            logger.warning(f"OpenAI failed: {e}")
        
        # 3. é™çº§åˆ°Claude
        try:
            result = await self._analyze_with_claude(text, analysis_type)
            await self.redis_cache.set(cache_key, result, ttl=1800)
            return result
        except Exception as e:
            logger.warning(f"Claude failed: {e}")
        
        # 4. æœ€åé™çº§åˆ°æœ¬åœ°æ¨¡å‹
        result = await self._analyze_with_local(text, analysis_type)
        await self.redis_cache.set(cache_key, result, ttl=600)
        return result
```

### 2. æˆæœ¬ä¼˜åŒ–ç­–ç•¥
```python
class CostOptimizedLLMManager:
    def __init__(self):
        self.usage_tracker = UsageTracker()
        self.smart_batching = SmartBatchingManager()
    
    async def analyze_batch_with_cost_optimization(self, segments: List[str]) -> List[AnalysisResult]:
        """æˆæœ¬ä¼˜åŒ–çš„æ‰¹é‡åˆ†æ"""
        
        # 1. æ™ºèƒ½åˆ†ç»„ - ç›¸ä¼¼åœºæ™¯åˆå¹¶åˆ†æ
        grouped_segments = self._group_similar_scenes(segments)
        
        # 2. é€‰æ‹©åˆé€‚çš„æ¨¡å‹
        for group in grouped_segments:
            if group.complexity_score < 0.3:  # ç®€å•åœºæ™¯
                model = "gpt-3.5-turbo"  # ä¾¿å®œæ¨¡å‹
            else:  # å¤æ‚åœºæ™¯
                model = "gpt-4o"  # é«˜è´¨é‡æ¨¡å‹
            
            # 3. æ‰¹é‡å¤„ç†
            results = await self._batch_analyze(group.segments, model)
            
        return results
    
    def _estimate_token_cost(self, text: str, model: str) -> float:
        """é¢„ä¼°tokenæˆæœ¬"""
        token_count = len(text) // 4  # ç²—ä¼°
        
        costs = {
            "gpt-4o": 0.005,  # per 1k tokens
            "gpt-3.5-turbo": 0.001,
            "claude-3": 0.003
        }
        
        return (token_count / 1000) * costs.get(model, 0.005)
```

### 3. å®æ—¶æ€§ä¼˜åŒ–
```python
class RealtimeLLMProcessor:
    def __init__(self):
        self.streaming_client = StreamingLLMClient()
        self.partial_result_handler = PartialResultHandler()
    
    async def stream_scene_analysis(self, text: str) -> AsyncGenerator[PartialAnalysis, None]:
        """æµå¼åœºæ™¯åˆ†æï¼Œå®æ—¶è¿”å›éƒ¨åˆ†ç»“æœ"""
        
        async for partial_response in self.streaming_client.stream_analyze(text):
            partial_analysis = self._parse_partial_response(partial_response)
            
            if partial_analysis.confidence > 0.7:  # ç½®ä¿¡åº¦å¤Ÿé«˜å°±å…ˆè¿”å›
                yield partial_analysis
    
    async def progressive_enhancement(self, initial_analysis: PartialAnalysis, full_text: str) -> EnhancedAnalysis:
        """æ¸è¿›å¼å¢å¼ºåˆ†æ"""
        
        # å…ˆåŸºäºåˆæ­¥åˆ†æå¼€å§‹éŸ³é¢‘å¤„ç†
        enhanced_analysis = initial_analysis
        
        # åå°ç»§ç»­æ·±åº¦åˆ†æ
        deep_analysis = await self._deep_analyze_background(full_text)
        
        # å¦‚æœæœ‰é‡å¤§å·®å¼‚ï¼Œè§¦å‘æ›´æ–°
        if self._has_significant_difference(enhanced_analysis, deep_analysis):
            return deep_analysis
        
        return enhanced_analysis
```

## ğŸ“Š å¤§æ¨¡å‹æœåŠ¡å¯¹æ¯”

| æœåŠ¡å•† | æ¨¡å‹ | åœºæ™¯ç†è§£èƒ½åŠ› | æˆæœ¬ | å»¶è¿Ÿ | é€‚ç”¨åœºæ™¯ |
|--------|------|-------------|------|------|----------|
| OpenAI | GPT-4o | â­â­â­â­â­ | é«˜ | ä¸­ | å¤æ‚åœºæ™¯åˆ†æ |
| OpenAI | GPT-3.5 | â­â­â­â­ | ä½ | ä½ | åŸºç¡€åœºæ™¯è¯†åˆ« |
| Anthropic | Claude-3 | â­â­â­â­â­ | ä¸­ | ä¸­ | åˆ›æ„éŸ³æ•ˆå»ºè®® |
| æœ¬åœ° | Ollama | â­â­â­ | æ—  | ä½ | å…œåº•æ–¹æ¡ˆ |

## ğŸš€ é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ

### APIæ¥å£æ‰©å±•
```python
# app/api/v1/llm_enhanced_mixing.py

@router.post("/api/v1/llm-scene-analysis/enhanced")
async def llm_enhanced_scene_analysis(request: LLMSceneAnalysisRequest):
    """LLMå¢å¼ºçš„åœºæ™¯åˆ†æ"""
    
    analyzer = LLMSceneAnalyzer()
    
    # å¹¶è¡Œå¤„ç†ï¼šæœ¬åœ°+äº‘ç«¯
    local_task = analyzer.local_analyze(request.text)
    llm_task = analyzer.llm_analyze(request.text)
    
    local_result, llm_result = await asyncio.gather(local_task, llm_task)
    
    # èåˆç»“æœ
    enhanced_result = analyzer.merge_results(local_result, llm_result)
    
    return {
        "success": True,
        "data": {
            "enhanced_scene_info": enhanced_result,
            "confidence_score": enhanced_result.confidence,
            "processing_method": enhanced_result.method,
            "cost_estimate": enhanced_result.cost
        }
    }

@router.post("/api/v1/llm-creative-soundscape/generate")
async def generate_creative_soundscape(request: CreativeSoundscapeRequest):
    """AIåˆ›æ„éŸ³æ•ˆè®¾è®¡"""
    
    designer = CreativeSoundscapeDesigner()
    
    soundscape = await designer.design_with_llm(
        scene_info=request.scene_info,
        style_preference=request.style,
        creative_level=request.creativity
    )
    
    return {
        "success": True, 
        "data": {
            "creative_soundscape": soundscape,
            "sound_layers": soundscape.layers,
            "mixing_instructions": soundscape.mixing_guide
        }
    }
```

## ğŸ’° æˆæœ¬ç®¡æ§ç­–ç•¥

### 1. æ™ºèƒ½ç¼“å­˜
- **åœºæ™¯ç¼“å­˜**: ç›¸ä¼¼åœºæ™¯å¤ç”¨åˆ†æç»“æœ
- **ç‰‡æ®µç¼“å­˜**: é‡å¤æ–‡æœ¬æ®µè½é¿å…é‡å¤åˆ†æ
- **ç»“æœç¼“å­˜**: é«˜è´¨é‡åˆ†æç»“æœé•¿æœŸä¿å­˜

### 2. åˆ†çº§å¤„ç†
- **ç®€å•åœºæ™¯**: æœ¬åœ°æ¨¡å‹å¤„ç†
- **å¤æ‚åœºæ™¯**: äº‘ç«¯å¤§æ¨¡å‹å¤„ç†
- **åˆ›æ„éœ€æ±‚**: é«˜çº§æ¨¡å‹å¤„ç†

### 3. æ‰¹é‡ä¼˜åŒ–
- **æ‰¹é‡åˆ†æ**: å¤šä¸ªåœºæ™¯åˆå¹¶è¯·æ±‚
- **å¼‚æ­¥å¤„ç†**: éå®æ—¶éœ€æ±‚åå°å¤„ç†
- **é¢„å¤„ç†**: çƒ­é—¨å†…å®¹æå‰åˆ†æ

---

**æ€»ç»“**: å¤§æ¨¡å‹æœåŠ¡å°†æå¤§æå‡ç¯å¢ƒéŸ³æ··åˆç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œä»ç®€å•çš„å…³é”®è¯åŒ¹é…å‡çº§åˆ°æ·±åº¦è¯­ä¹‰ç†è§£å’Œåˆ›æ„éŸ³æ•ˆè®¾è®¡ï¼ŒçœŸæ­£å®ç°"AIå¯¼æ¼”"çº§åˆ«çš„éŸ³é¢‘åˆ¶ä½œä½“éªŒï¼