# éŸ³é¢‘æ··åˆå¼•æ“è®¾è®¡

## ğŸ¯ åŠŸèƒ½ç›®æ ‡

åŸºäºæ—¶é—´è½´æ§åˆ¶æ–‡ä»¶ï¼Œå°†å¤šè½¨éŸ³é¢‘ï¼ˆå¯¹è¯ã€ç¯å¢ƒéŸ³ã€éŸ³æ•ˆï¼‰æ™ºèƒ½æ··åˆæˆæœ€ç»ˆçš„æ²‰æµ¸å¼éŸ³é¢‘ä½“éªŒã€‚

## ğŸ›ï¸ æ ¸å¿ƒæŠ€æœ¯æ¶æ„

### éŸ³é¢‘æ··åˆæµæ°´çº¿
```python
class AudioMixingEngine:
    def __init__(self):
        self.audio_processor = AudioProcessor()
        self.quality_analyzer = AudioQualityAnalyzer()
        self.export_manager = AudioExportManager()
    
    def mix_audio(self, timeline: Timeline) -> MixedAudio:
        # 1. é¢„å¤„ç†æ‰€æœ‰éŸ³é¢‘è½¨é“
        processed_tracks = self._preprocess_tracks(timeline.tracks)
        
        # 2. æ‰§è¡Œå¤šè½¨æ··åˆ
        mixed_audio = self._perform_mixing(processed_tracks, timeline)
        
        # 3. åå¤„ç†å’Œè´¨é‡ä¼˜åŒ–
        enhanced_audio = self._post_process(mixed_audio)
        
        # 4. è´¨é‡åˆ†æå’ŒéªŒè¯
        quality_report = self._analyze_quality(enhanced_audio)
        
        return MixedAudio(enhanced_audio, quality_report)
```

## ğŸ”§ éŸ³é¢‘é¢„å¤„ç†æ¨¡å—

### 1. éŸ³é¢‘æ ‡å‡†åŒ–
```python
def _normalize_audio_tracks(self, tracks: List[AudioTrack]) -> List[AudioTrack]:
    """ç»Ÿä¸€éŸ³é¢‘æ ¼å¼å’Œè´¨é‡"""
    
    normalized_tracks = []
    
    for track in tracks:
        # æ ‡å‡†åŒ–é‡‡æ ·ç‡
        if track.sample_rate != self.target_sample_rate:
            track = self._resample_audio(track, self.target_sample_rate)
        
        # æ ‡å‡†åŒ–å£°é“æ•°
        if track.channels != self.target_channels:
            track = self._convert_channels(track, self.target_channels)
        
        # æ ‡å‡†åŒ–ä½æ·±åº¦
        if track.bit_depth != self.target_bit_depth:
            track = self._convert_bit_depth(track, self.target_bit_depth)
        
        # éŸ³é¢‘å¢ç›Šæ ‡å‡†åŒ–
        track = self._normalize_gain(track)
        
        normalized_tracks.append(track)
    
    return normalized_tracks

def _normalize_gain(self, audio_track: AudioTrack) -> AudioTrack:
    """éŸ³é¢‘å¢ç›Šæ ‡å‡†åŒ–"""
    
    # åˆ†æéŸ³é¢‘å³°å€¼
    peak_level = audio_track.get_peak_level()
    rms_level = audio_track.get_rms_level()
    
    # æ ¹æ®è½¨é“ç±»å‹è®¾ç½®ç›®æ ‡ç”µå¹³
    if audio_track.type == "dialogue":
        target_rms = -12.0  # dB
    elif audio_track.type == "environment":
        target_rms = -18.0  # dB
    elif audio_track.type == "effects":
        target_rms = -15.0  # dB
    
    # è®¡ç®—å¢ç›Šè°ƒæ•´
    gain_adjustment = target_rms - rms_level
    
    # åº”ç”¨å¢ç›Šï¼Œç¡®ä¿ä¸å‰Šæ³¢
    max_gain = -1.0 - peak_level  # ç•™1dBä½™é‡
    final_gain = min(gain_adjustment, max_gain)
    
    return audio_track.apply_gain(final_gain)
```

### 2. éŸ³é¢‘åˆ†æ®µå¤„ç†
```python
def _segment_audio_tracks(self, tracks: List[AudioTrack], timeline: Timeline) -> List[AudioSegment]:
    """æ ¹æ®æ—¶é—´è½´åˆ†æ®µå¤„ç†éŸ³é¢‘"""
    
    segments = []
    
    for track in tracks:
        for timeline_segment in timeline.get_segments_for_track(track.type):
            # æå–å¯¹åº”æ—¶é—´æ®µçš„éŸ³é¢‘
            audio_segment = track.extract_segment(
                start=timeline_segment.start,
                end=timeline_segment.end
            )
            
            # åº”ç”¨æ·¡å…¥æ·¡å‡º
            if timeline_segment.fade_in > 0:
                audio_segment = audio_segment.fade_in(timeline_segment.fade_in * 1000)  # ms
            
            if timeline_segment.fade_out > 0:
                audio_segment = audio_segment.fade_out(timeline_segment.fade_out * 1000)  # ms
            
            # åº”ç”¨éŸ³é‡è°ƒèŠ‚
            volume_db = timeline_segment.volume_db
            audio_segment = audio_segment.apply_gain(volume_db)
            
            segments.append({
                "segment": audio_segment,
                "start_time": timeline_segment.start,
                "end_time": timeline_segment.end,
                "track_type": track.type,
                "metadata": timeline_segment.metadata
            })
    
    return segments
```

## ğŸµ æ™ºèƒ½æ··åˆç®—æ³•

### 1. å¤šè½¨æ··åˆæ ¸å¿ƒç®—æ³•
```python
def _perform_intelligent_mixing(self, segments: List[AudioSegment], timeline: Timeline) -> AudioTrack:
    """æ‰§è¡Œæ™ºèƒ½å¤šè½¨æ··åˆ"""
    
    # åˆ›å»ºä¸»éŸ³è½¨ï¼ˆæœ€é•¿æ—¶é•¿ï¼‰
    total_duration = timeline.total_duration
    master_track = AudioTrack.create_silence(
        duration=total_duration,
        sample_rate=self.target_sample_rate,
        channels=self.target_channels
    )
    
    # æŒ‰è½¨é“ç±»å‹åˆ†ç»„
    track_groups = self._group_segments_by_type(segments)
    
    # æŒ‰ä¼˜å…ˆçº§æ··åˆè½¨é“
    mixing_order = ["environment", "effects", "dialogue"]  # å¯¹è¯è½¨æœ€åæ··åˆï¼Œä¿è¯æ¸…æ™°åº¦
    
    for track_type in mixing_order:
        if track_type in track_groups:
            master_track = self._mix_track_group(
                master_track, 
                track_groups[track_type],
                mix_strategy=self._get_mix_strategy(track_type)
            )
    
    return master_track

def _mix_track_group(self, master_track: AudioTrack, segments: List[AudioSegment], mix_strategy: str) -> AudioTrack:
    """æ··åˆå•ä¸ªè½¨é“ç»„"""
    
    for segment in segments:
        start_sample = int(segment.start_time * self.target_sample_rate)
        
        if mix_strategy == "overlay":
            # å åŠ æ··åˆï¼ˆé€‚ç”¨äºç¯å¢ƒéŸ³ï¼‰
            master_track = master_track.overlay(segment.audio, position=start_sample)
        
        elif mix_strategy == "ducking":
            # é—ªé¿æ··åˆï¼ˆç¯å¢ƒéŸ³åœ¨å¯¹è¯æ—¶è‡ªåŠ¨é™ä½ï¼‰
            master_track = self._apply_ducking(master_track, segment, start_sample)
        
        elif mix_strategy == "priority":
            # ä¼˜å…ˆçº§æ··åˆï¼ˆå¯¹è¯è½¨ä¼˜å…ˆï¼‰
            master_track = self._apply_priority_mix(master_track, segment, start_sample)
    
    return master_track
```

### 2. æ™ºèƒ½é—ªé¿ï¼ˆDuckingï¼‰ç®—æ³•
```python
def _apply_intelligent_ducking(self, background_track: AudioTrack, dialogue_segments: List[AudioSegment]) -> AudioTrack:
    """æ™ºèƒ½é—ªé¿ï¼šå¯¹è¯æ—¶è‡ªåŠ¨é™ä½èƒŒæ™¯éŸ³"""
    
    ducked_track = background_track.copy()
    
    for dialogue_segment in dialogue_segments:
        start_time = dialogue_segment.start_time
        end_time = dialogue_segment.end_time
        
        # åˆ†æå¯¹è¯éŸ³é‡
        dialogue_rms = dialogue_segment.audio.get_rms_level()
        
        # è®¡ç®—é—ªé¿å‚æ•°
        duck_amount = self._calculate_duck_amount(dialogue_rms)
        duck_attack = 0.1  # 100ms attack
        duck_release = 0.3  # 300ms release
        
        # åº”ç”¨é—ªé¿æ•ˆæœ
        ducked_track = ducked_track.apply_ducking(
            start_time=start_time - duck_attack,
            end_time=end_time + duck_release,
            reduction_db=duck_amount,
            attack_time=duck_attack,
            release_time=duck_release
        )
    
    return ducked_track

def _calculate_duck_amount(self, dialogue_rms: float) -> float:
    """æ ¹æ®å¯¹è¯éŸ³é‡è®¡ç®—é—ªé¿é‡"""
    
    # å¯¹è¯è¶Šå“ï¼ŒèƒŒæ™¯éŸ³é™ä½è¶Šå¤š
    if dialogue_rms > -6:  # å¾ˆå“çš„å¯¹è¯
        return -8.0  # èƒŒæ™¯éŸ³é™ä½8dB
    elif dialogue_rms > -12:  # æ­£å¸¸å¯¹è¯
        return -5.0  # èƒŒæ™¯éŸ³é™ä½5dB
    else:  # è½»å£°å¯¹è¯
        return -3.0  # èƒŒæ™¯éŸ³é™ä½3dB
```

### 3. åŠ¨æ€èŒƒå›´æ§åˆ¶
```python
def _apply_dynamic_range_control(self, audio_track: AudioTrack) -> AudioTrack:
    """åº”ç”¨åŠ¨æ€èŒƒå›´æ§åˆ¶ï¼Œç¡®ä¿éŸ³é¢‘å¹³è¡¡"""
    
    # å¤šæ®µå‹ç¼©å™¨è®¾ç½®
    compressor_settings = {
        "low_freq": {
            "threshold": -20.0,
            "ratio": 3.0,
            "attack": 10.0,
            "release": 50.0
        },
        "mid_freq": {
            "threshold": -15.0,
            "ratio": 4.0,
            "attack": 5.0,
            "release": 30.0
        },
        "high_freq": {
            "threshold": -12.0,
            "ratio": 2.5,
            "attack": 2.0,
            "release": 20.0
        }
    }
    
    # åº”ç”¨å¤šæ®µå‹ç¼©
    compressed_track = audio_track
    for freq_band, settings in compressor_settings.items():
        compressed_track = compressed_track.apply_multiband_compressor(
            freq_band=freq_band,
            **settings
        )
    
    # é™åˆ¶å™¨é˜²æ­¢å‰Šæ³¢
    limited_track = compressed_track.apply_limiter(
        threshold=-1.0,
        release=5.0
    )
    
    return limited_track
```

## ğŸšï¸ éŸ³é¢‘åå¤„ç†æ¨¡å—

### 1. éŸ³è´¨å¢å¼º
```python
def _enhance_audio_quality(self, mixed_audio: AudioTrack) -> AudioTrack:
    """éŸ³è´¨å¢å¼ºå¤„ç†"""
    
    enhanced_audio = mixed_audio
    
    # 1. ç©ºé—´å¢å¼º
    enhanced_audio = self._apply_spatial_enhancement(enhanced_audio)
    
    # 2. é¢‘ç‡å¹³è¡¡
    enhanced_audio = self._apply_eq_optimization(enhanced_audio)
    
    # 3. ç«‹ä½“å£°æ‰©å±•
    enhanced_audio = self._apply_stereo_widening(enhanced_audio)
    
    # 4. ç»†èŠ‚å¢å¼º
    enhanced_audio = self._apply_detail_enhancement(enhanced_audio)
    
    return enhanced_audio

def _apply_spatial_enhancement(self, audio: AudioTrack) -> AudioTrack:
    """ç©ºé—´éŸ³æ•ˆå¢å¼º"""
    
    # ä¸ºä¸åŒç±»å‹çš„éŸ³é¢‘åº”ç”¨ä¸åŒçš„ç©ºé—´æ•ˆæœ
    enhanced = audio
    
    # å¯¹è¯ï¼šä¿æŒä¸­å¤®å®šä½ï¼Œè½»å¾®æ··å“
    dialogue_reverb = Reverb(
        room_size=0.2,
        damping=0.7,
        wet_level=0.15
    )
    
    # ç¯å¢ƒéŸ³ï¼šå¢åŠ ç©ºé—´æ„Ÿ
    environment_reverb = Reverb(
        room_size=0.6,
        damping=0.4,
        wet_level=0.3
    )
    
    return enhanced.apply_contextual_reverb(
        dialogue_reverb=dialogue_reverb,
        environment_reverb=environment_reverb
    )
```

### 2. è‡ªé€‚åº”å‡è¡¡
```python
def _apply_adaptive_eq(self, audio: AudioTrack) -> AudioTrack:
    """è‡ªé€‚åº”é¢‘ç‡å‡è¡¡"""
    
    # åˆ†æé¢‘è°±ç‰¹å¾
    spectrum_analysis = audio.analyze_spectrum()
    
    # æ£€æµ‹é—®é¢˜é¢‘æ®µ
    problem_frequencies = self._detect_problematic_frequencies(spectrum_analysis)
    
    # ç”Ÿæˆå‡è¡¡æ›²çº¿
    eq_curve = self._generate_eq_curve(spectrum_analysis, problem_frequencies)
    
    # åº”ç”¨å‡è¡¡
    equalized_audio = audio.apply_parametric_eq(eq_curve)
    
    return equalized_audio

def _detect_problematic_frequencies(self, spectrum: SpectrumAnalysis) -> List[ProblemFreq]:
    """æ£€æµ‹æœ‰é—®é¢˜çš„é¢‘æ®µ"""
    
    problems = []
    
    # æ£€æµ‹ä½é¢‘è½°é¸£
    if spectrum.low_freq_energy > 0.3:
        problems.append(ProblemFreq(
            freq=80,
            type="rumble",
            severity=spectrum.low_freq_energy
        ))
    
    # æ£€æµ‹ä¸­é¢‘æ³¥æ³
    if spectrum.mid_freq_clarity < 0.7:
        problems.append(ProblemFreq(
            freq=500,
            type="muddy",
            severity=1.0 - spectrum.mid_freq_clarity
        ))
    
    # æ£€æµ‹é«˜é¢‘åˆºæ¿€
    if spectrum.high_freq_harshness > 0.4:
        problems.append(ProblemFreq(
            freq=8000,
            type="harsh",
            severity=spectrum.high_freq_harshness
        ))
    
    return problems
```

## ğŸ“Š è´¨é‡åˆ†æä¸éªŒè¯

### 1. éŸ³é¢‘è´¨é‡æŒ‡æ ‡
```python
class AudioQualityAnalyzer:
    def analyze_quality(self, mixed_audio: AudioTrack) -> QualityReport:
        """åˆ†æéŸ³é¢‘è´¨é‡"""
        
        report = QualityReport()
        
        # 1. åŠ¨æ€èŒƒå›´åˆ†æ
        report.dynamic_range = self._analyze_dynamic_range(mixed_audio)
        
        # 2. é¢‘ç‡å¹³è¡¡åˆ†æ
        report.frequency_balance = self._analyze_frequency_balance(mixed_audio)
        
        # 3. ç«‹ä½“å£°æˆåƒåˆ†æ
        report.stereo_imaging = self._analyze_stereo_imaging(mixed_audio)
        
        # 4. å£°éŸ³æ¸…æ™°åº¦åˆ†æ
        report.clarity = self._analyze_clarity(mixed_audio)
        
        # 5. æ•´ä½“å“åº¦åˆ†æ
        report.loudness = self._analyze_loudness(mixed_audio)
        
        # 6. ç”Ÿæˆå»ºè®®
        report.recommendations = self._generate_quality_recommendations(report)
        
        return report
    
    def _analyze_dynamic_range(self, audio: AudioTrack) -> DynamicRangeMetrics:
        """åŠ¨æ€èŒƒå›´åˆ†æ"""
        
        peak_level = audio.get_peak_level()
        rms_level = audio.get_rms_level()
        crest_factor = peak_level - rms_level
        
        return DynamicRangeMetrics(
            peak_level=peak_level,
            rms_level=rms_level,
            crest_factor=crest_factor,
            dr_score=self._calculate_dr_score(audio)
        )
```

### 2. è‡ªåŠ¨è´¨é‡ä¼˜åŒ–
```python
def _auto_optimize_quality(self, mixed_audio: AudioTrack, quality_report: QualityReport) -> AudioTrack:
    """åŸºäºè´¨é‡åˆ†æè‡ªåŠ¨ä¼˜åŒ–éŸ³é¢‘"""
    
    optimized_audio = mixed_audio
    
    # æ ¹æ®è´¨é‡æŠ¥å‘Šåº”ç”¨ä¼˜åŒ–
    for issue in quality_report.issues:
        if issue.type == "low_dynamic_range":
            optimized_audio = self._enhance_dynamic_range(optimized_audio)
        
        elif issue.type == "frequency_imbalance":
            optimized_audio = self._correct_frequency_balance(optimized_audio, issue.parameters)
        
        elif issue.type == "poor_stereo_imaging":
            optimized_audio = self._improve_stereo_imaging(optimized_audio)
        
        elif issue.type == "clarity_issues":
            optimized_audio = self._enhance_clarity(optimized_audio)
    
    return optimized_audio
```

## ğŸš€ å¯¼å‡ºå’Œæ ¼å¼æ”¯æŒ

### å¤šæ ¼å¼å¯¼å‡º
```python
class AudioExportManager:
    def export_mixed_audio(self, mixed_audio: AudioTrack, export_config: ExportConfig) -> ExportResult:
        """å¯¼å‡ºæ··åˆéŸ³é¢‘åˆ°å¤šç§æ ¼å¼"""
        
        results = []
        
        for format_config in export_config.formats:
            exported_file = self._export_to_format(mixed_audio, format_config)
            
            # éªŒè¯å¯¼å‡ºè´¨é‡
            quality_check = self._verify_export_quality(exported_file, format_config)
            
            results.append(ExportResult(
                file_path=exported_file.path,
                format=format_config.format,
                quality_score=quality_check.score,
                file_size=exported_file.size,
                duration=exported_file.duration
            ))
        
        return ExportResults(results)

EXPORT_FORMATS = {
    "high_quality": {
        "format": "wav",
        "sample_rate": 48000,
        "bit_depth": 24,
        "channels": 2
    },
    "streaming": {
        "format": "mp3",
        "bitrate": 320,
        "vbr": True,
        "channels": 2
    },
    "mobile": {
        "format": "aac",
        "bitrate": 128,
        "profile": "LC",
        "channels": 2
    }
}
```

## ğŸ“‹ APIæ¥å£è®¾è®¡

```python
@router.post("/api/v1/audio-mixing/mix")
async def mix_audio(request: AudioMixingRequest):
    """æ‰§è¡ŒéŸ³é¢‘æ··åˆ"""
    
    mixer = AudioMixingEngine()
    
    try:
        # æ‰§è¡Œæ··åˆ
        result = mixer.mix_audio(request.timeline)
        
        # å¯¼å‡ºéŸ³é¢‘
        export_results = mixer.export_audio(
            result.mixed_audio,
            request.export_config
        )
        
        return {
            "success": True,
            "data": {
                "mixed_audio_url": export_results.primary_file.url,
                "alternative_formats": [f.url for f in export_results.alternative_files],
                "quality_report": result.quality_report.to_dict(),
                "mixing_stats": result.mixing_stats
            }
        }
        
    except AudioMixingException as e:
        return {
            "success": False,
            "error": {
                "type": "mixing_error",
                "message": str(e),
                "details": e.details
            }
        }

@router.get("/api/v1/audio-mixing/preview")
async def preview_mix(timeline_id: str, preview_duration: int = 30):
    """ç”Ÿæˆæ··åˆé¢„è§ˆï¼ˆå‰30ç§’ï¼‰"""
    
    mixer = AudioMixingEngine()
    
    # ç”Ÿæˆé¢„è§ˆç‰‡æ®µ
    preview_audio = mixer.generate_preview(
        timeline_id=timeline_id,
        duration=preview_duration
    )
    
    return {
        "success": True,
        "data": {
            "preview_url": preview_audio.url,
            "duration": preview_audio.duration,
            "quality_preview": preview_audio.quality_metrics
        }
    }
```

---

**æ€»ç»“**: è‡³æ­¤ï¼Œç¯å¢ƒéŸ³æ··åˆç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯æ¨¡å—è®¾è®¡å®Œæˆã€‚åŒ…æ‹¬åœºæ™¯åˆ†æã€æ—¶é—´è½´ç”Ÿæˆã€éŸ³é¢‘æ··åˆä¸‰å¤§æ ¸å¿ƒå¼•æ“ï¼Œèƒ½å¤Ÿå®ç°ä»"å¬ä¹¦"åˆ°"å¬ç”µå½±"çš„æ™ºèƒ½åŒ–éŸ³é¢‘ä½“éªŒå‡çº§ã€‚