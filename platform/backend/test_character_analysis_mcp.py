#!/usr/bin/env python3
"""
æµ‹è¯•è§’è‰²åˆ†æåŠŸèƒ½ - åŒ…æ‹¬MCPç­‰å¤šç§æ–¹å¼
"""

import sys
import os
import asyncio
import json
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath('.'))

try:
    from app.database import SessionLocal
    from app.models import BookChapter
    from app.detectors import OllamaCharacterDetector, AdvancedCharacterDetector, ProgrammaticCharacterDetector
    print("âœ… å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_character_analysis():
    """æµ‹è¯•è§’è‰²åˆ†æåŠŸèƒ½"""
    db = SessionLocal()
    
    try:
        print("ğŸ” æŸ¥æ‰¾æµ‹è¯•ç« èŠ‚...")
        
        # è·å–ä¹¦ç±IDä¸º11çš„ç¬¬ä¸€ç« è¿›è¡Œæµ‹è¯•
        chapter = db.query(BookChapter).filter(
            BookChapter.book_id == 11,
            BookChapter.chapter_number == 1
        ).first()
        
        if not chapter:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç« èŠ‚")
            return
            
        print(f"âœ… æ‰¾åˆ°æµ‹è¯•ç« èŠ‚: {chapter.chapter_title}")
        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(chapter.content)} å­—ç¬¦")
        
        # å–ä¸€å°æ®µå†…å®¹è¿›è¡Œæµ‹è¯•
        test_content = chapter.content[:500] if chapter.content else "å­™æ‚Ÿç©ºè¯´é“ï¼š\"å¸ˆçˆ¶ï¼Œæˆ‘ä»¬èµ°å§ã€‚\"å”åƒ§ç‚¹äº†ç‚¹å¤´ã€‚"
        print(f"ğŸ§ª æµ‹è¯•å†…å®¹: {test_content[:100]}...")
        
        chapter_info = {
            'chapter_id': chapter.id,
            'chapter_title': chapter.chapter_title,
            'chapter_number': chapter.chapter_number
        }
        
        # æµ‹è¯•1: Ollamaæ£€æµ‹å™¨
        print("\nğŸ¤– æµ‹è¯•1: Ollama AIæ£€æµ‹å™¨")
        try:
            ollama_detector = OllamaCharacterDetector()
            print(f"   æ¨¡å‹: {ollama_detector.model_name}")
            print(f"   URL: {ollama_detector.ollama_url}")
            
            result = await ollama_detector.analyze_text(test_content, chapter_info)
            
            print(f"   âœ… Ollamaåˆ†ææˆåŠŸ")
            print(f"   ğŸ“Š è¯†åˆ«è§’è‰²æ•°: {len(result.get('detected_characters', []))}")
            print(f"   ğŸ“ åˆ†æ®µæ•°: {len(result.get('segments', []))}")
            print(f"   ğŸ” åˆ†ææ–¹æ³•: {result.get('processing_stats', {}).get('analysis_method', 'unknown')}")
            
            # æ˜¾ç¤ºè¯†åˆ«çš„è§’è‰²
            characters = result.get('detected_characters', [])
            if characters:
                print("   ğŸ­ è¯†åˆ«çš„è§’è‰²:")
                for char in characters[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    name = char.get('name', 'æœªçŸ¥')
                    freq = char.get('frequency', 0)
                    gender = char.get('recommended_config', {}).get('gender', 'unknown')
                    print(f"      - {name} (é¢‘æ¬¡: {freq}, æ€§åˆ«: {gender})")
            
        except Exception as e:
            print(f"   âŒ Ollamaæ£€æµ‹å™¨å¤±è´¥: {str(e)}")
        
        # æµ‹è¯•2: é«˜çº§æ£€æµ‹å™¨
        print("\nğŸ§  æµ‹è¯•2: é«˜çº§è§„åˆ™æ£€æµ‹å™¨")
        try:
            advanced_detector = AdvancedCharacterDetector()
            result = advanced_detector.analyze_text(test_content, chapter_info)
            
            print(f"   âœ… é«˜çº§æ£€æµ‹å™¨åˆ†ææˆåŠŸ")
            print(f"   ğŸ“Š è¯†åˆ«è§’è‰²æ•°: {len(result.get('detected_characters', []))}")
            print(f"   ğŸ“ åˆ†æ®µæ•°: {len(result.get('segments', []))}")
            
            # æ˜¾ç¤ºè¯†åˆ«çš„è§’è‰²
            characters = result.get('detected_characters', [])
            if characters:
                print("   ğŸ­ è¯†åˆ«çš„è§’è‰²:")
                for char in characters[:3]:
                    name = char.get('name', 'æœªçŸ¥')
                    freq = char.get('frequency', 0)
                    print(f"      - {name} (é¢‘æ¬¡: {freq})")
                    
        except Exception as e:
            print(f"   âŒ é«˜çº§æ£€æµ‹å™¨å¤±è´¥: {str(e)}")
        
        # æµ‹è¯•3: åŸºç¡€æ£€æµ‹å™¨
        print("\nğŸ“ æµ‹è¯•3: åŸºç¡€ç¨‹åºåŒ–æ£€æµ‹å™¨")
        try:
            basic_detector = ProgrammaticCharacterDetector()
            result = basic_detector.analyze_text(test_content, chapter_info)
            
            print(f"   âœ… åŸºç¡€æ£€æµ‹å™¨åˆ†ææˆåŠŸ")
            print(f"   ğŸ“Š è¯†åˆ«è§’è‰²æ•°: {len(result.get('detected_characters', []))}")
            
        except Exception as e:
            print(f"   âŒ åŸºç¡€æ£€æµ‹å™¨å¤±è´¥: {str(e)}")
        
        # æµ‹è¯•4: æ£€æŸ¥ç½‘ç»œè¿æ¥
        print("\nğŸŒ æµ‹è¯•4: æ£€æŸ¥OllamaæœåŠ¡è¿æ¥")
        try:
            import requests
            response = requests.get("http://localhost:11434/api/version", timeout=5)
            if response.status_code == 200:
                print("   âœ… OllamaæœåŠ¡æ­£å¸¸è¿è¡Œ")
                version_info = response.json()
                print(f"   ğŸ“‹ ç‰ˆæœ¬ä¿¡æ¯: {version_info}")
            else:
                print(f"   âš ï¸ OllamaæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
        except requests.exceptions.ConnectionError:
            print("   âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ (http://localhost:11434)")
            print("   ğŸ’¡ å»ºè®®: è¯·å¯åŠ¨OllamaæœåŠ¡")
        except Exception as e:
            print(f"   âŒ ç½‘ç»œæµ‹è¯•å¤±è´¥: {str(e)}")
        
        # æµ‹è¯•5: æ£€æŸ¥MCPå¯ç”¨æ€§
        print("\nğŸ”Œ æµ‹è¯•5: æ£€æŸ¥MCPå¯ç”¨æ€§")
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ MCPç›¸å…³çš„æµ‹è¯•
            print("   â„¹ï¸ MCPæµ‹è¯•åŠŸèƒ½å¾…å®ç°")
        except Exception as e:
            print(f"   âŒ MCPæµ‹è¯•å¤±è´¥: {str(e)}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()

def test_simple_analysis():
    """æµ‹è¯•ç®€å•åˆ†æåŠŸèƒ½"""
    print("\nğŸ”§ æµ‹è¯•ç®€å•æ–‡æœ¬åˆ†æ")
    
    test_text = """
    å­™æ‚Ÿç©ºè¯´é“ï¼š"å¸ˆçˆ¶ï¼Œå‰é¢æœ‰å¦–ç²¾ï¼"
    å”åƒ§æƒŠé“ï¼š"æ‚Ÿç©ºï¼Œä½ å¯çœ‹æ¸…æ¥šäº†ï¼Ÿ"
    çŒªå…«æˆ’åœ¨æ—è¾¹è¯´ï¼š"å¤§å¸ˆå…„ï¼Œæˆ‘ä»¬å¿«é€ƒå§ï¼"
    å­™æ‚Ÿç©ºå†·ç¬‘ä¸€å£°ï¼š"å‘†å­ï¼Œæˆ‘ä»¬æ˜¯å»é™¤å¦–çš„ï¼"
    """
    
    # ç®€å•çš„è§’è‰²æå–
    import re
    
    # æå–è¯´è¯çš„è§’è‰²
    speakers = re.findall(r'(\w+)(?:è¯´é“|æƒŠé“|å†·ç¬‘|è¯´)ï¼š', test_text)
    characters = list(set(speakers))
    
    print(f"   ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text.strip()}")
    print(f"   ğŸ­ æå–çš„è§’è‰²: {characters}")
    
    # ç®€å•åˆ†æ®µ
    sentences = [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿ]', test_text) if s.strip()]
    print(f"   ğŸ“„ åˆ†æ®µæ•°: {len(sentences)}")
    
    for i, sentence in enumerate(sentences[:3]):
        print(f"      {i+1}. {sentence}")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è§’è‰²åˆ†ææµ‹è¯•...")
    
    # å…ˆæµ‹è¯•ç®€å•åˆ†æ
    test_simple_analysis()
    
    # å†æµ‹è¯•å®Œæ•´åˆ†æ
    asyncio.run(test_character_analysis())
    
    print("\nâœ¨ æµ‹è¯•å®Œæˆï¼") 