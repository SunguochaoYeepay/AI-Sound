"""
ç« èŠ‚ç®¡ç†API
æä¾›ä¹¦ç±ç« èŠ‚ç®¡ç†åŠŸèƒ½
"""

from fastapi import APIRouter, Depends, HTTPException, Query, Form, UploadFile, File
from sqlalchemy.orm import Session
from sqlalchemy import desc, asc, func, or_, and_
from typing import List, Optional, Dict, Any
import json
import logging
from datetime import datetime
import re
import requests
import os

from app.database import get_db
from app.models import BookChapter, Book  # TextSegmentå·²åºŸå¼ƒ
from app.utils import log_system_event
from app.services.content_preparation_service import ContentPreparationService

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/chapters", tags=["Chapters"])

class ProgrammaticCharacterDetector:
    """
    ç¼–ç¨‹è§„åˆ™è§’è‰²è¯†åˆ«å™¨ - å¯å¤ç”¨çš„è§’è‰²è¯†åˆ«å¼•æ“
    åŸºäºå°è¯´è§’è‰²ç¼–ç¨‹è¯†åˆ«è§„åˆ™.mdçš„å®ç°
    """
    
    def __init__(self):
        # å¯¹è¯æ ‡è¯†ç¬¦æ¨¡å¼
        self.dialogue_patterns = {
            'direct_quote': [
                r'^([^""''ã€Œã€ã€ã€ï¼š:ï¼Œã€‚ï¼ï¼Ÿ\s]{2,6})[è¯´é“è®²å«å–Šé—®ç­”å›å¤è¡¨ç¤º][:ï¼š]\s*[""''ã€Œã€ã€ã€]',
                r'^([^""''ã€Œã€ã€ã€ï¼š:ï¼Œã€‚ï¼ï¼Ÿ\s]{2,6})[è¯´é“è®²å«å–Šé—®ç­”å›å¤è¡¨ç¤º]ï¼Œ\s*[""''ã€Œã€ã€ã€]',
                r'^([^""''ã€Œã€ã€ã€ï¼š:ï¼Œã€‚ï¼ï¼Ÿ\s]{2,6})[:ï¼š]\s*[""''ã€Œã€ã€ã€]'
            ],
            'colon_marker': [
                r'^([^ï¼š:ï¼Œã€‚ï¼ï¼Ÿ\s]{2,6})[:ï¼š]'
            ],
            'quote_dialogue': [
                r'^([^""''ã€Œã€ã€ã€ï¼Œã€‚ï¼ï¼Ÿ\s]{2,6})[^""''ã€Œã€ã€ã€]{0,10}[""''ã€Œã€ã€ã€]',
                r'[""''ã€Œã€ã€ã€][^""''ã€Œã€ã€ã€]+[""''ã€Œã€ã€ã€][^ï¼Œã€‚ï¼ï¼Ÿ]*?([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,6})[è¯´é“]'
            ],
            'mixed_separation': [
                r'^(.+?)([ä¸€-é¾¯]{2,4}[è¯´é“è®²å«å–Šé—®ç­”å›å¤è¡¨ç¤ºè‡ªè¨€è‡ªè¯­][:ï¼š])\s*[""''ã€Œã€ã€ã€](.+?)[""''ã€Œã€ã€ã€](.*)$'
            ]
        }
        
        # æ’é™¤è¯æ±‡
        self.excluded_words = [
            'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'å“ªé‡Œ', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ',
            'å¯æ˜¯', 'ä½†æ˜¯', 'æ‰€ä»¥', 'å› ä¸º', 'å¦‚æœ', 'è™½ç„¶',
            'é‡åˆ°', 'æ…¢æ…¢', 'è€Œè¿™', 'è¿™ä¸€', 'é‚£ä¸€', 'å½“ä»–', 'å½“å¥¹',
            'æ­¤æ—¶', 'æ­¤å', 'ç„¶å', 'æ¥ç€', 'æœ€å', 'ä»é‚£', 'ç»è¿‡',
            'ç¥å¥‡', 'åœ¨ä¸€', 'æ­£å‘', 'æ— å¥ˆ', 'å°½ç®¡'
        ]
        
        # å™è¿°è¯æ±‡
        self.narrative_words = [
            'åªè§', 'å¿½ç„¶', 'æ­¤æ—¶', 'è¿™æ—¶', 'çªç„¶', 'æ¥ç€', 'ç„¶å', 
            'äºæ˜¯', 'ä¸€å¤©', 'å¸ˆå¾’', 'å±±åŠ¿', 'å³°å²©', 'è¯è¯´', 'å´è¯´'
        ]
    
    def segment_text_with_speakers(self, text: str) -> List[Dict]:
        """å°†æ–‡æœ¬åˆ†æ®µå¹¶è¯†åˆ«è¯´è¯è€…"""
        segments = []
        
        # æŒ‰å¥å·åˆ†å‰²æ–‡æœ¬
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        
        for i, sentence in enumerate(sentences):
            segment_info = self.identify_speaker(sentence)
            segments.append({
                'order': i + 1,
                'text': sentence + 'ã€‚',
                'speaker': segment_info['speaker'],
                'confidence': segment_info['confidence'],
                'detection_rule': segment_info['rule'],
                'text_type': segment_info['text_type']
            })
        
        return segments
    
    def identify_speaker(self, text: str) -> Dict:
        """è¯†åˆ«å•ä¸ªå¥å­çš„è¯´è¯è€…"""
        
        # è§„åˆ™1: æ··åˆæ–‡æœ¬åˆ†ç¦»æ¨¡å¼
        mixed_result = self.detect_mixed_text(text)
        if mixed_result:
            return mixed_result
        
        # è§„åˆ™2: ç›´æ¥å¼•è¯­æ¨¡å¼ - å¢å¼ºç‰ˆ
        direct_patterns = [
            # æ ‡å‡†æ ¼å¼: è§’è‰²å+è¯´è¯åŠ¨è¯+å†’å·+å¼•å·
            r'^([ä¸€-é¾¯]{2,6})[è¯´é“è®²å«å–Šé—®ç­”å›å¤è¡¨ç¤º][:ï¼š]\s*[""''ã€Œã€ã€ã€]',
            # å¸¦é€—å·æ ¼å¼: è§’è‰²å+è¯´è¯åŠ¨è¯+é€—å·+å¼•å·  
            r'^([ä¸€-é¾¯]{2,6})[è¯´é“è®²å«å–Šé—®ç­”å›å¤è¡¨ç¤º]ï¼Œ\s*[""''ã€Œã€ã€ã€]',
            # ç®€åŒ–æ ¼å¼: è§’è‰²å+å†’å·+å¼•å·
            r'^([ä¸€-é¾¯]{2,6})[:ï¼š]\s*[""''ã€Œã€ã€ã€]',
            # åç½®æ ¼å¼: å¼•å·+å†…å®¹+å¼•å·+è§’è‰²å+è¯´è¯åŠ¨è¯
            r'[""''ã€Œã€ã€ã€][^""''ã€Œã€ã€ã€]+[""''ã€Œã€ã€ã€][^ï¼Œã€‚ï¼ï¼Ÿ]*?([ä¸€-é¾¯]{2,6})[è¯´é“è®²å«å–Šé—®ç­”å›å¤è¡¨ç¤º]'
        ]
        
        for pattern in direct_patterns:
            match = re.search(pattern, text)
            if match:
                potential_speaker = match.group(1)
                # ä»æ½œåœ¨è¯´è¯è€…ä¸­æå–è§’è‰²å
                speaker = self._extract_character_name_from_action(potential_speaker)
                if not speaker:
                    speaker = potential_speaker  # å¦‚æœæå–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åŒ¹é…
                
                if self.is_valid_character_name(speaker):
                    return {
                        'speaker': speaker,
                        'confidence': 0.95,
                        'rule': 'direct_quote',
                        'text_type': 'dialogue'
                    }
        
        # è§„åˆ™3: å¯¹è¯æ ‡è®°æ¨¡å¼
        for pattern in self.dialogue_patterns['colon_marker']:
            match = re.search(pattern, text)
            if match:
                speaker = match.group(1)
                if self.is_valid_character_name(speaker):
                    return {
                        'speaker': speaker,
                        'confidence': 0.9,
                        'rule': 'colon_marker',
                        'text_type': 'dialogue'
                    }
        
        # è§„åˆ™4: å¼•å·å¯¹è¯æ¨¡å¼
        if any(quote in text for quote in ['"', '"', '"', 'ã€Œ', 'ã€', 'ã€', 'ã€', "'", "'"]):
            for pattern in self.dialogue_patterns['quote_dialogue']:
                match = re.search(pattern, text)
                if match:
                    speaker = match.group(1)
                    if self.is_valid_character_name(speaker):
                        return {
                            'speaker': speaker,
                            'confidence': 0.85,
                            'rule': 'quote_dialogue',
                            'text_type': 'dialogue'
                        }
        
        # è§„åˆ™5: æ—ç™½è¯†åˆ«æ¨¡å¼
        return self.detect_narration(text)
    
    def detect_mixed_text(self, text: str) -> Optional[Dict]:
        """æ£€æµ‹æ··åˆæ–‡æœ¬ï¼ˆå™è¿°+å¯¹è¯ï¼‰"""
        # å…ˆå°è¯•æå–è¯´è¯åŠ¨ä½œéƒ¨åˆ† - æ›´ç²¾ç¡®çš„æ¨¡å¼
        action_patterns = [
            # æ¨¡å¼1: è§’è‰²å+ä¿®é¥°è¯+è¯´è¯åŠ¨è¯+å†’å·+å¼•å·
            r'([ä¸€-é¾¯]{2,6})[^ï¼Œã€‚ï¼ï¼Ÿ]*?[è¯´é“è®²å«å–Šé—®ç­”å›å¤è¡¨ç¤º][:ï¼š]\s*[""''ã€Œã€ã€ã€]',
            # æ¨¡å¼2: è§’è‰²å+è‡ªè¨€è‡ªè¯­+å†’å·+å¼•å· (ç‰¹æ®Šå¤„ç†)
            r'([ä¸€-é¾¯]{2,6})[^ï¼Œã€‚ï¼ï¼Ÿ]*?è‡ªè¨€è‡ªè¯­é“[:ï¼š]\s*[""''ã€Œã€ã€ã€]'
        ]
        
        for pattern in action_patterns:
            match = re.search(pattern, text)
            if match:
                potential_speaker = match.group(1)
                
                # ä»æ½œåœ¨è¯´è¯è€…ä¸­æå–çœŸæ­£çš„è§’è‰²å
                speaker_name = self._extract_character_name_from_action(potential_speaker)
                
                if speaker_name and self.is_valid_character_name(speaker_name):
                    return {
                        'speaker': speaker_name,
                        'confidence': 0.95,
                        'rule': 'mixed_separation',
                        'text_type': 'dialogue'
                    }
        return None
    
    def _extract_character_name_from_action(self, action_text: str) -> Optional[str]:
        """ä»è¯´è¯åŠ¨ä½œæ–‡æœ¬ä¸­æå–è§’è‰²å"""
        # å¸¸è§çš„è§’è‰²åæ¨¡å¼
        character_patterns = [
            # ç›´æ¥åŒ¹é…å¸¸è§è§’è‰²å
            r'(å­™æ‚Ÿç©º|å”åƒ§|çŒªå…«æˆ’|æ²™åƒ§|ç™½éª¨ç²¾|è§‚éŸ³|å¦‚æ¥|ç‰å¸)',
            # åŒ¹é…ä»¥ç‰¹å®šå­—å¼€å¤´çš„è§’è‰²å
            r'(ç™½[ä¸€-é¾¯]{1,2})',  # ç™½éª¨ç²¾ã€ç™½å¨˜å­ç­‰
            r'(å­™[ä¸€-é¾¯]{1,2})',  # å­™æ‚Ÿç©ºç­‰
            r'(å”[ä¸€-é¾¯]{0,2})',  # å”åƒ§ç­‰
            # é€šç”¨æ¨¡å¼ï¼šå»æ‰ä¿®é¥°è¯åçš„2-4å­—è§’è‰²å
            r'(?:ä¸èƒœ|ååˆ†|éå¸¸|å¾ˆæ˜¯|é¢‡ä¸º|ç”šæ˜¯|æå…¶)?([ä¸€-é¾¯]{2,4})(?:ä¸èƒœ|ååˆ†|éå¸¸|å¾ˆæ˜¯|é¢‡ä¸º|ç”šæ˜¯|æå…¶|æ¬¢å–œ|æ„¤æ€’|é«˜å…´|æ‚²ä¼¤|æƒŠè®¶|å®³æ€•|ç€æ€¥|ç„¦æ€¥)?'
        ]
        
        for pattern in character_patterns:
            match = re.search(pattern, action_text)
            if match:
                candidate = match.group(1)
                # éªŒè¯å€™é€‰è§’è‰²å
                if self.is_valid_character_name(candidate):
                    return candidate
        
        return None
    
    def detect_narration(self, text: str) -> Dict:
        """æ£€æµ‹æ—ç™½/å™è¿°æ–‡æœ¬"""
        # 1. ä¸åŒ…å«ä»»ä½•å¯¹è¯æ ‡è®°çš„æ–‡æœ¬
        has_dialogue_markers = bool(re.search(r'[""''ã€Œã€ã€ã€ï¼š:][è¯´é“è®²å«å–Šé—®ç­”å›å¤è¡¨ç¤º]', text))
        
        # 2. ä¸ä»¥è§’è‰²åå¼€å¤´çš„æè¿°æ€§æ–‡æœ¬  
        starts_with_character = bool(re.search(r'^[ä¸€-é¾¯]{2,4}[è¯´é“è®²å«å–Šé—®ç­”å›å¤è¡¨ç¤º]', text))
        
        # 3. åŒ…å«æè¿°æ€§è¯æ±‡çš„æ–‡æœ¬
        has_narrative_words = any(word in text for word in self.narrative_words)
        
        if not has_dialogue_markers and not starts_with_character and (has_narrative_words or len(text) > 50):
            return {
                'speaker': 'æ—ç™½',
                'confidence': 0.9,
                'rule': 'narration',
                'text_type': 'narration'
            }
        
        # é»˜è®¤å½’ç±»ä¸ºæ—ç™½
        return {
            'speaker': 'æ—ç™½',
            'confidence': 0.7,
            'rule': 'default_narration',
            'text_type': 'narration'
        }
    
    def is_valid_character_name(self, name: str) -> bool:
        """éªŒè¯è§’è‰²åæ˜¯å¦æœ‰æ•ˆ"""
        if not name or len(name) < 2 or len(name) > 6:
            return False
        
        if name in self.excluded_words:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ ‡ç‚¹ç¬¦å·
        if any(punct in name for punct in ['ã€‚', 'ï¼Œ', 'ï¼', 'ï¼Ÿ', 'ï¼›']):
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºä¸­æ–‡å§“å
        if not re.match(r'^[ä¸€-é¾¯]{2,6}$', name):
            return False
        
        return True
    
    def extract_dialogue_characters(self, segments: List[Dict]) -> Dict[str, int]:
        """æå–æœ‰å¯¹è¯çš„è§’è‰²åŠå…¶é¢‘æ¬¡"""
        dialogue_characters = {}
        
        for segment in segments:
            if segment['text_type'] == 'dialogue' and segment['speaker'] != 'æ—ç™½':
                speaker = segment['speaker']
                dialogue_characters[speaker] = dialogue_characters.get(speaker, 0) + 1
        
        return dialogue_characters
    
    def analyze_text_segments(self, text: str) -> Dict:
        """åˆ†ææ–‡æœ¬å¹¶è¿”å›å®Œæ•´çš„åˆ†æ®µå’Œè§’è‰²ä¿¡æ¯"""
        segments = self.segment_text_with_speakers(text)
        dialogue_characters = self.extract_dialogue_characters(segments)
        
        # ç»Ÿè®¡æ—ç™½æ®µè½
        narrator_segments = [s for s in segments if s['speaker'] == 'æ—ç™½']
        
        # æ„å»ºè§’è‰²åˆ—è¡¨ï¼ˆåªåŒ…å«æœ‰å¯¹è¯çš„è§’è‰² + æ—ç™½ï¼‰
        characters = []
        
        # æ·»åŠ å¯¹è¯è§’è‰²
        for char_name, frequency in dialogue_characters.items():
            characters.append({
                'name': char_name,
                'frequency': frequency,
                'character_trait': {
                    'trait': 'calm',  # é»˜è®¤æ€§æ ¼
                    'confidence': 0.8,
                    'description': f'{char_name}è§’è‰²ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†ææ€§æ ¼ç‰¹å¾'
                },
                'first_appearance': 1,
                'is_main_character': frequency >= 3,
                'recommended_config': {
                    'gender': 'unknown',
                    'personality': 'calm',
                    'personality_description': 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ',
                    'personality_confidence': 0.8,
                    'description': f'{char_name}è§’è‰²ï¼Œåœ¨æ–‡æœ¬ä¸­æœ‰{frequency}æ¬¡å¯¹è¯ã€‚',
                    'recommended_tts_params': {'time_step': 32, 'p_w': 1.4, 't_w': 3.0},
                    'voice_type': 'unknown',
                    'color': '#007bff'
                }
            })
        
        # æ·»åŠ æ—ç™½è§’è‰²ï¼ˆå¦‚æœæœ‰æ—ç™½å†…å®¹ï¼‰
        if narrator_segments:
            characters.append({
                'name': 'æ—ç™½',
                'frequency': len(narrator_segments),
                'character_trait': {
                    'trait': 'calm',
                    'confidence': 1.0,
                    'description': 'å°è¯´å™è¿°è€…ï¼Œè´Ÿè´£æè¿°åœºæ™¯å’Œæƒ…èŠ‚å‘å±•'
                },
                'first_appearance': 1,
                'is_main_character': True,
                'recommended_config': {
                    'gender': 'neutral',
                    'personality': 'calm',
                    'personality_description': 'ä¸“ä¸šæ—ç™½ï¼Œå£°éŸ³æ¸…æ™°ç¨³å®šï¼Œé€‚åˆå™è¿°',
                    'personality_confidence': 1.0,
                    'description': 'æ—ç™½è§’è‰²ï¼Œè´Ÿè´£å°è¯´çš„å™è¿°éƒ¨åˆ†ï¼Œéœ€è¦ä¸“ä¸šã€æ¸…æ™°çš„å£°éŸ³ã€‚',
                    'recommended_tts_params': {'time_step': 32, 'p_w': 1.4, 't_w': 3.0},
                    'voice_type': 'narrator',
                    'color': '#6c757d'
                }
            })
        
        return {
            'segments': segments,
            'detected_characters': characters,
            'processing_stats': {
                'total_segments': len(segments),
                'dialogue_segments': len([s for s in segments if s['text_type'] == 'dialogue']),
                'narration_segments': len(narrator_segments),
                'characters_found': len(characters),
                'analysis_method': 'programming_rules'
            }
        }

@router.post("")
async def create_chapter(
    book_id: int = Form(..., description="ä¹¦ç±ID"),
    title: str = Form(..., description="ç« èŠ‚æ ‡é¢˜"),
    content: str = Form(..., description="ç« èŠ‚å†…å®¹"),
    chapter_number: Optional[int] = Form(None, description="ç« èŠ‚åºå·"),
    db: Session = Depends(get_db)
):
    """åˆ›å»ºæ–°ç« èŠ‚"""
    try:
        # æ£€æŸ¥ä¹¦ç±æ˜¯å¦å­˜åœ¨
        book = db.query(Book).filter(Book.id == book_id).first()
        if not book:
            raise HTTPException(status_code=404, detail="ä¹¦ç±ä¸å­˜åœ¨")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç« èŠ‚åºå·ï¼Œè‡ªåŠ¨åˆ†é…
        if chapter_number is None:
            max_chapter = db.query(func.max(BookChapter.chapter_number)).filter(
                BookChapter.book_id == book_id
            ).scalar()
            chapter_number = (max_chapter or 0) + 1
        else:
            # æ£€æŸ¥ç« èŠ‚åºå·æ˜¯å¦å·²å­˜åœ¨
            existing_chapter = db.query(BookChapter).filter(
                BookChapter.book_id == book_id,
                BookChapter.chapter_number == chapter_number
            ).first()
            if existing_chapter:
                raise HTTPException(status_code=400, detail=f"ç« èŠ‚åºå· {chapter_number} å·²å­˜åœ¨")
        
        # åˆ›å»ºæ–°ç« èŠ‚
        new_chapter = BookChapter(
            book_id=book_id,
            chapter_number=chapter_number,
            chapter_title=title.strip(),
            content=content,
            word_count=len(content.strip()),
            analysis_status='pending',
            synthesis_status='pending',
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
        db.add(new_chapter)
        db.commit()
        db.refresh(new_chapter)
        
        # æ›´æ–°ä¹¦ç±çš„ç« èŠ‚æ•°
        chapter_count = db.query(BookChapter).filter(BookChapter.book_id == book_id).count()
        book.chapter_count = chapter_count
        db.commit()
        
        # è®°å½•åˆ›å»ºæ—¥å¿—
        await log_system_event(
            db=db,
            level="info",
            message=f"æ–°ç« èŠ‚åˆ›å»º: {title}",
            module="chapters",
            details={"chapter_id": new_chapter.id, "book_id": book_id}
        )
        
        return {
            "success": True,
            "message": "ç« èŠ‚åˆ›å»ºæˆåŠŸ",
            "data": new_chapter.to_dict()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ›å»ºç« èŠ‚å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºç« èŠ‚å¤±è´¥: {str(e)}")

@router.get("")
async def get_chapters(
    book_id: Optional[int] = Query(None, description="ä¹¦ç±IDè¿‡æ»¤"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    search: str = Query("", description="æœç´¢å…³é”®è¯"),
    status: str = Query("", description="çŠ¶æ€è¿‡æ»¤"),
    sort_by: str = Query("chapter_number", description="æ’åºå­—æ®µ"),
    sort_order: str = Query("asc", description="æ’åºæ–¹å‘"),
    db: Session = Depends(get_db)
):
    """è·å–ç« èŠ‚åˆ—è¡¨"""
    try:
        # æ„å»ºæŸ¥è¯¢
        query = db.query(BookChapter)
        
        # ä¹¦ç±è¿‡æ»¤
        if book_id:
            query = query.filter(BookChapter.book_id == book_id)
        
        # æœç´¢è¿‡æ»¤
        if search:
            search_pattern = f"%{search}%"
            query = query.filter(
                or_(
                    BookChapter.title.like(search_pattern),
                    BookChapter.content.like(search_pattern)
                )
            )
        
        # çŠ¶æ€è¿‡æ»¤
        if status:
            query = query.filter(BookChapter.analysis_status == status)
        
        # æ’åº
        sort_field = getattr(BookChapter, sort_by, BookChapter.chapter_number)
        if sort_order == "asc":
            query = query.order_by(asc(sort_field))
        else:
            query = query.order_by(desc(sort_field))
        
        # ç»Ÿè®¡æ€»æ•°
        total = query.count()
        
        # åˆ†é¡µ
        offset = (page - 1) * page_size
        chapters = query.offset(offset).limit(page_size).all()
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        chapter_list = []
        for chapter in chapters:
            chapter_data = chapter.to_dict()
            
            # æ·»åŠ ä¹¦ç±ä¿¡æ¯
            if chapter.book:
                chapter_data['book'] = {
                    "id": chapter.book.id,
                    "title": chapter.book.title,
                    "author": chapter.book.author
                }
            
            chapter_list.append(chapter_data)
        
        # åˆ†é¡µä¿¡æ¯
        total_pages = (total + page_size - 1) // page_size
        
        return {
            "success": True,
            "data": chapter_list,
            "pagination": {
                "page": page,
                "pageSize": page_size,
                "total": total,
                "totalPages": total_pages,
                "hasMore": page < total_pages
            },
            "filters": {
                "book_id": book_id,
                "search": search,
                "status": status
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.get("/{chapter_id}")
async def get_chapter(
    chapter_id: int,
    db: Session = Depends(get_db)
):
    """è·å–ç« èŠ‚è¯¦æƒ…"""
    try:
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        chapter_data = chapter.to_dict()
        
        # æ·»åŠ ä¹¦ç±ä¿¡æ¯
        if chapter.book:
            chapter_data['book'] = {
                "id": chapter.book.id,
                "title": chapter.book.title,
                "author": chapter.book.author
            }
        
        # æ·»åŠ å…³è”çš„æ–‡æœ¬åˆ†æ®µä¿¡æ¯
        # æ³¨æ„ï¼šTextSegmentæ¨¡å‹ä½¿ç”¨project_idè€Œä¸æ˜¯chapter_idï¼Œè¿™é‡Œå…ˆè¿”å›ç©ºåˆ—è¡¨
        segments = []
        
        chapter_data['segments'] = [
            {
                "id": seg.id,
                "order": seg.segment_order,
                "text": seg.text_content[:100] + "..." if len(seg.text_content) > 100 else seg.text_content,
                "status": seg.status,
                "detected_speaker": getattr(seg, 'detected_speaker', None)
            }
            for seg in segments
        ]
        
        return {
            "success": True,
            "data": chapter_data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ç« èŠ‚è¯¦æƒ…å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç« èŠ‚è¯¦æƒ…å¤±è´¥: {str(e)}")

@router.patch("/{chapter_id}")
async def update_chapter(
    chapter_id: int,
    title: Optional[str] = Form(None),
    content: Optional[str] = Form(None),
    analysis_status: Optional[str] = Form(None),
    db: Session = Depends(get_db)
):
    """æ›´æ–°ç« èŠ‚ä¿¡æ¯"""
    try:
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # æ›´æ–°å­—æ®µ
        if title is not None:
            if not title.strip():
                raise HTTPException(status_code=400, detail="ç« èŠ‚æ ‡é¢˜ä¸èƒ½ä¸ºç©º")
            chapter.chapter_title = title.strip()
        
        if content is not None:
            chapter.content = content
            # æ›´æ–°å­—æ•°ç»Ÿè®¡
            chapter.word_count = len(content.strip())
        
        if analysis_status is not None:
            if analysis_status not in ['pending', 'processing', 'completed', 'failed']:
                raise HTTPException(status_code=400, detail="æ— æ•ˆçš„åˆ†æçŠ¶æ€")
            chapter.analysis_status = analysis_status
        
        chapter.updated_at = datetime.utcnow()
        db.commit()
        
        # è®°å½•æ›´æ–°æ—¥å¿—
        await log_system_event(
            db=db,
            level="info",
            message=f"ç« èŠ‚æ›´æ–°: {chapter.chapter_title}",
            module="chapters",
            details={"chapter_id": chapter_id}
        )
        
        return {
            "success": True,
            "message": "ç« èŠ‚æ›´æ–°æˆåŠŸ",
            "data": chapter.to_dict()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°ç« èŠ‚å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±è´¥: {str(e)}")

@router.delete("/{chapter_id}")
async def delete_chapter(
    chapter_id: int,
    force: bool = Query(False, description="å¼ºåˆ¶åˆ é™¤"),
    db: Session = Depends(get_db)
):
    """åˆ é™¤ç« èŠ‚"""
    try:
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # æ³¨æ„ï¼šTextSegmentæ¨¡å‹ä½¿ç”¨project_idè€Œä¸æ˜¯chapter_idï¼Œè¿™é‡Œè·³è¿‡åˆ†æ®µæ£€æŸ¥
        chapter_title = chapter.chapter_title
        
        # åˆ é™¤ç« èŠ‚
        db.delete(chapter)
        db.commit()
        
        # è®°å½•åˆ é™¤æ—¥å¿—
        await log_system_event(
            db=db,
            level="info",
            message=f"ç« èŠ‚åˆ é™¤: {chapter_title}",
            module="chapters",
            details={"chapter_id": chapter_id, "force": force}
        )
        
        return {
            "success": True,
            "message": "ç« èŠ‚åˆ é™¤æˆåŠŸ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤ç« èŠ‚å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")

@router.post("/{chapter_id}/split")
async def split_chapter(
    chapter_id: int,
    split_position: int = Form(..., description="åˆ†å‰²ä½ç½®"),
    new_title: str = Form(..., description="æ–°ç« èŠ‚æ ‡é¢˜"),
    db: Session = Depends(get_db)
):
    """åˆ†å‰²ç« èŠ‚"""
    try:
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        if not chapter.content:
            raise HTTPException(status_code=400, detail="ç« èŠ‚å†…å®¹ä¸ºç©ºï¼Œæ— æ³•åˆ†å‰²")
        
        content_length = len(chapter.content)
        if split_position <= 0 or split_position >= content_length:
            raise HTTPException(status_code=400, detail="åˆ†å‰²ä½ç½®æ— æ•ˆ")
        
        if not new_title.strip():
            raise HTTPException(status_code=400, detail="æ–°ç« èŠ‚æ ‡é¢˜ä¸èƒ½ä¸ºç©º")
        
        # åˆ†å‰²å†…å®¹
        original_content = chapter.content[:split_position]
        new_content = chapter.content[split_position:]
        
        # æ›´æ–°åŸç« èŠ‚
        chapter.content = original_content
        chapter.word_count = len(original_content.strip())
        chapter.updated_at = datetime.utcnow()
        
        # åˆ›å»ºæ–°ç« èŠ‚
        new_chapter = BookChapter(
            book_id=chapter.book_id,
            chapter_number=chapter.chapter_number + 1,
            chapter_title=new_title.strip(),
            content=new_content,
            word_count=len(new_content.strip()),
            analysis_status='pending',
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
        # æ›´æ–°åç»­ç« èŠ‚çš„ç¼–å·
        db.query(BookChapter).filter(
            BookChapter.book_id == chapter.book_id,
            BookChapter.chapter_number > chapter.chapter_number
        ).update({
            BookChapter.chapter_number: BookChapter.chapter_number + 1
        })
        
        db.add(new_chapter)
        db.commit()
        db.refresh(new_chapter)
        
        # è®°å½•åˆ†å‰²æ—¥å¿—
        await log_system_event(
            db=db,
            level="info",
            message=f"ç« èŠ‚åˆ†å‰²: {chapter.chapter_title} -> {new_title}",
            module="chapters",
            details={
                "original_chapter_id": chapter_id,
                "new_chapter_id": new_chapter.id,
                "split_position": split_position
            }
        )
        
        return {
            "success": True,
            "message": "ç« èŠ‚åˆ†å‰²æˆåŠŸ",
            "data": {
                "original_chapter": chapter.to_dict(),
                "new_chapter": new_chapter.to_dict()
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ†å‰²ç« èŠ‚å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ†å‰²å¤±è´¥: {str(e)}")

@router.post("/{chapter_id}/merge")
async def merge_chapters(
    chapter_id: int,
    target_chapter_id: int = Form(..., description="ç›®æ ‡ç« èŠ‚ID"),
    merge_direction: str = Form("after", description="åˆå¹¶æ–¹å‘: before/after"),
    db: Session = Depends(get_db)
):
    """åˆå¹¶ç« èŠ‚"""
    try:
        if chapter_id == target_chapter_id:
            raise HTTPException(status_code=400, detail="ä¸èƒ½ä¸è‡ªå·±åˆå¹¶")
        
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        target_chapter = db.query(BookChapter).filter(BookChapter.id == target_chapter_id).first()
        
        if not chapter:
            raise HTTPException(status_code=404, detail="æºç« èŠ‚ä¸å­˜åœ¨")
        if not target_chapter:
            raise HTTPException(status_code=404, detail="ç›®æ ‡ç« èŠ‚ä¸å­˜åœ¨")
        
        if chapter.book_id != target_chapter.book_id:
            raise HTTPException(status_code=400, detail="åªèƒ½åˆå¹¶åŒä¸€æœ¬ä¹¦çš„ç« èŠ‚")
        
        if merge_direction not in ["before", "after"]:
            raise HTTPException(status_code=400, detail="åˆå¹¶æ–¹å‘å¿…é¡»æ˜¯ before æˆ– after")
        
        # åˆå¹¶å†…å®¹
        if merge_direction == "after":
            # å°†ç›®æ ‡ç« èŠ‚å†…å®¹åˆå¹¶åˆ°å½“å‰ç« èŠ‚åé¢
            merged_content = chapter.content + "\n\n" + target_chapter.content
            merged_title = chapter.title
            keep_chapter = chapter
            delete_chapter = target_chapter
        else:
            # å°†å½“å‰ç« èŠ‚å†…å®¹åˆå¹¶åˆ°ç›®æ ‡ç« èŠ‚å‰é¢
            merged_content = chapter.content + "\n\n" + target_chapter.content
            merged_title = target_chapter.title
            keep_chapter = target_chapter
            delete_chapter = chapter
        
        # æ›´æ–°ä¿ç•™çš„ç« èŠ‚
        keep_chapter.content = merged_content
        keep_chapter.word_count = len(merged_content.strip())
        keep_chapter.updated_at = datetime.utcnow()
        
        # åˆ é™¤åˆå¹¶çš„ç« èŠ‚
        delete_chapter_number = delete_chapter.chapter_number
        db.delete(delete_chapter)
        
        # æ›´æ–°åç»­ç« èŠ‚ç¼–å·
        db.query(BookChapter).filter(
            BookChapter.book_id == chapter.book_id,
            BookChapter.chapter_number > delete_chapter_number
        ).update({
            BookChapter.chapter_number: BookChapter.chapter_number - 1
        })
        
        db.commit()
        
        # è®°å½•åˆå¹¶æ—¥å¿—
        await log_system_event(
            db=db,
            level="info",
            message=f"ç« èŠ‚åˆå¹¶: {chapter.title} + {target_chapter.title}",
            module="chapters",
            details={
                "chapter_id": chapter_id,
                "target_chapter_id": target_chapter_id,
                "merge_direction": merge_direction,
                "kept_chapter_id": keep_chapter.id
            }
        )
        
        return {
            "success": True,
            "message": "ç« èŠ‚åˆå¹¶æˆåŠŸ",
            "data": keep_chapter.to_dict()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆå¹¶ç« èŠ‚å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆå¹¶å¤±è´¥: {str(e)}")

@router.get("/{chapter_id}/statistics")
async def get_chapter_statistics(
    chapter_id: int,
    db: Session = Depends(get_db)
):
    """è·å–ç« èŠ‚ç»Ÿè®¡ä¿¡æ¯"""
    try:
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # ğŸš€ æ–°æ¶æ„ï¼šä¸å†ä½¿ç”¨TextSegmentåˆ†æ®µç»Ÿè®¡
        # å› ä¸ºæ–°æ¶æ„ç›´æ¥åŸºäºæ™ºèƒ½å‡†å¤‡ç»“æœåˆæˆï¼Œç« èŠ‚ä¸ä¾èµ–TextSegment
        status_counts = {}
        total_segments = 0
        
        return {
            "success": True,
            "data": {
                "chapter_id": chapter_id,
                "title": chapter.title,
                "word_count": chapter.word_count,
                "total_segments": total_segments,
                "segment_status_counts": status_counts,
                "analysis_status": chapter.analysis_status,
                "created_at": chapter.created_at.isoformat() if chapter.created_at else None,
                "updated_at": chapter.updated_at.isoformat() if chapter.updated_at else None
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ç« èŠ‚ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}")

@router.post("/batch-character-analysis")
async def batch_character_analysis(
    request: dict,
    db: Session = Depends(get_db)
):
    """
    æ‰¹é‡åˆ†æç« èŠ‚ä¸­çš„è§’è‰²
    åŸºäºç¼–ç¨‹è¯†åˆ«è§„åˆ™ï¼Œä»é€‰å®šç« èŠ‚ä¸­å‘ç°æ‰€æœ‰è§’è‰²
    """
    try:
        chapter_ids = request.get("chapter_ids", [])
        detection_method = request.get("detection_method", "programming")
        emotion_detection = request.get("emotion_detection", True)
        
        if not chapter_ids:
            raise HTTPException(status_code=400, detail="æœªæä¾›ç« èŠ‚IDåˆ—è¡¨")
        
        results = []
        
        for chapter_id in chapter_ids:
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                logger.warning(f"ç« èŠ‚ {chapter_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
                
            # ä½¿ç”¨å¢å¼ºçš„è§’è‰²è¯†åˆ«åˆ†æç« èŠ‚
            analysis_result = await analyze_chapter_characters(
                chapter, 
                detection_method, 
                emotion_detection
            )
            
            results.append(analysis_result)
        
        return {
            "success": True,
            "data": results,
            "message": f"æˆåŠŸåˆ†æ {len(results)} ä¸ªç« èŠ‚ï¼Œå‘ç°è§’è‰²ä¿¡æ¯"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰¹é‡è§’è‰²åˆ†æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


async def analyze_chapter_characters(chapter: BookChapter, detection_method: str, emotion_detection: bool):
    """
    åˆ†æå•ä¸ªç« èŠ‚çš„è§’è‰²
    åŸºäºç¼–ç¨‹è¯†åˆ«è§„åˆ™å®ç° - å¢å¼ºç‰ˆ
    """
    try:
        logger.info(f"å¼€å§‹åˆ†æç« èŠ‚ {chapter.id}: {chapter.chapter_title}")
        
        content = chapter.content or ""
        if not content.strip():
            return {
                "chapter_id": chapter.id,
                "chapter_title": chapter.chapter_title,
                "chapter_number": chapter.chapter_number,
                "detected_characters": [],
                "segments": [],
                "processing_stats": {"total_segments": 0, "dialogue_segments": 0, "characters_found": 0}
            }
        
        # ä¼˜å…ˆä½¿ç”¨Ollamaæ™ºèƒ½æ£€æµ‹å™¨
        try:
            detector = OllamaCharacterDetector()
            logger.info(f"ä½¿ç”¨Ollama AIè¿›è¡Œè§’è‰²åˆ†æ")
        except Exception as e:
            logger.warning(f"Ollamaæ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™æ£€æµ‹å™¨: {str(e)}")
            detector = AdvancedCharacterDetector()
        
        # æ‰§è¡Œè§’è‰²åˆ†æ
        if isinstance(detector, OllamaCharacterDetector):
            analysis_result = await detector.analyze_text(content, {
                'chapter_id': chapter.id,
                'chapter_title': chapter.chapter_title,
                'chapter_number': chapter.chapter_number
            })
        else:
            analysis_result = detector.analyze_text(content, {
                'chapter_id': chapter.id,
                'chapter_title': chapter.chapter_title,
                'chapter_number': chapter.chapter_number
            })
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"åˆ†æç« èŠ‚ {chapter.id} å¤±è´¥: {str(e)}")
        return {
            "chapter_id": chapter.id,
            "chapter_title": chapter.chapter_title or "æœªçŸ¥ç« èŠ‚",
            "chapter_number": chapter.chapter_number,
            "detected_characters": [],
            "segments": [],
            "error": str(e)
        }


class AdvancedCharacterDetector:
    """
    é«˜çº§è§’è‰²æ£€æµ‹å™¨ - åŸºäºå¤šé‡è§„åˆ™å’Œå¯å‘å¼æ–¹æ³•
    """
    
    def __init__(self):
        # å¯¹è¯æ ‡è¯†ç¬¦æ¨¡å¼
        self.dialogue_patterns = [
            # ç›´æ¥å¼•è¯­æ¨¡å¼
            r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,4})[è¯´é“è®²é—®ç­”å›åº”å–Šå«å˜Ÿå›”å˜€å’•][:ï¼š]?"([^"]+)"',
            r'"([^"]+)"[ï¼Œã€‚]?([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,4})[è¯´é“è®²é—®ç­”]',
            
            # å†’å·å¯¹è¯æ¨¡å¼
            r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,4})[ï¼š:]"([^"]+)"',
            r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,4})[ï¼š:]\s*([^ï¼Œã€‚ï¼ï¼Ÿ\n]+)',
            
            # æ ‡è®°å¯¹è¯æ¨¡å¼
            r'ã€([^ã€‘]+)ã€‘[ï¼š:]?([^ï¼Œã€‚ï¼ï¼Ÿ\n]*)',
            r'ã€–([^ã€—]+)ã€—[ï¼š:]?([^ï¼Œã€‚ï¼ï¼Ÿ\n]*)',
            
            # åŠ¨ä½œæè¿°ä¸­çš„è§’è‰²
            r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,4})[èµ°æ¥å»åˆ°ç«™åèººè·‘è·³]',
            r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,4})[çœ‹è§å¬åˆ°æƒ³èµ·è®°å¾—]',
            r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,4})[ç¬‘å“­æ€’å–œæƒŠ]',
            
            # ç§°å‘¼æ¨¡å¼
            r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,4})[å¸ˆçˆ¶å¸ˆå‚…å¤§äººå…ˆç”Ÿå°å§]',
            r'[å¸ˆçˆ¶å¸ˆå‚…å¤§äººå…ˆç”Ÿå°å§]([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,4})',
        ]
        
        # æ’é™¤è¯æ±‡ - å¸¸è§çš„éè§’è‰²è¯æ±‡
        self.excluded_words = {
            'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'å“ªé‡Œ', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¯æ˜¯', 'ä½†æ˜¯', 'æ‰€ä»¥', 'å› ä¸º',
            'å¦‚æœ', 'è™½ç„¶', 'é‡åˆ°', 'æ…¢æ…¢', 'è€Œè¿™', 'è¿™ä¸€', 'é‚£ä¸€', 'å½“ä»–', 'å½“å¥¹', 'æ­¤æ—¶',
            'æ­¤å', 'ç„¶å', 'æ¥ç€', 'æœ€å', 'ä»é‚£', 'ç»è¿‡', 'ç¥å¥‡', 'åœ¨ä¸€', 'æ­£å‘', 'æ— å¥ˆ',
            'å°½ç®¡', 'è‡ªè¨€', 'å¿ƒæƒ³', 'æš—æƒ³', 'æš—é“', 'å¿ƒé“', 'æƒ³é“', 'æ€è€ƒ', 'çªç„¶', 'å¿½ç„¶',
            'åŸæ¥', 'æœç„¶', 'ç«Ÿç„¶', 'å±…ç„¶', 'å½“ç„¶', 'è‡ªç„¶', 'æ˜¾ç„¶', 'æ˜æ˜¾', 'æ¸…æ¥š', 'çŸ¥é“',
            'çœ‹åˆ°', 'å¬åˆ°', 'æ„Ÿåˆ°', 'è§‰å¾—', 'è®¤ä¸º', 'ä»¥ä¸º', 'ä¼¼ä¹', 'å¥½åƒ', 'ä»¿ä½›', 'çŠ¹å¦‚'
        }
        
        # è§’è‰²æ€§æ ¼å…³é”®è¯
        self.personality_keywords = {
            'gentle': ['æ¸©æŸ”', 'è½»å£°', 'æŸ”å£°', 'ç»†å£°', 'æ¸©å’Œ', 'å’Œè”¼', 'æ…ˆç¥¥', 'æ¸©æŸ”åœ°', 'è½»å£°é“'],
            'fierce': ['å‡¶çŒ›', 'æš´èº', 'ç²—æš´', 'å‡¶ç‹ ', 'ç‹‚æš´', 'éœ¸é“', 'æ€’å¼', 'å’†å“®', 'å¤§å–', 'å‰å£°'],
            'calm': ['æ²‰ç¨³', 'å†·é™', 'æ·¡å®š', 'ä»å®¹', 'é•‡å®š', 'å¹³é™', 'æ·¡æ·¡åœ°', 'å¹³é™åœ°', 'ä»å®¹è¯´'],
            'lively': ['æ´»æ³¼', 'å¼€æœ—', 'çˆ½æœ—', 'æ¬¢å¿«', 'å…´å¥‹', 'çƒ­æƒ…', 'å…´å¥‹åœ°', 'æ¬¢å¿«åœ°', 'çˆ½æœ—ç¬‘'],
            'wise': ['æ™ºæ…§', 'ç¿æ™º', 'èªæ˜', 'æœºæ™º', 'æ·±æ€', 'æ²‰æ€', 'æ€ç´¢', 'æ·±æ€ç†Ÿè™‘'],
            'brave': ['å‹‡æ•¢', 'è‹±å‹‡', 'æ— ç•', 'æœæ•¢', 'åšæ¯…', 'åˆšå¼º', 'å‹‡æ°”', 'èƒ†é‡']
        }
        
        # æ€§åˆ«æ¨æ–­å…³é”®è¯
        self.gender_keywords = {
            'male': ['å¸ˆçˆ¶', 'å¸ˆå‚…', 'å¤§äºº', 'å…ˆç”Ÿ', 'å…¬å­', 'å°‘çˆ·', 'è€çˆ·', 'çˆ·çˆ·', 'çˆ¶äº²', 'çˆ¸çˆ¸'],
            'female': ['å°å§', 'å§‘å¨˜', 'å¤«äºº', 'å¨˜å­', 'å¥³å£«', 'å¥¶å¥¶', 'æ¯äº²', 'å¦ˆå¦ˆ', 'é˜¿å§¨']
        }
    
    def analyze_text(self, text: str, chapter_info: dict):
        """åˆ†ææ–‡æœ¬ä¸­çš„è§’è‰²"""
        
        # 1. åˆ†æ®µå¤„ç†
        segments = self._split_into_segments(text)
        
        # 2. è§’è‰²æå–
        character_candidates = self._extract_characters(segments)
        
        # 3. è§’è‰²éªŒè¯å’Œè¿‡æ»¤
        valid_characters = self._validate_characters(character_candidates, text)
        
        # 4. è§’è‰²å±æ€§åˆ†æ
        analyzed_characters = self._analyze_character_attributes(valid_characters, text)
        
        # 5. æ„å»ºè¿”å›ç»“æœ
        return {
            "chapter_id": chapter_info['chapter_id'],
            "chapter_title": chapter_info['chapter_title'],
            "chapter_number": chapter_info['chapter_number'],
            "detected_characters": analyzed_characters,
            "segments": [],  # å¯ä»¥åç»­æ·»åŠ æ®µè½åˆ†æ
            "processing_stats": {
                "total_segments": len(segments),
                "dialogue_segments": len([s for s in segments if self._is_dialogue(s)]),
                "characters_found": len(analyzed_characters)
            }
        }
    
    def _split_into_segments(self, text: str):
        """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ®µè½"""
        import re
        # æŒ‰å¥å·ã€æ„Ÿå¹å·ã€é—®å·åˆ†å‰²ï¼Œä¿ç•™æ ‡ç‚¹
        segments = re.split(r'([ã€‚ï¼ï¼Ÿ])', text)
        
        # é‡æ–°ç»„åˆå¥å­
        sentences = []
        for i in range(0, len(segments)-1, 2):
            if i+1 < len(segments):
                sentence = segments[i] + segments[i+1]
                if sentence.strip():
                    sentences.append(sentence.strip())
        
        return sentences
    
    def _extract_characters(self, segments):
        """ä»æ®µè½ä¸­æå–è§’è‰²å€™é€‰"""
        import re
        character_mentions = {}
        
        for segment_idx, segment in enumerate(segments):
            for pattern in self.dialogue_patterns:
                matches = re.findall(pattern, segment)
                for match in matches:
                    # å¤„ç†ä¸åŒçš„åŒ¹é…ç»„
                    if isinstance(match, tuple):
                        for name in match:
                            if name and len(name) >= 2 and len(name) <= 6:
                                if self._is_valid_character_name(name):
                                    if name not in character_mentions:
                                        character_mentions[name] = {
                                            'frequency': 0,
                                            'segments': [],
                                            'contexts': []
                                        }
                                    character_mentions[name]['frequency'] += 1
                                    character_mentions[name]['segments'].append(segment_idx)
                                    character_mentions[name]['contexts'].append(segment)
                    else:
                        name = match
                        if name and len(name) >= 2 and len(name) <= 6:
                            if self._is_valid_character_name(name):
                                if name not in character_mentions:
                                    character_mentions[name] = {
                                        'frequency': 0,
                                        'segments': [],
                                        'contexts': []
                                    }
                                character_mentions[name]['frequency'] += 1
                                character_mentions[name]['segments'].append(segment_idx)
                                character_mentions[name]['contexts'].append(segment)
        
        return character_mentions
    
    def _is_valid_character_name(self, name: str) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„è§’è‰²å"""
        # è¿‡æ»¤æ’é™¤è¯æ±‡
        if name in self.excluded_words:
            return False
        
        # è¿‡æ»¤çº¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
        import re
        if re.match(r'^[\d\s\W]+$', name):
            return False
        
        # è¿‡æ»¤è¿‡çŸ­æˆ–è¿‡é•¿çš„åå­—
        if len(name) < 2 or len(name) > 6:
            return False
        
        # è¿‡æ»¤å¸¸è§çš„éè§’è‰²è¯æ±‡
        non_character_patterns = [
            r'^[çš„åœ°å¾—]',  # åŠ©è¯å¼€å¤´
            r'[çš„åœ°å¾—]$',  # åŠ©è¯ç»“å°¾
            r'^[åœ¨ä»åˆ°]',  # ä»‹è¯å¼€å¤´
            r'^[å’Œä¸åŠ]',  # è¿è¯å¼€å¤´
        ]
        
        for pattern in non_character_patterns:
            if re.match(pattern, name):
                return False
        
        return True
    
    def _validate_characters(self, candidates: dict, full_text: str):
        """éªŒè¯å’Œè¿‡æ»¤è§’è‰²å€™é€‰"""
        valid_characters = {}
        
        for name, data in candidates.items():
            # é¢‘ç‡è¿‡æ»¤ï¼šè‡³å°‘å‡ºç°2æ¬¡
            if data['frequency'] >= 2:
                valid_characters[name] = data
            # æˆ–è€…åœ¨å…¨æ–‡ä¸­æœ‰å¤šæ¬¡æåŠ
            elif full_text.count(name) >= 3:
                data['frequency'] = full_text.count(name)
                valid_characters[name] = data
        
        return valid_characters
    
    def _analyze_character_attributes(self, characters: dict, full_text: str):
        """åˆ†æè§’è‰²å±æ€§"""
        analyzed_characters = []
        
        for name, data in characters.items():
            # åˆ†ææ€§æ ¼ç‰¹å¾
            personality = self._analyze_personality(data['contexts'])
            
            # æ¨æ–­æ€§åˆ«
            gender = self._infer_gender(name, data['contexts'])
            
            # ç”Ÿæˆè§’è‰²é…ç½®
            character_config = {
                'name': name,
                'frequency': data['frequency'],
                'character_trait': {
                    'trait': personality['trait'],
                    'confidence': personality['confidence'],
                    'description': personality['description']
                },
                'first_appearance': min(data['segments']) + 1 if data['segments'] else 1,
                'is_main_character': data['frequency'] >= 5,  # å‡ºç°5æ¬¡ä»¥ä¸Šä¸ºä¸»è¦è§’è‰²
                'recommended_config': {
                    'gender': gender,
                    'personality': personality['trait'],
                    'personality_description': personality['description'],
                    'personality_confidence': personality['confidence'],
                    'description': f'{name}ï¼Œ{gender}è§’è‰²ï¼Œ{personality["description"]}ï¼Œåœ¨æ–‡æœ¬ä¸­å‡ºç°{data["frequency"]}æ¬¡ã€‚',
                    'recommended_tts_params': self._get_tts_params(personality['trait']),
                    'voice_type': f'{gender}_{personality["trait"]}',
                    'color': self._get_character_color(personality['trait'])
                }
            }
            
            analyzed_characters.append(character_config)
        
        # æŒ‰é¢‘ç‡æ’åº
        analyzed_characters.sort(key=lambda x: x['frequency'], reverse=True)
        
        return analyzed_characters
    
    def _analyze_personality(self, contexts: list):
        """åˆ†æè§’è‰²æ€§æ ¼"""
        personality_scores = {trait: 0 for trait in self.personality_keywords.keys()}
        
        # ç»Ÿè®¡æ€§æ ¼å…³é”®è¯
        for context in contexts:
            for trait, keywords in self.personality_keywords.items():
                for keyword in keywords:
                    if keyword in context:
                        personality_scores[trait] += 1
        
        # æ‰¾å‡ºæœ€é«˜åˆ†çš„æ€§æ ¼ç‰¹å¾
        if max(personality_scores.values()) > 0:
            dominant_trait = max(personality_scores, key=personality_scores.get)
            confidence = min(personality_scores[dominant_trait] / len(contexts), 1.0)
        else:
            dominant_trait = 'calm'  # é»˜è®¤æ€§æ ¼
            confidence = 0.3
        
        trait_descriptions = {
            'gentle': 'æ€§æ ¼æ¸©æŸ”ï¼Œè¯´è¯è½»å£°ç»†è¯­',
            'fierce': 'æ€§æ ¼åˆšçƒˆï¼Œè¯´è¯ç›´æ¥æœ‰åŠ›',
            'calm': 'æ€§æ ¼æ²‰ç¨³ï¼Œå¤„äº‹å†·é™',
            'lively': 'æ€§æ ¼æ´»æ³¼ï¼Œå……æ»¡æ´»åŠ›',
            'wise': 'æ™ºæ…§ç¿æ™ºï¼Œæ·±æ€ç†Ÿè™‘',
            'brave': 'å‹‡æ•¢æœæ•¢ï¼Œæ— æ‰€ç•æƒ§'
        }
        
        return {
            'trait': dominant_trait,
            'confidence': confidence,
            'description': trait_descriptions.get(dominant_trait, 'æ€§æ ¼æ¸©å’Œ')
        }
    
    def _infer_gender(self, name: str, contexts: list):
        """æ¨æ–­è§’è‰²æ€§åˆ«"""
        male_score = 0
        female_score = 0
        
        # åŸºäºç§°å‘¼æ¨æ–­
        for context in contexts:
            for keyword in self.gender_keywords['male']:
                if keyword in context:
                    male_score += 1
            for keyword in self.gender_keywords['female']:
                if keyword in context:
                    female_score += 1
        
        # åŸºäºåå­—æ¨æ–­ï¼ˆç®€å•è§„åˆ™ï¼‰
        common_male_chars = ['é¾™', 'è™', 'è±¹', 'é¹°', 'ç‹¼', 'é›„', 'å¼º', 'åˆš', 'å‹‡', 'å¨']
        common_female_chars = ['å‡¤', 'ç‡•', 'èº', 'èŠ±', 'æœˆ', 'é›ª', 'ç‰', 'ç ', 'ç¾', 'ä¸½']
        
        for char in common_male_chars:
            if char in name:
                male_score += 0.5
        
        for char in common_female_chars:
            if char in name:
                female_score += 0.5
        
        return 'male' if male_score > female_score else 'female'
    
    def _get_tts_params(self, personality: str):
        """æ ¹æ®æ€§æ ¼è·å–TTSå‚æ•°"""
        params_map = {
            'gentle': {'time_step': 35, 'p_w': 1.2, 't_w': 2.8},
            'fierce': {'time_step': 28, 'p_w': 1.6, 't_w': 3.2},
            'calm': {'time_step': 32, 'p_w': 1.4, 't_w': 3.0},
            'lively': {'time_step': 30, 'p_w': 1.3, 't_w': 2.9},
            'wise': {'time_step': 34, 'p_w': 1.3, 't_w': 3.1},
            'brave': {'time_step': 29, 'p_w': 1.5, 't_w': 3.1}
        }
        return params_map.get(personality, {'time_step': 32, 'p_w': 1.4, 't_w': 3.0})
    
    def _get_character_color(self, personality: str):
        """æ ¹æ®æ€§æ ¼è·å–è§’è‰²é¢œè‰²"""
        color_map = {
            'gentle': '#FFB6C1',  # æµ…ç²‰è‰²
            'fierce': '#FF6347',  # ç•ªèŒ„çº¢
            'calm': '#06b6d4',   # é’è‰²
            'lively': '#32CD32', # ç»¿è‰²
            'wise': '#9370DB',   # ç´«è‰²
            'brave': '#FF8C00'   # æ©™è‰²
        }
        return color_map.get(personality, '#06b6d4')
    
    def _is_dialogue(self, segment: str):
        """åˆ¤æ–­æ®µè½æ˜¯å¦åŒ…å«å¯¹è¯"""
        dialogue_indicators = ['"', '"', '"', 'ï¼š', ':', 'è¯´', 'é“', 'é—®', 'ç­”', 'å«', 'å–Š']
        return any(indicator in segment for indicator in dialogue_indicators)

class OllamaCharacterDetector:
    """ä½¿ç”¨Ollamaè¿›è¡Œè§’è‰²åˆ†æçš„æ£€æµ‹å™¨ - ä¸»åŠ›æ–¹æ¡ˆ"""
    
    def __init__(self, model_name: str = "qwen3:30b", ollama_url: str = None):
        import os
        self.model_name = model_name
        # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œæ”¯æŒDockeréƒ¨ç½²
        self.ollama_url = ollama_url or os.getenv("OLLAMA_URL", "http://localhost:11434")
        self.api_url = f"{self.ollama_url}/api/generate"
        
    async def analyze_text(self, text: str, chapter_info: dict) -> dict:
        """ä½¿ç”¨Ollamaåˆ†ææ–‡æœ¬ä¸­çš„è§’è‰² - ç›´æ¥AIåˆ†æï¼Œç®€å•é«˜æ•ˆ"""
        import time
        from ...utils.websocket_manager import send_analysis_progress
        
        start_time = time.time()
        session_id = chapter_info.get('session_id', chapter_info['chapter_id'])
        
        try:
            # å‘é€å¼€å§‹åˆ†æè¿›åº¦
            await send_analysis_progress(session_id, 10, f"å¼€å§‹åˆ†æç« èŠ‚: {chapter_info['chapter_title']}")
            
            # 1. ç›´æ¥è°ƒç”¨Ollamaè¿›è¡Œå…¨æ–‡åˆ†æï¼ˆåŒ…æ‹¬è§’è‰²è¯†åˆ«å’Œæ–‡æœ¬åˆ†æ®µï¼‰
            await send_analysis_progress(session_id, 30, "æ­£åœ¨è°ƒç”¨AIæ¨¡å‹è¿›è¡Œè§’è‰²è¯†åˆ«...")
            
            prompt = self._build_comprehensive_analysis_prompt(text)
            response = self._call_ollama(prompt)
            
            processing_time = time.time() - start_time
            
            if response:
                await send_analysis_progress(session_id, 80, "æ­£åœ¨è§£æAIåˆ†æç»“æœ...")
                
                # è§£æOllamaè¿”å›çš„å®Œæ•´ç»“æœ
                result = self._parse_comprehensive_response(response)
                
                await send_analysis_progress(session_id, 100, f"åˆ†æå®Œæˆï¼Œè¯†åˆ«åˆ°{len(result['characters'])}ä¸ªè§’è‰²")
                
                # æ£€æŸ¥è§’è‰²æ˜¯å¦å·²å­˜åœ¨äºè§’è‰²åº“ä¸­
                filtered_characters = await self._filter_existing_characters(result['characters'])
                
                return {
                    "chapter_id": chapter_info['chapter_id'],
                    "chapter_title": chapter_info['chapter_title'],
                    "chapter_number": chapter_info['chapter_number'],
                    "detected_characters": filtered_characters,
                    "segments": result['segments'],
                    "processing_stats": {
                        "total_segments": len(result['segments']),
                        "dialogue_segments": len([s for s in result['segments'] if s['text_type'] == 'dialogue']),
                        "narration_segments": len([s for s in result['segments'] if s['text_type'] == 'narration']),
                        "characters_found": len(result['characters']),
                        "new_characters_found": len(filtered_characters),
                        "analysis_method": "ollama_ai_primary",
                        "processing_time": round(processing_time, 2),
                        "text_length": len(text),
                        "ai_model": self.model_name
                    }
                }
            else:
                # Ollamaè°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•è§„åˆ™æ–¹æ³•
                await send_analysis_progress(session_id, 50, "AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ...")
                return self._fallback_simple_analysis(text, chapter_info, processing_time)
                
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Ollamaè§’è‰²åˆ†æå¤±è´¥: {str(e)}")
            await send_analysis_progress(session_id, 0, f"åˆ†æå¤±è´¥: {str(e)}")
            return self._fallback_simple_analysis(text, chapter_info, processing_time)
    
    def _fallback_simple_analysis(self, text: str, chapter_info: dict, processing_time: float = 0) -> dict:
        """ç®€å•å›é€€æ–¹æ¡ˆ - åŸºç¡€è§„åˆ™è¯†åˆ«"""
        logger.warning("ä½¿ç”¨ç®€å•å›é€€æ–¹æ¡ˆ")
        
        # ç®€å•åˆ†æ®µï¼šæŒ‰å¥å·åˆ†å‰²
        sentences = [s.strip() + 'ã€‚' for s in text.split('ã€‚') if s.strip()]
        
        # ç®€å•è§’è‰²è¯†åˆ«ï¼šæŸ¥æ‰¾å¼•å·å¯¹è¯
        characters = {'æ—ç™½': 0}  # é»˜è®¤æœ‰æ—ç™½
        segments = []
        
        for i, sentence in enumerate(sentences):
            if any(quote in sentence for quote in ['"', '"', '"', 'ã€Œ', 'ã€', 'ã€', 'ã€']):
                # ç®€å•å¯¹è¯è¯†åˆ«
                speaker = "æœªçŸ¥è§’è‰²"
                text_type = "dialogue"
            else:
                # é»˜è®¤ä¸ºæ—ç™½
                speaker = "æ—ç™½"
                text_type = "narration"
                characters['æ—ç™½'] += 1
            
            segments.append({
                'order': i + 1,
                'text': sentence,
                'speaker': speaker,
                'confidence': 0.6,
                'detection_rule': 'simple_fallback',
                'text_type': text_type
            })
        
        # æ„å»ºè§’è‰²åˆ—è¡¨
        character_list = []
        for name, freq in characters.items():
            if freq > 0:
                character_list.append({
                    'name': name,
                    'frequency': freq,
                    'character_trait': {'trait': 'calm', 'confidence': 0.6, 'description': 'ç®€å•è¯†åˆ«'},
                    'first_appearance': 1,
                    'is_main_character': freq >= 3,
                    'recommended_config': {
                        'gender': 'neutral' if name == 'æ—ç™½' else 'unknown',
                        'personality': 'calm',
                        'personality_description': 'ç®€å•è¯†åˆ«ï¼Œéœ€è¦æ‰‹åŠ¨é…ç½®',
                        'personality_confidence': 0.6,
                        'description': f'{name}è§’è‰²ï¼Œç®€å•è¯†åˆ«ç»“æœ',
                        'recommended_tts_params': {'time_step': 32, 'p_w': 1.4, 't_w': 3.0},
                        'voice_type': 'narrator' if name == 'æ—ç™½' else 'unknown',
                        'color': '#6c757d' if name == 'æ—ç™½' else '#007bff'
                    }
                })
        
        return {
            "chapter_id": chapter_info['chapter_id'],
            "chapter_title": chapter_info['chapter_title'],
            "chapter_number": chapter_info['chapter_number'],
            "detected_characters": character_list,
            "segments": segments,
            "processing_stats": {
                "total_segments": len(segments),
                "dialogue_segments": len([s for s in segments if s['text_type'] == 'dialogue']),
                "narration_segments": len([s for s in segments if s['text_type'] == 'narration']),
                "characters_found": len(character_list),
                "analysis_method": "simple_fallback"
            }
        }
    


    def _build_comprehensive_analysis_prompt(self, text: str) -> str:
        """æ„å»ºç»¼åˆåˆ†ææç¤ºè¯ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ­£ç¡®å¤„ç†æ··åˆå¯¹è¯åˆ†æ®µ"""
        # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¶…æ—¶
        limited_text = text[:1500] if len(text) > 1500 else text
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡å°è¯´æ–‡æœ¬åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å°è¯´æ–‡æœ¬ï¼Œè¯†åˆ«è§’è‰²å¹¶æ­£ç¡®åˆ†æ®µã€‚

æ–‡æœ¬ï¼š
{limited_text}

åˆ†æè¦æ±‚ï¼š
1. è¯†åˆ«æ‰€æœ‰è¯´è¯çš„è§’è‰²ï¼ˆåŒ…æ‹¬æ—ç™½ï¼‰
2. å°†æ–‡æœ¬æŒ‰å¥å­åˆ†æ®µï¼Œæ¯æ®µæ ‡è®°æ­£ç¡®çš„è¯´è¯è€…
3. **æ ¸å¿ƒè¦æ±‚**ï¼šæ­£ç¡®åˆ†ç¦»æ··åˆå¯¹è¯æ–‡æœ¬

å…³é”®åˆ†æ®µè§„åˆ™ï¼š
- æ··åˆæ–‡æœ¬å¦‚"é¡¹ç¾½å†·ç¬‘ä¸€å£°ï¼š"ä½ åˆæ˜¯ä½•äººï¼Ÿ""å¿…é¡»åˆ†ä¸ºä¸¤æ®µï¼š
  ç¬¬ä¸€æ®µï¼š"é¡¹ç¾½å†·ç¬‘ä¸€å£°ï¼š" â†’ è¯´è¯è€…ï¼šæ—ç™½
  ç¬¬äºŒæ®µï¼š"ä½ åˆæ˜¯ä½•äººï¼Ÿ" â†’ è¯´è¯è€…ï¼šé¡¹ç¾½
- æ‰€æœ‰"æŸæŸè¯´ï¼š"ã€"æŸæŸé“ï¼š"ã€"æŸæŸä½å£°é“ï¼š"ç­‰æè¿°æ€§åŠ¨ä½œæ–‡å­—éƒ½æ˜¯æ—ç™½
- åªæœ‰å¼•å·""å†…çš„å®é™…å¯¹è¯å†…å®¹æ‰æ˜¯è§’è‰²å‘è¨€
- çº¯å™è¿°æ–‡å­—ï¼ˆå¦‚"åªè§å±±åŠ¿é™©å³»"ã€"æ­¤æ—¶å¤©è‰²å·²æ™š"ï¼‰éƒ½æ˜¯æ—ç™½

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
  "segments": [
    {{"order": 1, "text": "æ–‡æœ¬å†…å®¹", "speaker": "è¯´è¯è€…", "text_type": "dialogue/narration", "confidence": 0.9}}
  ],
  "characters": [
    {{"name": "è§’è‰²å", "frequency": å‡ºç°æ¬¡æ•°, "gender": "male/female/neutral", "personality": "calm/brave/gentle", "personality_description": "æ€§æ ¼æè¿°", "is_main_character": true/false, "confidence": 0.8}}
  ]
}}

é‡è¦æé†’ï¼š
- è§’è‰²åå¿…é¡»å®Œæ•´ï¼ˆå¦‚"å­™æ‚Ÿç©º"è€Œä¸æ˜¯"æ‚Ÿç©º"ï¼‰
- ä¸¥æ ¼åŒºåˆ†å™è¿°ï¼ˆæ—ç™½ï¼‰å’Œå¯¹è¯ï¼ˆè§’è‰²ï¼‰
- å¿…é¡»æ­£ç¡®åˆ†ç¦»åŠ¨ä½œæè¿°å’Œå®é™…å¯¹è¯
- åªè¾“å‡ºJSONï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—"""
        
        return prompt

    def _call_ollama(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Ollama API"""
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.2,  # Qwen3é€‚åˆç¨é«˜ä¸€ç‚¹çš„æ¸©åº¦
                    "top_p": 0.8,
                    "max_tokens": 2000,
                    "num_ctx": 4096  # å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦
                }
            }
            
            response = requests.post(
                self.api_url,
                json=payload,
                timeout=180  # 3åˆ†é’Ÿè¶…æ—¶ï¼Œé€‚åº”é•¿æ–‡æœ¬åˆ†æ
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                logger.error(f"Ollama APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return None
                
        except requests.exceptions.Timeout:
            logger.error("Ollama APIè°ƒç”¨è¶…æ—¶")
            return None
        except Exception as e:
            logger.error(f"Ollama APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return None
    
    def _parse_comprehensive_response(self, response: str) -> Dict:
        """è§£æOllamaè¿”å›çš„ç»¼åˆåˆ†æç»“æœ"""
        try:
            # è®°å½•åŸå§‹å“åº”ç”¨äºè°ƒè¯•
            logger.info(f"OllamaåŸå§‹å“åº”: {response[:500]}...")
            
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                data = json.loads(json_str)
                
                logger.info(f"è§£æçš„JSONæ•°æ®: {json.dumps(data, ensure_ascii=False, indent=2)}")
                
                # å¤„ç†segments
                segments = []
                for i, seg_data in enumerate(data.get('segments', [])):
                    segments.append({
                        'order': seg_data.get('order', i + 1),
                        'text': seg_data.get('text', ''),
                        'speaker': seg_data.get('speaker', 'æ—ç™½'),
                        'confidence': seg_data.get('confidence', 0.8),
                        'detection_rule': 'ollama_ai',
                        'text_type': seg_data.get('text_type', 'narration')
                    })
                
                # å¤„ç†characters
                characters = []
                for char_data in data.get('characters', []):
                    if isinstance(char_data, dict) and 'name' in char_data:
                        name = char_data.get('name', '')
                        if name and len(name) >= 2:
                            characters.append({
                                'name': name,
                                'frequency': char_data.get('frequency', 1),
                                'character_trait': {
                                    'trait': char_data.get('personality', 'calm'),
                                    'confidence': char_data.get('confidence', 0.8),
                                    'description': char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ')
                                },
                                'first_appearance': 1,
                                'is_main_character': char_data.get('is_main_character', False),
                                'recommended_config': {
                                    'gender': self._infer_gender_smart(name, char_data.get('gender', 'unknown')),
                                    'personality': char_data.get('personality', 'calm'),
                                    'personality_description': char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ'),
                                    'personality_confidence': char_data.get('confidence', 0.8),
                                    'description': f"{name}ï¼Œ{self._infer_gender_smart(name, char_data.get('gender', 'unknown'))}è§’è‰²ï¼Œ{char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ')}ï¼Œåœ¨æ–‡æœ¬ä¸­å‡ºç°{char_data.get('frequency', 1)}æ¬¡ã€‚",
                                    'recommended_tts_params': self._get_tts_params(char_data.get('personality', 'calm')),
                                    'voice_type': f"{self._infer_gender_smart(name, char_data.get('gender', 'unknown'))}_{char_data.get('personality', 'calm')}",
                                    'color': self._get_character_color(char_data.get('personality', 'calm'))
                                }
                            })
                
                return {
                    'segments': segments,
                    'characters': characters
                }
            
            else:
                logger.error("æ— æ³•ä»Ollamaå“åº”ä¸­æå–JSONæ•°æ®")
                return {'segments': [], 'characters': []}
                
        except json.JSONDecodeError as e:
            logger.error(f"è§£æOllama JSONå“åº”å¤±è´¥: {str(e)}")
            logger.error(f"åŸå§‹å“åº”: {response}")
            return {'segments': [], 'characters': []}
        except Exception as e:
            logger.error(f"å¤„ç†Ollamaå“åº”å¼‚å¸¸: {str(e)}")
            return {'segments': [], 'characters': []}

    def _parse_ollama_response(self, response: str) -> List[Dict]:
        """è§£æOllamaè¿”å›çš„JSONç»“æœ"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                characters_data = json.loads(json_str)
                
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                processed_characters = []
                for char_data in characters_data:
                    if isinstance(char_data, dict) and 'name' in char_data:
                        # éªŒè¯å’Œæ¸…ç†è§’è‰²å
                        name = self._clean_character_name(char_data.get('name', ''))
                        if name and len(name) >= 2:
                            processed_char = {
                                'name': name,
                                'frequency': char_data.get('frequency', 1),
                                'character_trait': {
                                    'trait': char_data.get('personality', 'calm'),
                                    'confidence': char_data.get('confidence', 0.8),
                                    'description': char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ')
                                },
                                'first_appearance': 1,
                                'is_main_character': char_data.get('is_main_character', False),
                                'recommended_config': {
                                    'gender': char_data.get('gender', 'female'),
                                    'personality': char_data.get('personality', 'calm'),
                                    'personality_description': char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ'),
                                    'personality_confidence': char_data.get('confidence', 0.8),
                                    'description': f"{name}ï¼Œ{char_data.get('gender', 'female')}è§’è‰²ï¼Œ{char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ')}ï¼Œåœ¨æ–‡æœ¬ä¸­å‡ºç°{char_data.get('frequency', 1)}æ¬¡ã€‚",
                                    'recommended_tts_params': self._get_tts_params(char_data.get('personality', 'calm')),
                                    'voice_type': f"{char_data.get('gender', 'female')}_{char_data.get('personality', 'calm')}",
                                    'color': self._get_character_color(char_data.get('personality', 'calm'))
                                }
                            }
                            processed_characters.append(processed_char)
                
                # æŒ‰é¢‘ç‡æ’åº
                processed_characters.sort(key=lambda x: x['frequency'], reverse=True)
                return processed_characters
            
            else:
                logger.error("æ— æ³•ä»Ollamaå“åº”ä¸­æå–JSONæ•°æ®")
                return []
                
        except json.JSONDecodeError as e:
            logger.error(f"è§£æOllama JSONå“åº”å¤±è´¥: {str(e)}")
            logger.error(f"åŸå§‹å“åº”: {response}")
            return []
        except Exception as e:
            logger.error(f"å¤„ç†Ollamaå“åº”å¼‚å¸¸: {str(e)}")
            return []
    
    def _clean_character_name(self, name: str) -> str:
        """æ¸…ç†è§’è‰²åç§°"""
        if not name:
            return ""
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        name = re.sub(r'["""''ï¼š:ï¼Œã€‚ï¼ï¼Ÿ\s]', '', name)
        
        # ç§»é™¤å¸¸è§çš„åŠ¨ä½œè¯åç¼€
        action_suffixes = ['è¯´', 'é“', 'è®²', 'é—®', 'ç­”', 'å«', 'å–Š', 'ç¬‘', 'å“­', 'èµ°', 'æ¥', 'å»']
        for suffix in action_suffixes:
            if name.endswith(suffix) and len(name) > len(suffix):
                name = name[:-len(suffix)]
        
        # ç§»é™¤å¸¸è§å‰ç¼€
        prefixes = ['"', '"', 'ã€', 'ã€–']
        for prefix in prefixes:
            if name.startswith(prefix):
                name = name[1:]
        
        return name.strip()
    
    def _get_tts_params(self, personality: str) -> Dict:
        """æ ¹æ®æ€§æ ¼è·å–TTSå‚æ•°"""
        params_map = {
            'gentle': {'time_step': 35, 'p_w': 1.2, 't_w': 2.8},
            'fierce': {'time_step': 28, 'p_w': 1.6, 't_w': 3.2},
            'calm': {'time_step': 32, 'p_w': 1.4, 't_w': 3.0},
            'lively': {'time_step': 30, 'p_w': 1.3, 't_w': 2.9},
            'wise': {'time_step': 34, 'p_w': 1.3, 't_w': 3.1},
            'brave': {'time_step': 29, 'p_w': 1.5, 't_w': 3.1}
        }
        return params_map.get(personality, {'time_step': 32, 'p_w': 1.4, 't_w': 3.0})
    
    def _get_character_color(self, personality: str) -> str:
        """æ ¹æ®æ€§æ ¼è·å–è§’è‰²é¢œè‰²"""
        color_map = {
            'gentle': '#FFB6C1',  # æµ…ç²‰è‰²
            'fierce': '#FF6347',  # ç•ªèŒ„çº¢
            'calm': '#06b6d4',   # é’è‰²
            'lively': '#32CD32', # ç»¿è‰²
            'wise': '#9370DB',   # ç´«è‰²
            'brave': '#FF8C00'   # æ©™è‰²
        }
        return color_map.get(personality, '#06b6d4')
    
    async def _filter_existing_characters(self, characters: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤å·²å­˜åœ¨äºè§’è‰²åº“ä¸­çš„è§’è‰²"""
        from ...database import get_db
        from ...models import VoiceProfile
        from sqlalchemy.orm import Session
        
        # è·å–æ•°æ®åº“ä¼šè¯
        db_gen = get_db()
        db: Session = next(db_gen)
        
        try:
            filtered_characters = []
            
            for char in characters:
                char_name = char.get('name', '')
                
                # æ£€æŸ¥è§’è‰²æ˜¯å¦å·²å­˜åœ¨
                existing_character = db.query(VoiceProfile).filter(
                    VoiceProfile.name == char_name
                ).first()
                
                if not existing_character:
                    # è§’è‰²ä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
                    filtered_characters.append(char)
                    logger.info(f"æ–°è§’è‰²å‘ç°: {char_name}")
                else:
                    logger.info(f"è§’è‰²å·²å­˜åœ¨ï¼Œè·³è¿‡: {char_name} (ID: {existing_character.id})")
            
            return filtered_characters
            
        except Exception as e:
            logger.error(f"è¿‡æ»¤å·²å­˜åœ¨è§’è‰²å¤±è´¥: {str(e)}")
            # å‡ºé”™æ—¶è¿”å›æ‰€æœ‰è§’è‰²
            return characters
        finally:
            db.close()
    
    def _infer_gender_smart(self, name: str, ai_gender: str) -> str:
        """æ™ºèƒ½æ¨æ–­è§’è‰²æ€§åˆ«"""
        # å¦‚æœAIå·²ç»æ­£ç¡®è¯†åˆ«äº†æ€§åˆ«ï¼Œç›´æ¥ä½¿ç”¨
        if ai_gender and ai_gender in ['male', 'female', 'neutral']:
            return ai_gender
        
        # åŸºäºè§’è‰²åç§°çš„æ€§åˆ«æ¨æ–­
        male_names = {
            'å”åƒ§', 'å­™æ‚Ÿç©º', 'çŒªå…«æˆ’', 'æ²™åƒ§', 'å¦‚æ¥ä½›ç¥–', 'è§‚éŸ³è©è¨', 'ç‰çš‡å¤§å¸',
            'å¤ªä¸Šè€å›', 'äºŒéƒç¥', 'å“ªå’', 'é¾™ç‹', 'åœŸåœ°å…¬', 'åŸéš', 'åˆ¤å®˜',
            'ç‰›é­”ç‹', 'çº¢å­©å„¿', 'é“æ‰‡å…¬ä¸»', 'ç™½é¾™é©¬', 'å¼¥å‹’ä½›', 'æ–‡æ®Šè©è¨',
            'æ™®è´¤è©è¨', 'åœ°è—è©è¨', 'éŸ¦é™€', 'å››å¤§å¤©ç‹', 'æ‰˜å¡”æå¤©ç‹', 'å·¨çµç¥'
        }
        
        female_names = {
            'ç™½éª¨ç²¾', 'èœ˜è››ç²¾', 'ç‰å…”ç²¾', 'å«¦å¨¥', 'ç‹æ¯å¨˜å¨˜', 'ä¸ƒä»™å¥³',
            'ç»‡å¥³', 'ç™½ç´ è´', 'å°é’', 'è®¸ä»™', 'æ³•æµ·', 'è§‚éŸ³', 'å¦ˆç¥–',
            'ä¹å¤©ç„å¥³', 'è¥¿ç‹æ¯', 'ç‘¶æ± é‡‘æ¯', 'å¤ªé˜´æ˜Ÿå›', 'æœˆå®«å«¦å¨¥'
        }
        
        neutral_names = {
            'æ—ç™½', 'å™è¿°è€…', 'è¯´ä¹¦äºº', 'ä½œè€…'
        }
        
        # ç²¾ç¡®åŒ¹é…
        if name in male_names:
            return 'male'
        elif name in female_names:
            return 'female'
        elif name in neutral_names:
            return 'neutral'
        
        # åŸºäºåç§°ç‰¹å¾æ¨æ–­
        # åŒ…å«"ç²¾"ã€"å¦–"ç­‰å­—çš„å¥³æ€§è§’è‰²
        if any(char in name for char in ['ç²¾', 'ä»™å­', 'å¨˜å¨˜', 'å…¬ä¸»', 'å¤«äºº', 'å¨˜å­']):
            return 'female'
        
        # åŒ…å«"ç‹"ã€"ç¥"ã€"ä½›"ç­‰å­—çš„ç”·æ€§è§’è‰²
        if any(char in name for char in ['ç‹', 'ç¥', 'ä½›', 'ä»™', 'å›', 'å¸', 'å…¬', 'çˆ·']):
            return 'male'
        
        # åŸºäºå¸¸è§å§“åæ¨¡å¼
        # ä¸¤å­—åç§°çš„æ€§åˆ«æ¨æ–­
        if len(name) == 2:
            # å¸¸è§ç”·æ€§åå­—ç‰¹å¾
            male_chars = ['å¼º', 'ä¼Ÿ', 'å†›', 'å', 'æ˜', 'åˆš', 'å‹‡', 'å³°', 'ç£Š', 'æ¶›']
            if any(char in name for char in male_chars):
                return 'male'
            
            # å¸¸è§å¥³æ€§åå­—ç‰¹å¾  
            female_chars = ['ç¾', 'ä¸½', 'å¨œ', 'å©·', 'é›…', 'é™', 'èŠ³', 'è‰', 'çº¢', 'ç‡•']
            if any(char in name for char in female_chars):
                return 'female'
        
        # é»˜è®¤è¿”å›unknownï¼Œè®©ç”¨æˆ·æ‰‹åŠ¨é€‰æ‹©
        logger.warning(f"æ— æ³•æ¨æ–­è§’è‰² '{name}' çš„æ€§åˆ«ï¼ŒAIè¿”å›: {ai_gender}")
        return 'unknown'


@router.post("/{chapter_id}/prepare-synthesis")
async def prepare_chapter_for_synthesis(
    chapter_id: int,
    include_emotion: bool = Query(True, description="æ˜¯å¦åŒ…å«æƒ…ç»ªè¯†åˆ«"),
    processing_mode: str = Query("auto", description="å¤„ç†æ¨¡å¼: auto/single/distributed"),
    db: Session = Depends(get_db)
):
    """
    å‡†å¤‡ç« èŠ‚å†…å®¹ç”¨äºè¯­éŸ³åˆæˆï¼ˆè¾“å‡ºå…¼å®¹ç°æœ‰æ ¼å¼ï¼‰
    
    è¿™æ˜¯æ™ºèƒ½å†…å®¹å‡†å¤‡çš„æ ¸å¿ƒAPIï¼Œå®ç°ï¼š
    - ğŸ­ æ™ºèƒ½è§’è‰²è¯†åˆ«ä¸åˆ†ç¦»
    - ğŸ”’ åŸæ–‡å†…å®¹100%ä¿æŒä¸å˜
    - ğŸ­ è‡ªåŠ¨æ·»åŠ æ—ç™½è§’è‰²
    - ğŸ“‹ è¾“å‡ºå®Œå…¨å…¼å®¹ç°æœ‰åˆæˆç³»ç»Ÿçš„JSONæ ¼å¼
    - ğŸ§  æ”¯æŒå¤§æ–‡æœ¬åˆ†å¸ƒå¼å¤„ç†
    """
    
    try:
        # åˆ›å»ºå†…å®¹å‡†å¤‡æœåŠ¡
        content_service = ContentPreparationService(db)
        
        # æ‰§è¡Œæ™ºèƒ½å†…å®¹å‡†å¤‡
        result = await content_service.prepare_chapter_for_synthesis(
            chapter_id=chapter_id,
            user_preferences={
                "include_emotion": include_emotion,
                "processing_mode": processing_mode
            }
        )
        
        # è®°å½•ç³»ç»Ÿäº‹ä»¶
        log_system_event(
            db, 
            "chapter_synthesis_prepared", 
            f"ç« èŠ‚ {chapter_id} æ™ºèƒ½å†…å®¹å‡†å¤‡å®Œæˆ",
            {
                "chapter_id": chapter_id,
                "processing_mode": result["processing_info"]["mode"],
                "total_segments": result["processing_info"]["total_segments"],
                "characters_found": result["processing_info"]["characters_found"]
            }
        )
        
        return {
            "success": True,
            "message": f"ç« èŠ‚å†…å®¹å‡†å¤‡å®Œæˆï¼Œå…±è¯†åˆ« {result['processing_info']['characters_found']} ä¸ªè§’è‰²ï¼Œ{result['processing_info']['total_segments']} ä¸ªæ®µè½",
            "data": result["synthesis_json"],  # å…¼å®¹ç°æœ‰æ ¼å¼çš„JSON
            "processing_info": result["processing_info"]
        }
        
    except Exception as e:
        logger.error(f"ç« èŠ‚ {chapter_id} å†…å®¹å‡†å¤‡å¤±è´¥: {str(e)}")
        
        # è®°å½•é”™è¯¯äº‹ä»¶
        log_system_event(
            db, 
            "chapter_synthesis_preparation_failed", 
            f"ç« èŠ‚ {chapter_id} æ™ºèƒ½å†…å®¹å‡†å¤‡å¤±è´¥: {str(e)}",
            {"chapter_id": chapter_id, "error": str(e)}
        )
        
        raise HTTPException(
            status_code=500, 
            detail=f"å†…å®¹å‡†å¤‡å¤±è´¥: {str(e)}"
        )


@router.get("/{chapter_id}/synthesis-preview")
async def get_synthesis_preview(
    chapter_id: int,
    max_segments: int = Query(10, ge=1, le=50, description="é¢„è§ˆæ®µè½æ•°é‡"),
    db: Session = Depends(get_db)
):
    """
    è·å–ç« èŠ‚åˆæˆé¢„è§ˆ
    
    å¿«é€Ÿé¢„è§ˆç« èŠ‚çš„æ™ºèƒ½åˆ†æç»“æœï¼Œä¸è¿›è¡Œå®Œæ•´å¤„ç†
    """
    
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # ä½¿ç”¨ç®€å•çš„è§’è‰²æ£€æµ‹å™¨è¿›è¡Œå¿«é€Ÿé¢„è§ˆ
        detector = ProgrammaticCharacterDetector()
        
        # å–å‰1000å­—ç¬¦è¿›è¡Œé¢„è§ˆåˆ†æ
        preview_text = chapter.content[:1000] if len(chapter.content) > 1000 else chapter.content
        
        # åˆ†ææ–‡æœ¬æ®µè½
        segments = detector.segment_text_with_speakers(preview_text)
        
        # æå–è§’è‰²ä¿¡æ¯
        character_stats = detector.extract_dialogue_characters(segments)
        
        # é™åˆ¶é¢„è§ˆæ®µè½æ•°é‡
        preview_segments = segments[:max_segments]
        
        return {
            "success": True,
            "chapter_info": {
                "id": chapter.id,
                "title": chapter.chapter_title,
                "content_length": len(chapter.content),
                "preview_length": len(preview_text)
            },
            "preview_segments": preview_segments,
            "detected_characters": [
                {"name": name, "segment_count": count}
                for name, count in character_stats.items()
            ],
            "statistics": {
                "total_segments": len(segments),
                "preview_segments": len(preview_segments),
                "character_count": len(character_stats),
                "is_truncated": len(chapter.content) > 1000
            }
        }
        
    except Exception as e:
        logger.error(f"ç« èŠ‚ {chapter_id} é¢„è§ˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é¢„è§ˆå¤±è´¥: {str(e)}")


@router.get("/{chapter_id}/content-stats")
async def get_chapter_content_stats(
    chapter_id: int,
    db: Session = Depends(get_db)
):
    """
    è·å–ç« èŠ‚å†…å®¹ç»Ÿè®¡ä¿¡æ¯
    
    ç”¨äºåˆ¤æ–­å¤„ç†ç­–ç•¥å’Œé¢„ä¼°å¤„ç†æ—¶é—´
    """
    
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        content = chapter.content
        
        # åŸºæœ¬ç»Ÿè®¡
        char_count = len(content)
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
        english_words = len(re.findall(r'[a-zA-Z]+', content))
        
        # ä¼°ç®—tokenæ•°é‡
        estimated_tokens = int(chinese_chars * 1.5 + english_words)
        
        # æ®µè½ç»Ÿè®¡
        paragraphs = re.split(r'\n\s*\n', content.strip())
        paragraph_count = len([p for p in paragraphs if p.strip()])
        
        # å¯¹è¯ç»Ÿè®¡
        dialogue_markers = ['"', '"', '"', 'ã€Œ', 'ã€', 'ã€', 'ã€', "'", "'"]
        dialogue_count = sum(content.count(marker) for marker in dialogue_markers)
        
        # æ¨èå¤„ç†æ¨¡å¼
        if estimated_tokens <= 3000:
            recommended_mode = "single"
            estimated_time = "30-60ç§’"
        else:
            recommended_mode = "distributed"
            estimated_time = "60-120ç§’"
        
        return {
            "success": True,
            "chapter_info": {
                "id": chapter.id,
                "title": chapter.chapter_title
            },
            "content_stats": {
                "total_characters": char_count,
                "chinese_characters": chinese_chars,
                "english_words": english_words,
                "estimated_tokens": estimated_tokens,
                "paragraph_count": paragraph_count,
                "dialogue_markers": dialogue_count
            },
            "processing_recommendation": {
                "recommended_mode": recommended_mode,
                "estimated_time": estimated_time,
                "complexity": "simple" if estimated_tokens <= 1500 else "medium" if estimated_tokens <= 3000 else "complex"
            }
        }
        
    except Exception as e:
        logger.error(f"ç« èŠ‚ {chapter_id} ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç»Ÿè®¡å¤±è´¥: {str(e)}") 