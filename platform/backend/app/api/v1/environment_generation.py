"""
ç¯å¢ƒéŸ³ç”ŸæˆAPIæ¥å£
æ•´åˆæ—ç™½ç¯å¢ƒåˆ†æå™¨å’Œç¯å¢ƒé…ç½®æ ¡å¯¹å™¨ä¸ºå®Œæ•´çš„APIæœåŠ¡
"""

from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
import logging
from datetime import datetime

from app.services.narration_environment_analyzer import NarrationEnvironmentAnalyzer
from app.services.chapter_environment_analyzer import ChapterEnvironmentAnalyzer
from app.services.sound_matching_engine import SoundMatchingEngine
from app.services.tangoflux_environment_generator import TangoFluxEnvironmentGenerator
from app.services.timeline_generator import EnvironmentTimelineGenerator
from app.services.environment_config_validator import EnvironmentConfigValidator
from app.utils.logger import get_logger
from app.database import get_db
from app.models.novel_project import NovelProject
from app.models.analysis_result import AnalysisResult
from app.models.book_chapter import BookChapter
from sqlalchemy.orm import Session

logger = get_logger(__name__)
router = APIRouter(prefix="/environment-generation", tags=["ç¯å¢ƒéŸ³ç”Ÿæˆ"])

# === Request/Response Models ===
class EnvironmentGenerationRequest(BaseModel):
    """ç¯å¢ƒéŸ³ç”Ÿæˆè¯·æ±‚"""
    project_id: int
    synthesis_plan: List[Dict[str, Any]]
    options: Optional[Dict[str, Any]] = {}

class ValidationEditRequest(BaseModel):
    """æ ¡å¯¹ç¼–è¾‘è¯·æ±‚"""
    track_index: int
    manual_edits: Dict[str, Any]

class ValidationApprovalRequest(BaseModel):
    """æ ¡å¯¹å®¡æ‰¹è¯·æ±‚"""
    track_index: int
    validation_result: str  # approved/rejected/needs_revision
    notes: Optional[str] = None

class ChapterEnvironmentAnalysisRequest(BaseModel):
    """ç« èŠ‚ç¯å¢ƒéŸ³åˆ†æè¯·æ±‚ - æ–°æµç¨‹"""
    chapter_ids: List[int]
    analysis_options: Optional[Dict[str, Any]] = {}

class EnvironmentMatchingRequest(BaseModel):
    """ç¯å¢ƒéŸ³åŒ¹é…è¯·æ±‚"""
    analysis_result: Dict[str, Any]
    matching_options: Optional[Dict[str, Any]] = {}

class EnvironmentGenerationRequest(BaseModel):
    """ç¯å¢ƒéŸ³ç”Ÿæˆè¯·æ±‚"""
    generation_plan: List[Dict[str, Any]]
    generation_options: Optional[Dict[str, Any]] = {}

class TimelineExportRequest(BaseModel):
    """æ—¶é—´è½´å¯¼å‡ºè¯·æ±‚"""
    timeline_data: Dict[str, Any]
    export_format: str = 'generic'  # generic, premiere_pro, davinci_resolve
    output_path: Optional[str] = None

# === Global Storage (Session-based) ===
# åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™åº”è¯¥å­˜å‚¨åœ¨Redisæˆ–æ•°æ®åº“ä¸­
_analysis_sessions: Dict[str, Dict] = {}

def get_session_id(project_id: int) -> str:
    """ç”Ÿæˆä¼šè¯ID"""
    return f"env_gen_{project_id}"

# === API Endpoints ===

@router.post("/analyze")
async def analyze_environment_from_synthesis_plan(
    request: EnvironmentGenerationRequest,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    ä»synthesis_planåˆ†æç¯å¢ƒéŸ³éœ€æ±‚
    ç¬¬ä¸€æ­¥ï¼šæ—ç™½æå–åˆ†æ
    æ”¯æŒforce_reanalyzeå‚æ•°å¼ºåˆ¶é‡æ–°åˆ†æ
    """
    session_id = get_session_id(request.project_id)
    force_reanalyze = request.options.get('force_reanalyze', False)
    
    logger.info(f"[ENV_GEN_API] å¼€å§‹ç¯å¢ƒéŸ³åˆ†æï¼Œé¡¹ç›®ID: {request.project_id}ï¼Œå¼ºåˆ¶é‡æ–°åˆ†æ: {force_reanalyze}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆ†æç»“æœ
    if not force_reanalyze and session_id in _analysis_sessions:
        existing_session = _analysis_sessions[session_id]
        if 'analysis_result' in existing_session:
            logger.info(f"[ENV_GEN_API] å‘ç°å·²æœ‰åˆ†æç»“æœï¼Œé¡¹ç›®ID: {request.project_id}")
            return {
                'success': True,
                'project_id': request.project_id,
                'session_id': session_id,
                'analysis_result': existing_session['analysis_result'],
                'analysis_stats': existing_session.get('analysis_stats', {}),
                'existing_analysis': True,
                'message': 'å‘ç°å·²æœ‰åˆ†æç»“æœï¼Œå¦‚éœ€é‡æ–°åˆ†æè¯·ä½¿ç”¨é‡æ–°åˆ†æåŠŸèƒ½'
            }
    
    try:
        # ğŸš¨ éªŒè¯é¡¹ç›®çŠ¶æ€
        project = db.query(NovelProject).filter(NovelProject.id == request.project_id).first()
        if not project:
            raise HTTPException(status_code=404, detail=f"é¡¹ç›® {request.project_id} ä¸å­˜åœ¨")
        
        if project.status == 'cancelled':
            raise HTTPException(
                status_code=422, 
                detail=f"é¡¹ç›® {request.project_id} å·²è¢«å–æ¶ˆï¼Œæ— æ³•è¿›è¡Œç¯å¢ƒéŸ³åˆ†æã€‚è¯·é‡æ–°å¯åŠ¨é¡¹ç›®æˆ–é€‰æ‹©å…¶ä»–é¡¹ç›®ã€‚"
            )
        
        # ğŸš¨ éªŒè¯synthesis_planæ•°æ®
        if not request.synthesis_plan:
            # å°è¯•ä»æ•°æ®åº“è·å–synthesis_plan
            # é€šè¿‡é¡¹ç›®ID -> ä¹¦ç±ID -> ç« èŠ‚ -> åˆ†æç»“æœçš„è·¯å¾„æŸ¥æ‰¾
            chapters = db.query(BookChapter).filter(BookChapter.book_id == project.book_id).all()
            
            analysis_result = None
            for chapter in chapters:
                chapter_analysis = db.query(AnalysisResult).filter(
                    AnalysisResult.chapter_id == chapter.id
                ).order_by(AnalysisResult.id.desc()).first()
                
                if chapter_analysis and chapter_analysis.synthesis_plan:
                    analysis_result = chapter_analysis
                    logger.info(f"[ENV_GEN_API] æ‰¾åˆ°ç« èŠ‚{chapter.chapter_number}çš„åˆ†æç»“æœ")
                    break
            
            if not analysis_result or not analysis_result.synthesis_plan:
                raise HTTPException(
                    status_code=422, 
                    detail=f"é¡¹ç›® {request.project_id} æ²¡æœ‰å¯ç”¨çš„åˆæˆè®¡åˆ’æ•°æ®ã€‚è¯·å…ˆå®Œæˆæ™ºèƒ½å‡†å¤‡æ­¥éª¤ã€‚"
                )
            
            # ä»æ•°æ®åº“ä¸­æå–synthesis_plan
            if isinstance(analysis_result.synthesis_plan, dict) and 'synthesis_plan' in analysis_result.synthesis_plan:
                request.synthesis_plan = analysis_result.synthesis_plan['synthesis_plan']
                logger.info(f"[ENV_GEN_API] ä»æ•°æ®åº“è·å–åˆ°synthesis_planï¼Œå…±{len(request.synthesis_plan)}ä¸ªæ®µè½")
            else:
                raise HTTPException(
                    status_code=422, 
                    detail=f"é¡¹ç›® {request.project_id} çš„åˆæˆè®¡åˆ’æ•°æ®æ ¼å¼ä¸æ­£ç¡®"
                )
        
        # éªŒè¯synthesis_planæ ¼å¼
        if not isinstance(request.synthesis_plan, list) or len(request.synthesis_plan) == 0:
            raise HTTPException(
                status_code=422, 
                detail=f"synthesis_planå¿…é¡»æ˜¯éç©ºçš„åˆ—è¡¨æ ¼å¼ï¼Œå½“å‰ç±»å‹: {type(request.synthesis_plan)}"
            )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ—ç™½å†…å®¹
        logger.error(f"ğŸ” [ENV_GEN_API] è°ƒè¯•ï¼šæ£€æŸ¥æ—ç™½å†…å®¹ï¼Œsynthesis_planæœ‰{len(request.synthesis_plan)}ä¸ªæ®µè½")
        for i, seg in enumerate(request.synthesis_plan):
            logger.error(f"ğŸ” æ®µè½{i+1}: speaker='{seg.get('speaker')}', character='{seg.get('character')}', text='{seg.get('text', '')[:30]}...'")
        
        # æ”¯æŒå¤šç§æ—ç™½æ ‡è¯†
        narration_speakers = ['æ—ç™½', 'narrator', 'å™è¿°è€…', 'narration']
        narration_count = len([seg for seg in request.synthesis_plan 
                              if seg.get('speaker') in narration_speakers or seg.get('character') in narration_speakers])
        
        logger.error(f"ğŸ” [ENV_GEN_API] æ—ç™½æ£€æµ‹ç»“æœï¼šnarration_count = {narration_count}")
        
        if narration_count == 0:
            logger.warning(f"[ENV_GEN_API] é¡¹ç›® {request.project_id} æ²¡æœ‰æ—ç™½å†…å®¹ï¼Œæ— æ³•åˆ†æç¯å¢ƒéŸ³")
            return {
                'success': True,
                'project_id': request.project_id,
                'session_id': session_id,
                'analysis_result': {'environment_tracks': []},
                'analysis_stats': {'total_tracks': 0, 'total_duration': 0.0},
                'message': 'è¯¥é¡¹ç›®æ²¡æœ‰æ—ç™½å†…å®¹ï¼Œæ— éœ€ç¯å¢ƒéŸ³åˆ†æ'
            }
        
        logger.info(f"[ENV_GEN_API] æ£€æµ‹åˆ°{narration_count}ä¸ªæ—ç™½æ®µè½ï¼Œå¼€å§‹åˆ†æ")
        
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = NarrationEnvironmentAnalyzer()
        
        # åˆ†æsynthesis_plan
        analysis_result = await analyzer.extract_and_analyze_narration(
            request.synthesis_plan
        )
        
        # ç”Ÿæˆåˆ†æç»Ÿè®¡
        analysis_stats = analyzer.get_analysis_stats(analysis_result)
        
        # ä¿å­˜ä¼šè¯æ•°æ®
        _analysis_sessions[session_id] = {
            'project_id': request.project_id,
            'analysis_result': analysis_result,
            'analysis_stats': analysis_stats,
            'session_stage': 'analyzed',
            'options': request.options
        }
        
        logger.info(f"[ENV_GEN_API] åˆ†æå®Œæˆï¼Œé¡¹ç›®ID: {request.project_id}ï¼Œ"
                   f"æ£€æµ‹åˆ°{len(analysis_result.get('environment_tracks', []))}ä¸ªç¯å¢ƒè½¨é“")
        
        return {
            'success': True,
            'project_id': request.project_id,
            'session_id': session_id,
            'analysis_result': analysis_result,
            'analysis_stats': analysis_stats
        }
        
    except HTTPException:
        # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
        raise
    except Exception as e:
        logger.error(f"[ENV_GEN_API] åˆ†æå¤±è´¥ï¼Œé¡¹ç›®ID: {request.project_id}ï¼Œé”™è¯¯: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç¯å¢ƒéŸ³åˆ†æå¤±è´¥: {str(e)}")

@router.post("/prepare-validation/{project_id}")
async def prepare_validation(project_id: int) -> Dict[str, Any]:
    """
    å‡†å¤‡äººå·¥æ ¡å¯¹
    ç¬¬äºŒæ­¥ï¼šåº”ç”¨åœºæ™¯ç»§æ‰¿é€»è¾‘ï¼Œç”Ÿæˆæ ¡å¯¹æ•°æ®
    """
    session_id = get_session_id(project_id)
    
    if session_id not in _analysis_sessions:
        raise HTTPException(status_code=404, detail="åˆ†æä¼šè¯ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ‰§è¡Œåˆ†ææ­¥éª¤")
        
    session_data = _analysis_sessions[session_id]
    if session_data['session_stage'] != 'analyzed':
        raise HTTPException(status_code=400, detail=f"ä¼šè¯çŠ¶æ€é”™è¯¯: {session_data['session_stage']}")
        
    try:
        # åˆå§‹åŒ–æ ¡å¯¹å™¨
        validator = EnvironmentConfigValidator()
        
        # å‡†å¤‡æ ¡å¯¹æ•°æ®
        environment_tracks = session_data['analysis_result'].get('environment_tracks', [])
        validation_data = validator.prepare_validation_data(environment_tracks)
        
        # æ›´æ–°ä¼šè¯æ•°æ®
        session_data['validation_data'] = validation_data
        session_data['session_stage'] = 'validation_prepared'
        
        logger.info(f"[ENV_GEN_API] æ ¡å¯¹æ•°æ®å‡†å¤‡å®Œæˆï¼Œé¡¹ç›®ID: {project_id}ï¼Œ"
                   f"å¾…æ ¡å¯¹è½¨é“: {validation_data['validation_summary']['total_tracks']}ä¸ª")
        
        return {
            'success': True,
            'project_id': project_id,
            'validation_data': validation_data,
            'validation_summary': validator.get_validation_summary(validation_data)
        }
        
    except Exception as e:
        logger.error(f"[ENV_GEN_API] æ ¡å¯¹å‡†å¤‡å¤±è´¥ï¼Œé¡¹ç›®ID: {project_id}ï¼Œé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ ¡å¯¹å‡†å¤‡å¤±è´¥: {str(e)}")

@router.post("/edit-validation/{project_id}")
async def edit_validation(
    project_id: int,
    request: ValidationEditRequest
) -> Dict[str, Any]:
    """
    åº”ç”¨äººå·¥ç¼–è¾‘
    æ ¡å¯¹æ­¥éª¤çš„å­é¡¹ï¼šç¯å¢ƒéŸ³IDåŒ¹é…
    """
    session_id = get_session_id(project_id)
    
    if session_id not in _analysis_sessions:
        raise HTTPException(status_code=404, detail="åˆ†æä¼šè¯ä¸å­˜åœ¨")
        
    session_data = _analysis_sessions[session_id]
    if 'validation_data' not in session_data:
        raise HTTPException(status_code=400, detail="æ ¡å¯¹æ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ‰§è¡Œæ ¡å¯¹å‡†å¤‡")
        
    try:
        # åˆå§‹åŒ–æ ¡å¯¹å™¨
        validator = EnvironmentConfigValidator()
        
        # åº”ç”¨äººå·¥ç¼–è¾‘
        updated_validation_data = validator.apply_manual_edits(
            request.track_index,
            request.manual_edits,
            session_data['validation_data']
        )
        
        # æ›´æ–°ä¼šè¯æ•°æ®
        session_data['validation_data'] = updated_validation_data
        
        logger.info(f"[ENV_GEN_API] äººå·¥ç¼–è¾‘å·²åº”ç”¨ï¼Œé¡¹ç›®ID: {project_id}ï¼Œ"
                   f"è½¨é“ç´¢å¼•: {request.track_index}")
        
        return {
            'success': True,
            'project_id': project_id,
            'track_index': request.track_index,
            'updated_track': updated_validation_data['validation_tracks'][request.track_index],
            'validation_summary': validator.get_validation_summary(updated_validation_data)
        }
        
    except Exception as e:
        logger.error(f"[ENV_GEN_API] äººå·¥ç¼–è¾‘å¤±è´¥ï¼Œé¡¹ç›®ID: {project_id}ï¼Œé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"äººå·¥ç¼–è¾‘å¤±è´¥: {str(e)}")

@router.post("/approve-validation/{project_id}")
async def approve_validation(
    project_id: int,
    request: ValidationApprovalRequest
) -> Dict[str, Any]:
    """
    æ ¡å¯¹å®¡æ‰¹ (é€šè¿‡/æ‹’ç»/éœ€è¦ä¿®æ”¹)
    """
    session_id = get_session_id(project_id)
    
    if session_id not in _analysis_sessions:
        raise HTTPException(status_code=404, detail="åˆ†æä¼šè¯ä¸å­˜åœ¨")
        
    session_data = _analysis_sessions[session_id]
    if 'validation_data' not in session_data:
        raise HTTPException(status_code=400, detail="æ ¡å¯¹æ•°æ®ä¸å­˜åœ¨")
        
    try:
        # åˆå§‹åŒ–æ ¡å¯¹å™¨
        validator = EnvironmentConfigValidator()
        
        # æ‰§è¡Œæ ¡å¯¹å®¡æ‰¹
        updated_validation_data = validator.validate_track(
            request.track_index,
            request.validation_result,
            session_data['validation_data'],
            request.notes
        )
        
        # æ›´æ–°ä¼šè¯æ•°æ®
        session_data['validation_data'] = updated_validation_data
        
        # è·å–æ ¡å¯¹æ€»ç»“
        validation_summary = validator.get_validation_summary(updated_validation_data)
        
        logger.info(f"[ENV_GEN_API] æ ¡å¯¹å®¡æ‰¹å®Œæˆï¼Œé¡¹ç›®ID: {project_id}ï¼Œ"
                   f"è½¨é“ç´¢å¼•: {request.track_index}ï¼Œç»“æœ: {request.validation_result}")
        
        return {
            'success': True,
            'project_id': project_id,
            'track_index': request.track_index,
            'validation_result': request.validation_result,
            'validation_summary': validation_summary,
            'ready_for_persistence': validation_summary['ready_for_persistence']
        }
        
    except Exception as e:
        logger.error(f"[ENV_GEN_API] æ ¡å¯¹å®¡æ‰¹å¤±è´¥ï¼Œé¡¹ç›®ID: {project_id}ï¼Œé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ ¡å¯¹å®¡æ‰¹å¤±è´¥: {str(e)}")

@router.post("/finalize/{project_id}")
async def finalize_environment_generation(project_id: int) -> Dict[str, Any]:
    """
    å®Œæˆç¯å¢ƒéŸ³ç”Ÿæˆæµç¨‹
    ç¬¬ä¸‰æ­¥ï¼šæŒä¹…åŒ–JSONé…ç½®
    æ”¯æŒç›´æ¥ä»åˆ†æç»“æœç”Ÿæˆé…ç½®ï¼ˆè·³è¿‡æ ¡å¯¹æ­¥éª¤ï¼‰
    """
    session_id = get_session_id(project_id)
    
    if session_id not in _analysis_sessions:
        raise HTTPException(status_code=404, detail="åˆ†æä¼šè¯ä¸å­˜åœ¨")
        
    session_data = _analysis_sessions[session_id]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æç»“æœ
    if 'analysis_result' not in session_data:
        raise HTTPException(status_code=400, detail="åˆ†æç»“æœä¸å­˜åœ¨")
        
    try:
        # å¦‚æœæœ‰æ ¡å¯¹æ•°æ®ï¼Œä½¿ç”¨æ ¡å¯¹åçš„é…ç½®
        if 'validation_data' in session_data:
            # åˆå§‹åŒ–æ ¡å¯¹å™¨
            validator = EnvironmentConfigValidator()
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥æŒä¹…åŒ–
            validation_summary = validator.get_validation_summary(session_data['validation_data'])
            if not validation_summary['ready_for_persistence']:
                raise HTTPException(
                    status_code=400, 
                    detail=f"æ ¡å¯¹æœªå®Œæˆï¼Œæ— æ³•æŒä¹…åŒ–ã€‚å·²å®¡æ‰¹: {validation_summary['status_distribution'].get('approved', 0)}/"
                          f"{validation_summary['total_tracks']}"
                )
                
            # å‡†å¤‡æŒä¹…åŒ–æ•°æ®
            persistence_data = validator.prepare_for_persistence(session_data['validation_data'])
            
            if not persistence_data['ready']:
                raise HTTPException(status_code=400, detail="æ²¡æœ‰æ ¡å¯¹é€šè¿‡çš„è½¨é“ï¼Œæ— æ³•æŒä¹…åŒ–")
        else:
            # ç›´æ¥ä»åˆ†æç»“æœç”Ÿæˆé…ç½®ï¼ˆè·³è¿‡æ ¡å¯¹æ­¥éª¤ï¼‰
            analysis_result = session_data['analysis_result']
            environment_tracks = analysis_result.get('environment_tracks', [])
            
            # ä¸ºæ¯ä¸ªè½¨é“ç”Ÿæˆé»˜è®¤çš„TangoFluxé…ç½®
            final_tracks = []
            for i, track in enumerate(environment_tracks):
                # ç”Ÿæˆé»˜è®¤çš„TangoFluxæç¤ºè¯
                keywords = track.get('environment_keywords', [])
                default_prompt = ', '.join(keywords) if keywords else 'ambient environment sound'
                
                final_track = {
                    'track_index': i,
                    'segment_id': track['segment_id'],
                    'start_time': track['start_time'],
                    'duration': track['duration'],
                    'environment_keywords': keywords,
                    'scene_description': track.get('scene_description', ''),
                    'confidence': track.get('confidence', 0.8),
                    'tangoflux_config': {
                        'prompt': default_prompt,
                        'volume': 0.6,
                        'duration': track['duration'],
                        'fade_in': 3.0,
                        'fade_out': 2.0,
                        'loop': True
                    },
                    'validation_status': 'auto_approved',
                    'auto_generated': True
                }
                final_tracks.append(final_track)
            
            # æ„å»ºæŒä¹…åŒ–æ•°æ®
            persistence_data = {
                'ready': True,
                'persistence_tracks': final_tracks,
                'persistence_summary': {
                    'total_tracks': len(final_tracks),
                    'approved_tracks_count': len(final_tracks),
                    'total_duration': sum(track['duration'] for track in final_tracks),
                    'inheritance_count': 0,
                    'manual_edits_count': 0,
                    'auto_generated_count': len(final_tracks)
                },
                'generation_method': 'direct_from_analysis'
            }
            
        # TODO: å®é™…æŒä¹…åŒ–åˆ°æ•°æ®åº“
        # è¿™é‡Œåº”è¯¥è°ƒç”¨æ•°æ®åº“æœåŠ¡ä¿å­˜environment_config JSON
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        session_data['persistence_data'] = persistence_data
        session_data['session_stage'] = 'completed'
        
        # ç”Ÿæˆæœ€ç»ˆé…ç½®
        final_config = {
            'environment_tracks': persistence_data['persistence_tracks'],
            'global_settings': {
                'default_volume': 0.6,
                'default_fade_in': 3.0,
                'default_fade_out': 2.0
            },
            'generation_metadata': {
                'project_id': project_id,
                'generation_method': persistence_data.get('generation_method', 'validation_based'),
                'generation_timestamp': session_data.get('analysis_result', {}).get('analysis_timestamp'),
                'total_tracks': persistence_data['persistence_summary']['total_tracks']
            }
        }
        
        # å°†æœ€ç»ˆé…ç½®æ·»åŠ åˆ°ä¼šè¯æ•°æ®
        session_data['final_config'] = final_config
        
        logger.info(f"[ENV_GEN_API] ç¯å¢ƒéŸ³ç”Ÿæˆå®Œæˆï¼Œé¡¹ç›®ID: {project_id}ï¼Œ"
                   f"æŒä¹…åŒ–{len(persistence_data['persistence_tracks'])}ä¸ªè½¨é“")
        
        return {
            'success': True,
            'project_id': project_id,
            'session_stage': 'completed',
            'config': final_config,
            'analysis_stats': session_data.get('analysis_stats', {}),
            'persistence_data': persistence_data,
            'next_steps': {
                'description': 'ç¯å¢ƒéŸ³é…ç½®å·²ç”Ÿæˆï¼Œå¯ä»¥è¿›è¡ŒéŸ³é¢‘æ··åˆ',
                'suggested_action': 'åœ¨åˆæˆä¸­å¿ƒæ‰§è¡ŒéŸ³é¢‘æ··åˆæµç¨‹'
            }
        }
        
    except Exception as e:
        logger.error(f"[ENV_GEN_API] ç¯å¢ƒéŸ³ç”Ÿæˆå®Œæˆå¤±è´¥ï¼Œé¡¹ç›®ID: {project_id}ï¼Œé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç¯å¢ƒéŸ³ç”Ÿæˆå®Œæˆå¤±è´¥: {str(e)}")

@router.get("/status/{project_id}")
async def get_generation_status(project_id: int) -> Dict[str, Any]:
    """
    è·å–ç¯å¢ƒéŸ³ç”ŸæˆçŠ¶æ€
    """
    session_id = get_session_id(project_id)
    
    if session_id not in _analysis_sessions:
        return {
            'exists': False,
            'project_id': project_id,
            'session_stage': 'not_started'
        }
        
    session_data = _analysis_sessions[session_id]
    
    # æ„å»ºçŠ¶æ€å“åº”
    status_response = {
        'exists': True,
        'project_id': project_id,
        'session_stage': session_data.get('session_stage', 'unknown')
    }
    
    # æ ¹æ®é˜¶æ®µæ·»åŠ ç›¸åº”æ•°æ®
    if 'analysis_stats' in session_data:
        status_response['analysis_stats'] = session_data['analysis_stats']
        
    if 'validation_data' in session_data:
        validator = EnvironmentConfigValidator()
        status_response['validation_summary'] = validator.get_validation_summary(
            session_data['validation_data']
        )
        
    if 'persistence_data' in session_data:
        status_response['persistence_summary'] = session_data['persistence_data']['persistence_summary']
        
    return status_response

@router.get("/config/{project_id}")
async def get_environment_config(project_id: int) -> Dict[str, Any]:
    """
    è·å–é¡¹ç›®çš„ç¯å¢ƒéŸ³é…ç½®ï¼ˆç±»ä¼¼è§’è‰²ç®¡ç†ï¼‰
    è¿”å›å·²åˆ†æçš„ç¯å¢ƒéŸ³è½¨é“é…ç½®ï¼Œæ”¯æŒæŸ¥çœ‹å’Œç¼–è¾‘
    """
    session_id = get_session_id(project_id)
    
    if session_id not in _analysis_sessions:
        return {
            'success': False,
            'error': 'No session found',
            'message': 'æœªæ‰¾åˆ°ç¯å¢ƒéŸ³åˆ†æä¼šè¯',
            'config': None
        }
        
    session_data = _analysis_sessions[session_id]
    
    # æ„å»ºé…ç½®å“åº”
    config_response = {
        'success': True,
        'project_id': project_id,
        'session_stage': session_data.get('session_stage', 'unknown'),
        'config': {
            'environment_tracks': [],
            'global_settings': {
                'default_volume': 0.6,
                'default_fade_in': 3.0,
                'default_fade_out': 2.0
            }
        }
    }
    
    # ä¼˜å…ˆä½¿ç”¨æœ€ç»ˆé…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'final_config' in session_data:
        config_response['config'] = session_data['final_config']
    # å…¶æ¬¡ä½¿ç”¨æ ¡å¯¹åçš„é…ç½®
    elif 'validation_data' in session_data:
        validation_tracks = session_data['validation_data'].get('validation_tracks', [])
        
        for i, val_track in enumerate(validation_tracks):
            track_config = {
                'track_index': i,
                'segment_id': val_track['segment_id'],
                'start_time': val_track['start_time'],
                'duration': val_track['duration'],
                'environment_keywords': val_track.get('environment_keywords', []),
                'scene_description': val_track.get('scene_description', ''),
                'confidence': val_track.get('confidence', 0.0),
                'tangoflux_config': val_track.get('tangoflux_config', {}),
                'validation_status': val_track.get('validation_status', 'pending'),
                'user_confirmed': val_track.get('user_confirmed', False),
                'inheritance_applied': val_track.get('inheritance_applied', False),
                'inherited_environment': val_track.get('inherited_environment')
            }
            
            config_response['config']['environment_tracks'].append(track_config)
    # æœ€åä½¿ç”¨åŸå§‹åˆ†æç»“æœ
    elif 'analysis_result' in session_data:
        tracks = session_data['analysis_result'].get('environment_tracks', [])
        
        for i, track in enumerate(tracks):
            # ç”Ÿæˆé»˜è®¤çš„TangoFluxé…ç½®
            keywords = track.get('environment_keywords', [])
            default_prompt = ', '.join(keywords) if keywords else 'ambient environment sound'
            
            track_config = {
                'track_index': i,
                'segment_id': track['segment_id'],
                'start_time': track['start_time'],
                'duration': track['duration'],
                'environment_keywords': keywords,
                'scene_description': track.get('scene_description', ''),
                'confidence': track.get('confidence', 0.0),
                'tangoflux_config': {
                    'prompt': default_prompt,
                    'volume': 0.6,
                    'duration': track['duration'],
                    'fade_in': 3.0,
                    'fade_out': 2.0,
                    'loop': True
                },
                'validation_status': 'auto_generated',
                'user_confirmed': False
            }
            
            config_response['config']['environment_tracks'].append(track_config)
    

    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    if 'analysis_stats' in session_data:
        config_response['analysis_stats'] = session_data['analysis_stats']
    
    return config_response

@router.put("/track/{project_id}/{track_index}")
async def update_track_config(
    project_id: int, 
    track_index: int, 
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    æ›´æ–°ç¯å¢ƒéŸ³è½¨é“é…ç½®ï¼ˆæ”¯æŒæ‰‹åŠ¨è®¾ç½®ç¯å¢ƒéŸ³IDï¼‰
    """
    session_id = get_session_id(project_id)
    
    if session_id not in _analysis_sessions:
        raise HTTPException(status_code=404, detail="åˆ†æä¼šè¯ä¸å­˜åœ¨")
        
    session_data = _analysis_sessions[session_id]
    
    if 'analysis_result' not in session_data:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰åˆ†æç»“æœå¯ä»¥æ›´æ–°")
    
    tracks = session_data['analysis_result'].get('environment_tracks', [])
    
    if track_index < 0 or track_index >= len(tracks):
        raise HTTPException(status_code=400, detail=f"è½¨é“ç´¢å¼• {track_index} è¶…å‡ºèŒƒå›´")
    
    try:
        track = tracks[track_index]
        
        # æ›´æ–°å…è®¸çš„é…ç½®é¡¹
        updatable_fields = [
            'environment_keywords', 'scene_description', 'environment_sound_id',
            'tangoflux_config', 'volume', 'fade_in', 'fade_out', 'loop_enabled',
            'validation_status', 'user_confirmed'
        ]
        
        for field in updatable_fields:
            if field in config:
                track[field] = config[field]
        
        # ç‰¹æ®Šå¤„ç†ç¯å¢ƒéŸ³IDå…³è”
        if 'environment_sound_id' in config:
            track['environment_sound_id'] = config['environment_sound_id']
            # å¦‚æœé€‰æ‹©äº†ç¯å¢ƒéŸ³åº“ä¸­çš„éŸ³é¢‘ï¼Œæ ‡è®°ä¸ºåº“éŸ³é¢‘
            track['use_library_sound'] = config['environment_sound_id'] is not None
        
        # æ›´æ–°æ—¶é—´æˆ³
        track['updated_at'] = datetime.now().isoformat()
        
        logger.info(f"[ENV_GEN_API] æ›´æ–°è½¨é“é…ç½®ï¼Œé¡¹ç›®ID: {project_id}ï¼Œè½¨é“: {track_index}")
        
        return {
            'success': True,
            'project_id': project_id,
            'track_index': track_index,
            'updated_config': track,
            'message': f'è½¨é“ {track_index + 1} é…ç½®æ›´æ–°æˆåŠŸ'
        }
        
    except Exception as e:
        logger.error(f"[ENV_GEN_API] æ›´æ–°è½¨é“é…ç½®å¤±è´¥ï¼Œé¡¹ç›®ID: {project_id}ï¼Œé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°è½¨é“é…ç½®å¤±è´¥: {str(e)}")

@router.delete("/session/{project_id}")
async def clear_generation_session(project_id: int) -> Dict[str, Any]:
    """
    æ¸…é™¤ç¯å¢ƒéŸ³ç”Ÿæˆä¼šè¯
    """
    session_id = get_session_id(project_id)
    
    if session_id in _analysis_sessions:
        del _analysis_sessions[session_id]
        logger.info(f"[ENV_GEN_API] ä¼šè¯å·²æ¸…é™¤ï¼Œé¡¹ç›®ID: {project_id}")
        return {
            'success': True,
            'project_id': project_id,
            'message': 'ä¼šè¯å·²æ¸…é™¤'
        }
    else:
        return {
            'success': False,
            'project_id': project_id,
            'message': 'ä¼šè¯ä¸å­˜åœ¨'
        }

# === æ–°æµç¨‹APIç«¯ç‚¹ ===

@router.post("/chapters/analyze")
async def analyze_chapters_environment(
    request: ChapterEnvironmentAnalysisRequest,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    ç« èŠ‚çº§ç¯å¢ƒéŸ³æ™ºèƒ½åˆ†æ - æ–°æµç¨‹ç¬¬2æ­¥
    æ”¯æŒå¤šç« èŠ‚æ‰¹é‡åˆ†æï¼Œç”Ÿæˆç²¾ç¡®æ—¶é—´è½´å’Œå¼ºåº¦é…ç½®
    """
    logger.info(f"[CHAPTER_ENV_API] å¼€å§‹ç« èŠ‚ç¯å¢ƒéŸ³åˆ†æï¼Œç« èŠ‚æ•°: {len(request.chapter_ids)}")
    
    try:
        # éªŒè¯ç« èŠ‚æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ™ºèƒ½å‡†å¤‡ç»“æœ
        analysis_results = []
        chapter_contents = {}
        
        for chapter_id in request.chapter_ids:
            # è·å–ç« èŠ‚ä¿¡æ¯
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise HTTPException(status_code=404, detail=f"ç« èŠ‚ {chapter_id} ä¸å­˜åœ¨")
            
            # è·å–ç« èŠ‚çš„æ™ºèƒ½å‡†å¤‡ç»“æœ
            analysis_result = db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter_id,
                AnalysisResult.status == 'completed',
                AnalysisResult.synthesis_plan.isnot(None)
            ).order_by(AnalysisResult.created_at.desc()).first()
            
            if not analysis_result or not analysis_result.synthesis_plan:
                raise HTTPException(
                    status_code=422, 
                    detail=f"ç« èŠ‚ {chapter_id} ({chapter.chapter_title}) æ²¡æœ‰å®Œæˆæ™ºèƒ½å‡†å¤‡ï¼Œè¯·å…ˆæ‰§è¡Œæ™ºèƒ½å‡†å¤‡æ­¥éª¤"
                )
            
            # æå–synthesis_plan
            synthesis_plan = []
            if isinstance(analysis_result.synthesis_plan, dict) and 'synthesis_plan' in analysis_result.synthesis_plan:
                synthesis_plan = analysis_result.synthesis_plan['synthesis_plan']
            elif isinstance(analysis_result.synthesis_plan, list):
                synthesis_plan = analysis_result.synthesis_plan
            
            if not synthesis_plan:
                raise HTTPException(
                    status_code=422,
                    detail=f"ç« èŠ‚ {chapter_id} çš„åˆæˆè®¡åˆ’æ ¼å¼ä¸æ­£ç¡®"
                )
            
            analysis_results.append({
                'chapter_id': chapter_id,
                'chapter_title': chapter.chapter_title,
                'chapter_number': chapter.chapter_number,
                'synthesis_plan': synthesis_plan,
                'word_count': chapter.word_count
            })
            
            chapter_contents[chapter_id] = chapter.content or ""
        
        logger.info(f"[CHAPTER_ENV_API] éªŒè¯é€šè¿‡ï¼Œå¼€å§‹åˆ†æ{len(analysis_results)}ä¸ªç« èŠ‚")
        
        # åˆå§‹åŒ–å¢å¼ºåˆ†æå™¨
        analyzer = ChapterEnvironmentAnalyzer()
        
        # é€ç« èŠ‚åˆ†æ
        chapter_analysis_results = []
        total_tracks = 0
        total_duration = 0.0
        
        for chapter_data in analysis_results:
            chapter_id = chapter_data['chapter_id']
            chapter_content = chapter_contents[chapter_id]
            synthesis_plan = chapter_data['synthesis_plan']
            
            logger.info(f"[CHAPTER_ENV_API] åˆ†æç« èŠ‚ {chapter_id}: {chapter_data['chapter_title']}")
            
            # æ‰§è¡Œç« èŠ‚çº§åˆ†æ
            chapter_result = await analyzer.analyze_chapter_environment(
                chapter_content=chapter_content,
                synthesis_plan=synthesis_plan,
                options=request.analysis_options
            )
            
            if chapter_result['success']:
                # æ·»åŠ ç« èŠ‚ä¿¡æ¯
                chapter_result['chapter_info'] = {
                    'chapter_id': chapter_id,
                    'chapter_title': chapter_data['chapter_title'],
                    'chapter_number': chapter_data['chapter_number'],
                    'word_count': chapter_data['word_count']
                }
                
                chapter_analysis_results.append(chapter_result)
                
                # ç»Ÿè®¡ä¿¡æ¯
                tracks_count = len(chapter_result['analysis_result']['environment_tracks'])
                duration = chapter_result['analysis_result']['analysis_metadata']['total_duration']
                total_tracks += tracks_count
                total_duration += duration
                
                logger.info(f"[CHAPTER_ENV_API] ç« èŠ‚ {chapter_id} åˆ†æå®Œæˆ: {tracks_count}ä¸ªè½¨é“, {duration:.1f}ç§’")
            else:
                logger.error(f"[CHAPTER_ENV_API] ç« èŠ‚ {chapter_id} åˆ†æå¤±è´¥")
                raise HTTPException(status_code=500, detail=f"ç« èŠ‚ {chapter_id} åˆ†æå¤±è´¥")
        
        # ç”Ÿæˆç»¼åˆç»“æœ
        combined_result = {
            'success': True,
            'analysis_timestamp': datetime.now().isoformat(),
            'chapters_analyzed': len(chapter_analysis_results),
            'total_tracks': total_tracks,
            'total_duration': round(total_duration, 1),
            'avg_tracks_per_chapter': round(total_tracks / len(chapter_analysis_results), 1) if chapter_analysis_results else 0,
            'chapters': chapter_analysis_results,
            'analysis_summary': {
                'analyzer_version': '2.0_enhanced',
                'features_enabled': [
                    'precise_timing',
                    'intensity_analysis', 
                    'continuity_analysis',
                    'video_timeline_generation'
                ],
                'ready_for_matching': True
            }
        }
        
        logger.info(f"[CHAPTER_ENV_API] ç« èŠ‚ç¯å¢ƒéŸ³åˆ†æå®Œæˆ: {len(chapter_analysis_results)}ä¸ªç« èŠ‚, "
                   f"æ€»è®¡{total_tracks}ä¸ªè½¨é“, {total_duration:.1f}ç§’")
        
        return combined_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[CHAPTER_ENV_API] ç« èŠ‚ç¯å¢ƒéŸ³åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç« èŠ‚ç¯å¢ƒéŸ³åˆ†æå¤±è´¥: {str(e)}")

@router.get("/chapters/{chapter_id}/timeline")
async def get_chapter_timeline(
    chapter_id: int,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    è·å–ç« èŠ‚ç¯å¢ƒéŸ³æ—¶é—´è½´ - ç”¨äºé¢„è§ˆå’Œç¼–è¾‘
    """
    try:
        # è¿™é‡Œåº”è¯¥ä»ç¼“å­˜æˆ–æ•°æ®åº“ä¸­è·å–å·²åˆ†æçš„æ—¶é—´è½´
        # æš‚æ—¶è¿”å›ç¤ºä¾‹æ•°æ®
        return {
            'success': True,
            'chapter_id': chapter_id,
            'timeline': {
                'timeline_version': '1.0',
                'total_duration': 300.0,
                'tracks': []
            },
            'message': 'æ—¶é—´è½´è·å–åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­'
        }
        
    except Exception as e:
        logger.error(f"[CHAPTER_ENV_API] è·å–ç« èŠ‚æ—¶é—´è½´å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ—¶é—´è½´å¤±è´¥: {str(e)}")

@router.post("/match-sounds")
async def match_environment_sounds(
    request: EnvironmentMatchingRequest,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    ç¯å¢ƒéŸ³æ™ºèƒ½åŒ¹é… - æ–°æµç¨‹ç¬¬3æ­¥
    ä¸ºåˆ†æç»“æœä¸­çš„ç¯å¢ƒéŸ³éœ€æ±‚åŒ¹é…å·²æœ‰ç¯å¢ƒéŸ³
    """
    logger.info("[MATCHING_API] å¼€å§‹ç¯å¢ƒéŸ³æ™ºèƒ½åŒ¹é…")
    
    try:
        # åˆå§‹åŒ–åŒ¹é…å¼•æ“
        matching_engine = SoundMatchingEngine()
        
        # æ‰§è¡Œæ‰¹é‡åŒ¹é…
        enhanced_result = await matching_engine.batch_match_analysis_result(
            request.analysis_result,
            db
        )
        
        # ç”Ÿæˆç¯å¢ƒéŸ³ç”Ÿæˆè®¡åˆ’
        generation_plan = matching_engine.get_generation_plan(enhanced_result)
        
        # æ„å»ºå“åº”
        matching_response = {
            'success': True,
            'matching_timestamp': datetime.now().isoformat(),
            'enhanced_analysis_result': enhanced_result,
            'generation_plan': generation_plan,
            'matching_summary': enhanced_result.get('matching_summary', {}),
            'ready_for_generation': len(generation_plan['need_generation']) > 0
        }
        
        # å®‰å…¨åœ°è·å–åŒ¹é…æ±‡æ€»ä¿¡æ¯
        matching_summary = enhanced_result.get('matching_summary', {})
        matched_tracks = matching_summary.get('matched_tracks', 0)
        need_generation_tracks = matching_summary.get('need_generation_tracks', 0)
        
        logger.info(f"[MATCHING_API] åŒ¹é…å®Œæˆ: "
                   f"{matched_tracks}ä¸ªå·²åŒ¹é…, "
                   f"{need_generation_tracks}ä¸ªéœ€ç”Ÿæˆ")
        
        return matching_response
        
    except Exception as e:
        logger.error(f"[MATCHING_API] ç¯å¢ƒéŸ³åŒ¹é…å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç¯å¢ƒéŸ³åŒ¹é…å¤±è´¥: {str(e)}")

@router.get("/sounds/search")
async def search_environment_sounds(
    keywords: str,
    max_results: int = 10,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    æœç´¢ç¯å¢ƒéŸ³ - æ”¯æŒå…³é”®è¯åŒ¹é…
    """
    try:
        matching_engine = SoundMatchingEngine()
        keyword_list = [kw.strip() for kw in keywords.split(',') if kw.strip()]
        
        matches = await matching_engine.find_matching_sounds(keyword_list, db, max_results)
        
        return {
            'success': True,
            'keywords': keyword_list,
            'total_matches': len(matches),
            'matches': [match.to_dict() for match in matches]
        }
        
    except Exception as e:
        logger.error(f"[MATCHING_API] ç¯å¢ƒéŸ³æœç´¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç¯å¢ƒéŸ³æœç´¢å¤±è´¥: {str(e)}")

@router.post("/generate-sounds")
async def generate_environment_sounds(
    request: EnvironmentGenerationRequest,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    ç¯å¢ƒéŸ³æ‰¹é‡ç”Ÿæˆ - æ–°æµç¨‹ç¬¬4æ­¥
    ä½¿ç”¨TangoFlux AIç”Ÿæˆéœ€è¦çš„ç¯å¢ƒéŸ³
    """
    logger.info("[GENERATION_API] å¼€å§‹ç¯å¢ƒéŸ³æ‰¹é‡ç”Ÿæˆ")
    
    try:
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = TangoFluxEnvironmentGenerator()
        
        # æ£€æŸ¥TangoFluxæœåŠ¡å¥åº·çŠ¶æ€
        if not await generator.check_service_health():
            raise HTTPException(status_code=503, detail="TangoFluxæœåŠ¡ä¸å¯ç”¨")
        
        # å‡†å¤‡ç”Ÿæˆè¯·æ±‚
        generation_requests = []
        for item in request.generation_plan:
            generation_request = {
                'keyword': item.get('keyword', ''),
                'description': item.get('example_scene', ''),
                'duration': item.get('suggested_duration', 30.0),
                'intensity': item.get('intensity_level', 'medium')
            }
            generation_requests.append(generation_request)
        
        # æ‰§è¡Œæ‰¹é‡ç”Ÿæˆ
        generation_tasks = await generator.batch_generate_environment_sounds(
            generation_requests,
            max_concurrent=request.generation_options.get('max_concurrent', 3)
        )
        
        # ä¿å­˜ç”Ÿæˆç»“æœåˆ°æ•°æ®åº“
        saved_sounds = await generator.save_generated_sounds_to_database(
            generation_tasks, db
        )
        
        # æ„å»ºå“åº”
        response = {
            'success': True,
            'generation_timestamp': datetime.now().isoformat(),
            'total_requested': len(generation_requests),
            'total_completed': len([task for task in generation_tasks if task.status == 'completed']),
            'total_failed': len([task for task in generation_tasks if task.status == 'failed']),
            'generation_tasks': [task.to_dict() for task in generation_tasks],
            'saved_sounds': [
                {
                    'id': sound.id,
                    'name': sound.name,
                    'file_path': sound.file_path,
                    'duration': sound.duration,
                    'tags': sound.tags
                } for sound in saved_sounds
            ]
        }
        
        logger.info(f"[GENERATION_API] æ‰¹é‡ç”Ÿæˆå®Œæˆ: {len(saved_sounds)}ä¸ªæˆåŠŸä¿å­˜")
        
        return response
        
    except Exception as e:
        logger.error(f"[GENERATION_API] ç¯å¢ƒéŸ³ç”Ÿæˆå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç¯å¢ƒéŸ³ç”Ÿæˆå¤±è´¥: {str(e)}")

@router.post("/create-timeline")
async def create_environment_timeline(
    analysis_result: Dict[str, Any],
    matching_result: Dict[str, Any],
    project_name: Optional[str] = None,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    åˆ›å»ºç¯å¢ƒéŸ³æ—¶é—´è½´ - é›†æˆåˆ†æå’ŒåŒ¹é…ç»“æœ
    """
    logger.info("[TIMELINE_API] å¼€å§‹åˆ›å»ºç¯å¢ƒéŸ³æ—¶é—´è½´")
    
    try:
        # åˆå§‹åŒ–æ—¶é—´è½´ç”Ÿæˆå™¨
        timeline_generator = EnvironmentTimelineGenerator()
        
        # åˆ›å»ºæ—¶é—´è½´
        timeline = timeline_generator.create_timeline_from_analysis(
            analysis_result, matching_result, project_name
        )
        
        # éªŒè¯æ—¶é—´è½´
        validation_result = timeline_generator.validate_timeline(timeline)
        
        # å¯¼å‡ºé€šç”¨æ ¼å¼
        timeline_data = timeline_generator.export_timeline(timeline, 'generic')
        
        response = {
            'success': True,
            'timeline_created': True,
            'project_name': timeline.project_name,
            'total_duration': timeline.total_duration,
            'total_tracks': len(timeline.tracks),
            'timeline_data': timeline_data,
            'validation_result': validation_result,
            'created_at': datetime.now().isoformat()
        }
        
        logger.info(f"[TIMELINE_API] æ—¶é—´è½´åˆ›å»ºå®Œæˆ: {timeline.project_name}")
        
        return response
        
    except Exception as e:
        logger.error(f"[TIMELINE_API] æ—¶é—´è½´åˆ›å»ºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ—¶é—´è½´åˆ›å»ºå¤±è´¥: {str(e)}")

@router.post("/export-timeline")
async def export_timeline(
    request: TimelineExportRequest
) -> Dict[str, Any]:
    """
    å¯¼å‡ºæ—¶é—´è½´ä¸ºè§†é¢‘ç¼–è¾‘è½¯ä»¶å…¼å®¹æ ¼å¼
    """
    logger.info(f"[TIMELINE_API] å¼€å§‹å¯¼å‡ºæ—¶é—´è½´: {request.export_format}")
    
    try:
        # åˆå§‹åŒ–æ—¶é—´è½´ç”Ÿæˆå™¨
        timeline_generator = EnvironmentTimelineGenerator()
        
        # ä»è¯·æ±‚æ•°æ®é‡å»ºæ—¶é—´è½´å¯¹è±¡
        timeline_data = request.timeline_data.get('timeline', {})
        
        # è¿™é‡Œéœ€è¦ä¸€ä¸ªhelperæ–¹æ³•æ¥ä»JSONæ•°æ®é‡å»ºTimelineObject
        # ç®€åŒ–å®ç°ï¼Œç›´æ¥å¤„ç†æ•°æ®
        if request.export_format == 'premiere_pro':
            exported_data = {
                'project': {
                    'name': timeline_data.get('project_name', 'Environment_Project'),
                    'format': 'adobe_premiere_pro',
                    'version': '2.0',
                    'settings': {
                        'frame_rate': timeline_data.get('frame_rate', 30),
                        'sample_rate': timeline_data.get('sample_rate', 44100),
                        'total_duration': timeline_data.get('total_duration', 300.0)
                    }
                },
                'sequences': [{
                    'name': f"{timeline_data.get('project_name', 'Environment')}_ç¯å¢ƒéŸ³è½¨é“",
                    'duration': timeline_data.get('total_duration', 300.0),
                    'audio_tracks': [
                        {
                            'track_number': i + 1,
                            'track_name': f"ç¯å¢ƒéŸ³è½¨é“_{i+1}",
                            'clips': [{
                                'clip_id': track.get('track_id'),
                                'name': track.get('sound_name'),
                                'media_source': track.get('audio_file_path'),
                                'in_point': 0,
                                'out_point': track.get('duration', 30.0),
                                'timeline_in': track.get('start_time', 0.0),
                                'timeline_out': track.get('end_time', 30.0),
                                'volume': track.get('volume', 0.7) * 100,
                                'fade_in_duration': track.get('fade_in', 1.0),
                                'fade_out_duration': track.get('fade_out', 1.0),
                                'loop_enabled': track.get('loop_enabled', True),
                                'audio_effects': []
                            }]
                        } for i, track in enumerate(timeline_data.get('tracks', []))
                    ]
                }],
                'metadata': timeline_data.get('metadata', {})
            }
        elif request.export_format == 'davinci_resolve':
            exported_data = {
                'resolve_project': {
                    'name': timeline_data.get('project_name', 'Environment_Project'),
                    'format': 'davinci_resolve',
                    'version': '2.0',
                    'timeline_settings': {
                        'frame_rate': f"{timeline_data.get('frame_rate', 30)}fps",
                        'resolution': "1920x1080",
                        'audio_sample_rate': timeline_data.get('sample_rate', 44100)
                    }
                },
                'timeline': {
                    'name': f"{timeline_data.get('project_name', 'Environment')}_Timeline",
                    'duration_frames': int(timeline_data.get('total_duration', 300.0) * timeline_data.get('frame_rate', 30)),
                    'audio_tracks': [
                        {
                            'track_index': i + 1,
                            'track_type': "audio",
                            'clips': [{
                                'clip_name': track.get('sound_name'),
                                'media_pool_item': track.get('audio_file_path'),
                                'start_frame': int(track.get('start_time', 0.0) * timeline_data.get('frame_rate', 30)),
                                'end_frame': int(track.get('end_time', 30.0) * timeline_data.get('frame_rate', 30)),
                                'duration_frames': int(track.get('duration', 30.0) * timeline_data.get('frame_rate', 30)),
                                'volume_db': 20 * (track.get('volume', 0.7) - 1),
                                'fade_in_frames': int(track.get('fade_in', 1.0) * timeline_data.get('frame_rate', 30)),
                                'fade_out_frames': int(track.get('fade_out', 1.0) * timeline_data.get('frame_rate', 30)),
                                'loop_enabled': track.get('loop_enabled', True)
                            }]
                        } for i, track in enumerate(timeline_data.get('tracks', []))
                    ]
                },
                'metadata': timeline_data.get('metadata', {})
            }
        else:
            # é€šç”¨æ ¼å¼
            exported_data = request.timeline_data
        
        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œä¿å­˜æ–‡ä»¶
        if request.output_path:
            output_file = Path(request.output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(exported_data, f, indent=2, ensure_ascii=False)
        
        response = {
            'success': True,
            'export_format': request.export_format,
            'exported_data': exported_data,
            'output_path': request.output_path,
            'exported_at': datetime.now().isoformat()
        }
        
        logger.info(f"[TIMELINE_API] æ—¶é—´è½´å¯¼å‡ºå®Œæˆ: {request.export_format}")
        
        return response
        
    except Exception as e:
        logger.error(f"[TIMELINE_API] æ—¶é—´è½´å¯¼å‡ºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ—¶é—´è½´å¯¼å‡ºå¤±è´¥: {str(e)}")

@router.get("/generation/status/{task_id}")
async def get_generation_task_status(task_id: str) -> Dict[str, Any]:
    """
    è·å–ç”Ÿæˆä»»åŠ¡çŠ¶æ€
    """
    try:
        generator = TangoFluxEnvironmentGenerator()
        task_status = generator.get_task_status(task_id)
        
        if task_status:
            return {
                'success': True,
                'task_status': task_status
            }
        else:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
            
    except Exception as e:
        logger.error(f"[GENERATION_API] è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")

@router.get("/generation/tasks")
async def get_all_generation_tasks() -> Dict[str, Any]:
    """
    è·å–æ‰€æœ‰æ´»åŠ¨ç”Ÿæˆä»»åŠ¡çŠ¶æ€
    """
    try:
        generator = TangoFluxEnvironmentGenerator()
        all_tasks = generator.get_all_active_tasks()
        
        return {
            'success': True,
            'total_tasks': len(all_tasks),
            'tasks': all_tasks
        }
        
    except Exception as e:
        logger.error(f"[GENERATION_API] è·å–æ‰€æœ‰ä»»åŠ¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ‰€æœ‰ä»»åŠ¡å¤±è´¥: {str(e)}") 