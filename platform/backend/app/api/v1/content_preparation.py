"""
å†…å®¹å‡†å¤‡API
æä¾›å°è¯´ç« èŠ‚è¯­éŸ³åˆæˆå‰çš„æ™ºèƒ½å†…å®¹å‡†å¤‡åŠŸèƒ½
"""

from fastapi import APIRouter, Depends, HTTPException, Query, Body
from sqlalchemy.orm import Session
from pydantic import BaseModel
from typing import Optional, Dict, Any
import logging

from app.database import get_db
from app.models import BookChapter, Book, AnalysisResult
from app.services.content_preparation_service import ContentPreparationService
from app.services.intelligent_detection_service import IntelligentDetectionService

router = APIRouter(prefix="/content-preparation")
logger = logging.getLogger(__name__)

# æœåŠ¡å®ä¾‹å°†åœ¨æ¯ä¸ªè¯·æ±‚ä¸­åˆ›å»º

class PreparationRequest(BaseModel):
    """æ™ºèƒ½å‡†å¤‡è¯·æ±‚æ¨¡å‹"""
    auto_add_narrator: bool = True
    processing_mode: str = "auto"
    tts_optimization: str = "balanced"  # fast, balanced, quality


@router.get("/content-stats/{chapter_id}")
async def get_content_stats(
    chapter_id: int,
    db: Session = Depends(get_db)
):
    """
    è·å–ç« èŠ‚å†…å®¹ç»Ÿè®¡ä¿¡æ¯
    ç”¨äºå¿«é€Ÿäº†è§£ç« èŠ‚åŸºæœ¬ä¿¡æ¯å’Œå¤„ç†å»ºè®®
    """
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹å¹¶è·å–ç»Ÿè®¡ä¿¡æ¯
        content_prep_service = ContentPreparationService(db)
        stats = await content_prep_service.get_content_stats(chapter_id, db)
        
        return {
            "success": True,
            "data": stats,
            "message": "å†…å®¹ç»Ÿè®¡è·å–æˆåŠŸ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.get("/synthesis-preview/{chapter_id}")
async def get_synthesis_preview(
    chapter_id: int,
    db: Session = Depends(get_db)
):
    """
    è·å–ç« èŠ‚è¯­éŸ³åˆæˆé¢„è§ˆ
    å¿«é€Ÿåˆ†æç« èŠ‚å†…å®¹ï¼Œæä¾›è§’è‰²æ£€æµ‹å’Œåˆ†æ®µé¢„è§ˆ
    """
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹å¹¶è·å–é¢„è§ˆä¿¡æ¯
        content_prep_service = ContentPreparationService(db)
        preview = await content_prep_service.get_synthesis_preview(chapter_id, db)
        
        return {
            "success": True,
            "data": preview,
            "message": "åˆæˆé¢„è§ˆè·å–æˆåŠŸ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–åˆæˆé¢„è§ˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–åˆæˆé¢„è§ˆå¤±è´¥: {str(e)}")


@router.post("/prepare-synthesis/{chapter_id}")
async def prepare_chapter_for_synthesis(
    chapter_id: int,
    request: PreparationRequest = Body(default=PreparationRequest()),
    db: Session = Depends(get_db)
):
    """
    æ™ºèƒ½å‡†å¤‡ç« èŠ‚ç”¨äºè¯­éŸ³åˆæˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ™ºèƒ½æ–‡æœ¬åˆ†å—ï¼ˆæœ€å¤§3000 tokensï¼‰
    2. è§’è‰²å¯¹è¯æ£€æµ‹å’Œåˆ†ç¦»
    3. è‡ªåŠ¨æ·»åŠ æ—ç™½è§’è‰²
    4. ç”Ÿæˆè¯­éŸ³åˆæˆé…ç½®ï¼ˆæ”¯æŒTTSä¼˜åŒ–æ¨¡å¼ï¼‰
    5. è¾“å‡ºJSONæ ¼å¼æ•°æ®
    
    å‚æ•°ï¼š
    - auto_add_narrator: æ˜¯å¦è‡ªåŠ¨æ·»åŠ æ—ç™½è§’è‰²
    - processing_mode: å¤„ç†æ¨¡å¼ï¼ˆauto/single/distributedï¼‰
      * auto: è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å¼
      * single: å•å—åˆ†ææ¨¡å¼ï¼ˆé€‚åˆè¾ƒçŸ­ç« èŠ‚ï¼‰
      * distributed: åˆ†å¸ƒå¼åˆ†ææ¨¡å¼ï¼ˆé€‚åˆé•¿ç« èŠ‚ï¼‰
    - tts_optimization: TTSä¼˜åŒ–æ¨¡å¼ï¼ˆfast/balanced/qualityï¼‰
      * fast: å¿«é€Ÿæ¨¡å¼ï¼Œä¼˜åŒ–æ€§èƒ½
      * balanced: å¹³è¡¡æ¨¡å¼ï¼Œæ€§èƒ½ä¸è´¨é‡å…¼é¡¾
      * quality: è´¨é‡æ¨¡å¼ï¼Œæœ€é«˜è´¨é‡åˆ†æ
    """
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹å¹¶æ‰§è¡Œæ™ºèƒ½å‡†å¤‡
        content_prep_service = ContentPreparationService(db)
        
        # ğŸ”§ ä¼˜åŒ–ï¼šæ„å»ºç”¨æˆ·åå¥½é…ç½®ï¼ŒåŒ…å«TTSä¼˜åŒ–æ¨¡å¼
        user_preferences = {
            "auto_add_narrator": request.auto_add_narrator,
            "processing_mode": request.processing_mode,
            "tts_optimization": request.tts_optimization
        }
        
        logger.info(f"ğŸ“‹ ç« èŠ‚{chapter_id}æ™ºèƒ½å‡†å¤‡è¯·æ±‚: {user_preferences}")
        
        result = await content_prep_service.prepare_chapter_for_synthesis(
            chapter_id=chapter_id,
            user_preferences=user_preferences
        )
        
        return {
            "success": True,
            "data": result,
            "message": "ç« èŠ‚æ™ºèƒ½å‡†å¤‡å®Œæˆ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = str(e)
        logger.error(f"ç« èŠ‚æ™ºèƒ½å‡†å¤‡å¤±è´¥: {error_msg}")
        
        # æä¾›ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
        user_friendly_msg = "ç« èŠ‚æ™ºèƒ½å‡†å¤‡å¤±è´¥"
        
        if "timeout" in error_msg.lower() or "è¶…æ—¶" in error_msg:
            user_friendly_msg = "æ™ºèƒ½å‡†å¤‡è¶…æ—¶ï¼Œè¯¥ç« èŠ‚å†…å®¹å¯èƒ½è¾ƒé•¿ï¼Œè¯·ç¨åé‡è¯•"
        elif "ollama" in error_msg.lower():
            user_friendly_msg = "AIåˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"
        elif "network" in error_msg.lower() or "connection" in error_msg.lower():
            user_friendly_msg = "ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œåé‡è¯•"
        elif "analysis" in error_msg.lower():
            user_friendly_msg = "æ–‡æœ¬åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç« èŠ‚å†…å®¹æ ¼å¼"
        elif "character" in error_msg.lower():
            user_friendly_msg = "è§’è‰²è¯†åˆ«å¤±è´¥ï¼Œè¯·ç¡®ä¿ç« èŠ‚åŒ…å«å¯¹è¯å†…å®¹"
        else:
            user_friendly_msg = f"æ™ºèƒ½å‡†å¤‡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{error_msg}"
        
        raise HTTPException(status_code=500, detail=user_friendly_msg)


@router.get("/preparation-status/{chapter_id}")
async def get_preparation_status(
    chapter_id: int,
    db: Session = Depends(get_db)
):
    """
    è·å–ç« èŠ‚å‡†å¤‡çŠ¶æ€
    æ£€æŸ¥ç« èŠ‚æ˜¯å¦å·²ç»å®Œæˆæ™ºèƒ½å‡†å¤‡
    """
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹å¹¶æ£€æŸ¥å‡†å¤‡çŠ¶æ€
        content_prep_service = ContentPreparationService(db)
        status = await content_prep_service.get_preparation_status(chapter_id, db)
        
        return {
            "success": True,
            "data": status,
            "message": "å‡†å¤‡çŠ¶æ€è·å–æˆåŠŸ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å‡†å¤‡çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å‡†å¤‡çŠ¶æ€å¤±è´¥: {str(e)}")


@router.get("/result/{chapter_id}")
async def get_preparation_result(
    chapter_id: int,
    force_refresh: bool = Query(False, description="å¼ºåˆ¶åˆ·æ–°ç¼“å­˜"),
    db: Session = Depends(get_db)
):
    """
    è·å–ç« èŠ‚çš„å·²æœ‰æ™ºèƒ½å‡†å¤‡ç»“æœ
    ä¸é‡æ–°æ‰§è¡Œæ™ºèƒ½å‡†å¤‡ï¼Œåªè¿”å›å·²å­˜å‚¨çš„ç»“æœ
    
    Args:
        chapter_id: ç« èŠ‚ID
        force_refresh: å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ï¼Œå¿½ç•¥final_config
    """
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ™ºèƒ½å‡†å¤‡ç»“æœ
        try:
            
            # æŸ¥æ‰¾æœ€æ–°çš„æ™ºèƒ½å‡†å¤‡ç»“æœ
            latest_result = db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter_id,
                AnalysisResult.status == 'completed'
            ).order_by(AnalysisResult.created_at.desc()).first()
            
            if not latest_result:
                raise HTTPException(status_code=404, detail="è¯¥ç« èŠ‚å°šæœªå®Œæˆæ™ºèƒ½å‡†å¤‡")
            
            # ç›´æ¥ä½¿ç”¨å­˜å‚¨çš„æ™ºèƒ½å‡†å¤‡ç»“æœï¼Œä¸å†è¿›è¡ŒåŠ¨æ€ç¼“å­˜åŒæ­¥
            synthesis_plan = latest_result.synthesis_plan or {}
            logger.info("ğŸ“‹ [ç»“æœè·å–] ä½¿ç”¨å­˜å‚¨çš„æ™ºèƒ½å‡†å¤‡ç»“æœï¼Œå·²åŒ…å«æ­£ç¡®çš„è§’è‰²é…ç½®")
            
            # æ„å»ºè¿”å›æ•°æ®
            result_data = {
                "synthesis_json": synthesis_plan,
                "processing_info": {
                    "mode": "stored",
                    "total_segments": len(synthesis_plan.get('synthesis_plan', [])) if synthesis_plan else 0,
                    "characters_found": len(latest_result.detected_characters) if latest_result.detected_characters else 0,
                    "saved_to_database": True,
                    "result_id": latest_result.id,
                    "created_at": latest_result.created_at.isoformat() if latest_result.created_at else None,
                    "completed_at": latest_result.completed_at.isoformat() if latest_result.completed_at else None,
                    "voice_sync_applied": False,  # ä¸å†è¿›è¡ŒåŠ¨æ€è¯­éŸ³åŒæ­¥
                    "cache_status": "fresh" if force_refresh else "cached",  # ğŸ”¥ æ–°å¢ï¼šç¼“å­˜çŠ¶æ€
                    "data_source": "synthesis_plan"  # ğŸ”¥ æ–°å¢ï¼šæ•°æ®æ¥æº
                },
                "last_updated": latest_result.updated_at.isoformat() if latest_result.updated_at else latest_result.created_at.isoformat()
            }
            
            # ğŸ”¥ æ™ºèƒ½ç¼“å­˜å¤„ç†ï¼šæ ¹æ®force_refreshå‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨final_config
            if not force_refresh and latest_result.final_config:
                try:
                    final_config = latest_result.final_config
                    if isinstance(final_config, str):
                        import json
                        final_config = json.loads(final_config)
                    
                    # ğŸ”¥ ç®€åŒ–é€»è¾‘ï¼šåªæœ‰final_configåŒ…å«æ˜ç¡®çš„æ›´æ–°æ—¶é—´æˆ³æ—¶æ‰ä½¿ç”¨
                    # å¦åˆ™ä¼˜å…ˆä½¿ç”¨synthesis_planï¼ˆè§’è‰²åŒæ­¥åçš„æœ€æ–°æ•°æ®ï¼‰
                    if (final_config.get('synthesis_json') and 
                        final_config.get('last_updated')):
                        
                        # æœ‰æ˜ç¡®æ›´æ–°æ—¶é—´çš„final_configï¼Œè®¤ä¸ºæ˜¯ç”¨æˆ·æ‰‹åŠ¨ç¼–è¾‘çš„æœ€æ–°æ•°æ®
                        result_data["synthesis_json"] = final_config['synthesis_json']
                        result_data["processing_info"]["data_source"] = "final_config"
                        result_data["processing_info"]["user_edited"] = True
                        logger.info(f"ä½¿ç”¨final_configæ•°æ® (æ‰‹åŠ¨ç¼–è¾‘äº: {final_config.get('last_updated')})")
                    else:
                        # æ²¡æœ‰æ—¶é—´æˆ³çš„final_configè®¤ä¸ºæ˜¯è¿‡æœŸæ•°æ®ï¼Œä½¿ç”¨synthesis_plan
                        result_data["processing_info"]["data_source"] = "synthesis_plan"
                        result_data["processing_info"]["user_edited"] = False
                        logger.info("final_configç¼ºå°‘æ—¶é—´æˆ³ï¼Œä½¿ç”¨synthesis_planæ•°æ®ï¼ˆè§’è‰²åŒæ­¥åçš„æœ€æ–°æ•°æ®ï¼‰")
                    
                    if final_config.get('processing_info'):
                        result_data["processing_info"].update(final_config['processing_info'])
                    
                except Exception as e:
                    logger.warning(f"è§£æfinal_configå¤±è´¥ï¼Œä½¿ç”¨synthesis_plan: {str(e)}")
                    result_data["processing_info"]["data_source"] = "synthesis_plan"
                    result_data["processing_info"]["user_edited"] = False
            else:
                if force_refresh:
                    logger.info("ğŸ”„ [å¼ºåˆ¶åˆ·æ–°] å¿½ç•¥final_configç¼“å­˜ï¼Œä½¿ç”¨æœ€æ–°synthesis_planæ•°æ®")
                result_data["processing_info"]["data_source"] = "synthesis_plan"
                result_data["processing_info"]["user_edited"] = False
            
            return {
                "success": True,
                "data": result_data,
                "message": "æ™ºèƒ½å‡†å¤‡ç»“æœè·å–æˆåŠŸ"
            }
            
        except Exception as e:
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šå¦‚æœæ²¡æœ‰AnalysisResultæ¨¡å‹ï¼Œå°è¯•ä»ç« èŠ‚å­—æ®µè·å–
            analysis_result = chapter.analysis_results[0].original_analysis if chapter.analysis_results else None
            if not analysis_result:
                raise HTTPException(status_code=404, detail="è¯¥ç« èŠ‚å°šæœªå®Œæˆæ™ºèƒ½å‡†å¤‡")
            
            try:
                import json
                if isinstance(analysis_result, str):
                    result_data = json.loads(analysis_result)
                else:
                    result_data = analysis_result
                
                return {
                    "success": True,
                    "data": result_data,
                    "message": "æ™ºèƒ½å‡†å¤‡ç»“æœè·å–æˆåŠŸ"
                }
            except Exception as e:
                logger.error(f"è§£æç« èŠ‚åˆ†æç»“æœå¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail="æ™ºèƒ½å‡†å¤‡ç»“æœæ•°æ®æ ¼å¼é”™è¯¯")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ™ºèƒ½å‡†å¤‡ç»“æœå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ™ºèƒ½å‡†å¤‡ç»“æœå¤±è´¥: {str(e)}")


# ğŸ”¥ æ–°å¢ï¼šç¼“å­˜ç®¡ç†API
@router.delete("/cache/{chapter_id}")
async def clear_preparation_cache(
    chapter_id: int,
    cache_type: str = Query("final_config", description="ç¼“å­˜ç±»å‹: final_config | all"),
    db: Session = Depends(get_db)
):
    """
    æ¸…é™¤ç« èŠ‚çš„ç¼“å­˜æ•°æ®
    
    Args:
        chapter_id: ç« èŠ‚ID
        cache_type: ç¼“å­˜ç±»å‹
            - final_config: åªæ¸…é™¤ç”¨æˆ·ç¼–è¾‘ç¼“å­˜
            - all: æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼ˆå°†é‡æ–°æ™ºèƒ½å‡†å¤‡ï¼‰
    """
    try:
        from sqlalchemy.orm.attributes import flag_modified
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ™ºèƒ½å‡†å¤‡ç»“æœ
        result = db.query(AnalysisResult).filter(
            AnalysisResult.chapter_id == chapter_id,
            AnalysisResult.status == 'completed'
        ).order_by(AnalysisResult.created_at.desc()).first()
        
        if not result:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ™ºèƒ½å‡†å¤‡ç»“æœ")
        
        if cache_type == "final_config":
            # åªæ¸…é™¤final_configç¼“å­˜
            result.final_config = None
            flag_modified(result, 'final_config')
            message = "å·²æ¸…é™¤ç”¨æˆ·ç¼–è¾‘ç¼“å­˜ï¼Œå°†æ˜¾ç¤ºæœ€æ–°çš„æ™ºèƒ½å‡†å¤‡ç»“æœ"
        elif cache_type == "all":
            # æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼Œæ ‡è®°ä¸ºéœ€è¦é‡æ–°åˆ†æ
            result.status = 'pending'
            result.final_config = None
            result.synthesis_plan = None
            flag_modified(result, 'final_config')
            flag_modified(result, 'synthesis_plan')
            message = "å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼Œéœ€è¦é‡æ–°è¿›è¡Œæ™ºèƒ½å‡†å¤‡"
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„ç¼“å­˜ç±»å‹")
        
        db.commit()
        
        logger.info(f"ğŸ—‘ï¸ [ç¼“å­˜æ¸…ç†] ç« èŠ‚{chapter_id}çš„{cache_type}ç¼“å­˜å·²æ¸…é™¤")
        
        return {
            "success": True,
            "data": {
                "chapter_id": chapter_id,
                "cache_type": cache_type,
                "cleared_at": result.updated_at.isoformat()
            },
            "message": message
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}")


@router.put("/result/{chapter_id}")
async def update_preparation_result(
    chapter_id: int,
    update_data: Dict[str, Any],
    db: Session = Depends(get_db)
):
    """
    æ›´æ–°ç« èŠ‚çš„æ™ºèƒ½å‡†å¤‡ç»“æœ
    å…è®¸ç”¨æˆ·ç¼–è¾‘å’Œä¿®æ­£AIåˆ†æçš„ç»“æœ
    """
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # ğŸ”¥ æ–°å¢ï¼šè·å–ä¹¦ç±IDç”¨äºåç»­è§’è‰²è¯­éŸ³åŒæ­¥
        book_id = chapter.book_id
        
        # å°è¯•æ‰¾åˆ°ç°æœ‰çš„åˆ†æç»“æœ
        try:
            
            # æŸ¥æ‰¾æœ€æ–°çš„æ™ºèƒ½å‡†å¤‡ç»“æœ
            latest_result = db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter_id,
                AnalysisResult.status == 'completed'
            ).order_by(AnalysisResult.created_at.desc()).first()
            
            if not latest_result:
                raise HTTPException(status_code=404, detail="è¯¥ç« èŠ‚å°šæœªå®Œæˆæ™ºèƒ½å‡†å¤‡ï¼Œæ— æ³•æ›´æ–°")
            
            # æ›´æ–°ç»“æœæ•°æ®
            if 'synthesis_json' in update_data:
                latest_result.synthesis_plan = update_data['synthesis_json']
            
            # æ›´æ–°final_configä»¥ä¿å­˜å®Œæ•´çš„ç¼–è¾‘åæ•°æ®
            import json
            from sqlalchemy import func
            from datetime import datetime
            
            # ğŸ”¥ æ·»åŠ æ—¶é—´æˆ³ï¼Œç¡®ä¿APIèƒ½å¤Ÿæ­£ç¡®è¯†åˆ«æ‰‹åŠ¨ç¼–è¾‘çš„æ•°æ®
            update_data['last_updated'] = datetime.utcnow().isoformat()
            
            latest_result.final_config = json.dumps(update_data, ensure_ascii=False)
            latest_result.updated_at = func.now()
            
            # å¦‚æœæœ‰è§’è‰²æ•°æ®ï¼Œæ›´æ–°detected_characters
            updated_characters = []
            if 'synthesis_json' in update_data and 'characters' in update_data['synthesis_json']:
                characters = update_data['synthesis_json']['characters']
                character_names = [char.get('name', '') for char in characters if char.get('name')]
                latest_result.detected_characters = character_names
                updated_characters = characters
            
            db.commit()
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ›´æ–°ä¹¦ç±è§’è‰²æ±‡æ€»
            # å½“ç”¨æˆ·æ‰‹åŠ¨ç¼–è¾‘ç« èŠ‚åˆ†ææ•°æ®æ—¶ï¼Œéœ€è¦åŒæ­¥æ›´æ–°ä¹¦ç±çš„è§’è‰²æ±‡æ€»
            try:
                if updated_characters:
                    logger.info(f"ğŸ”„ [æ›´æ–°ä¹¦ç±è§’è‰²æ±‡æ€»] ç« èŠ‚ {chapter_id} çš„è§’è‰²æ•°æ®å·²æ›´æ–°ï¼ŒåŒæ­¥åˆ°ä¹¦ç±æ±‡æ€»")
                    
                    # æ›´æ–°ä¹¦ç±è§’è‰²æ±‡æ€»
                    book = db.query(Book).filter(Book.id == book_id).first()
                    if book:
                        book.update_character_summary(updated_characters, chapter_id)
                        db.commit()
                        logger.info(f"âœ… [æ›´æ–°ä¹¦ç±è§’è‰²æ±‡æ€»] æˆåŠŸæ›´æ–°ä¹¦ç± {book_id} çš„è§’è‰²æ±‡æ€»")
                    else:
                        logger.warning(f"âš ï¸ [æ›´æ–°ä¹¦ç±è§’è‰²æ±‡æ€»] æœªæ‰¾åˆ°ä¹¦ç± {book_id}")
                        
            except Exception as summary_error:
                logger.warning(f"âš ï¸ [æ›´æ–°ä¹¦ç±è§’è‰²æ±‡æ€»] æ›´æ–°å¤±è´¥: {str(summary_error)}")
                # æ±‡æ€»æ›´æ–°å¤±è´¥ä¸å½±å“ä¸»è¦çš„ä¿å­˜åŠŸèƒ½
            
            # ğŸ”¥ æ–°å¢ï¼šè‡ªåŠ¨è§¦å‘è§’è‰²è¯­éŸ³é…ç½®åŒæ­¥
            # æå–è§’è‰²è¯­éŸ³æ˜ å°„å¹¶åŒæ­¥åˆ°æ‰€æœ‰ç›¸å…³ç« èŠ‚çš„synthesis_plan
            updated_chapters_count = 0
            try:
                if updated_characters:
                    # æ„å»ºè§’è‰²è¯­éŸ³æ˜ å°„
                    character_voice_mappings = {}
                    for char in updated_characters:
                        if char.get('voice_id') and char.get('name'):
                            character_voice_mappings[char['name']] = str(char['voice_id'])
                    
                    logger.info(f"ğŸ”„ [è‡ªåŠ¨åŒæ­¥] ä»ç¼–è¾‘ç»“æœä¸­æå–åˆ°è§’è‰²æ˜ å°„: {character_voice_mappings}")
                    
                    if character_voice_mappings:
                        # å¯¼å…¥åŒæ­¥å‡½æ•°
                        from ..books import _sync_character_voice_to_synthesis_plans
                        
                        # åŒæ­¥è§’è‰²è¯­éŸ³é…ç½®åˆ°ç›¸å…³ç« èŠ‚
                        updated_chapters_count = await _sync_character_voice_to_synthesis_plans(
                            book_id, character_voice_mappings, db
                        )
                        
                        logger.info(f"âœ… [è‡ªåŠ¨åŒæ­¥] æˆåŠŸåŒæ­¥è§’è‰²é…ç½®åˆ° {updated_chapters_count} ä¸ªç« èŠ‚")
                        
            except Exception as sync_error:
                logger.warning(f"âš ï¸ [è‡ªåŠ¨åŒæ­¥] è§’è‰²è¯­éŸ³é…ç½®åŒæ­¥å¤±è´¥: {str(sync_error)}")
                # åŒæ­¥å¤±è´¥ä¸å½±å“ä¸»è¦çš„ä¿å­˜åŠŸèƒ½
                updated_chapters_count = 0
            
            logger.info(f"å·²æ›´æ–°ç« èŠ‚ {chapter_id} çš„æ™ºèƒ½å‡†å¤‡ç»“æœï¼ŒåŒæ­¥äº† {updated_chapters_count} ä¸ªç« èŠ‚")
            
            return {
                "success": True,
                "data": {
                    "result_id": latest_result.id,
                    "updated_at": latest_result.updated_at.isoformat(),
                    "characters_count": len(latest_result.detected_characters) if latest_result.detected_characters else 0,
                    "segments_count": len(update_data.get('synthesis_json', {}).get('synthesis_plan', [])),
                    "synced_chapters": updated_chapters_count  # ğŸ”¥ æ–°å¢ï¼šè¿”å›åŒæ­¥çš„ç« èŠ‚æ•°é‡
                },
                "message": f"æ™ºèƒ½å‡†å¤‡ç»“æœæ›´æ–°æˆåŠŸï¼Œå·²è‡ªåŠ¨åŒæ­¥ {updated_chapters_count} ä¸ªç« èŠ‚çš„è§’è‰²é…ç½®"
            }
            
        except Exception as e:
            # å¦‚æœæ²¡æœ‰AnalysisResultæ¨¡å‹ï¼Œå°è¯•æ›´æ–°ç« èŠ‚å­—æ®µ
            try:
                import json
                chapter.analysis_results[0].original_analysis = json.dumps(update_data, ensure_ascii=False)
                db.commit()
                
                from datetime import datetime
                return {
                    "success": True,
                    "data": {
                        "chapter_id": chapter_id,
                        "updated_at": datetime.now().isoformat()
                    },
                    "message": "æ™ºèƒ½å‡†å¤‡ç»“æœæ›´æ–°æˆåŠŸ"
                }
            except Exception as e:
                logger.error(f"æ›´æ–°ç« èŠ‚åˆ†æç»“æœå¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail="æ›´æ–°æ™ºèƒ½å‡†å¤‡ç»“æœå¤±è´¥")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°æ™ºèƒ½å‡†å¤‡ç»“æœå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°æ™ºèƒ½å‡†å¤‡ç»“æœå¤±è´¥: {str(e)}")


@router.post("/ai-resegment")
async def ai_resegment_text(
    request_data: Dict[str, Any],
    db: Session = Depends(get_db)
):
    """
    ä½¿ç”¨AIå¤§æ¨¡å‹é‡æ–°åˆ†æ®µæ–‡æœ¬
    æ­£ç¡®åˆ†ç¦»å¯¹è¯å’Œæ—ç™½ï¼Œè§£å†³"æŸæŸè¯´"çš„åˆ†æ®µé—®é¢˜
    """
    try:
        text = request_data.get("text", "")
        characters = request_data.get("characters", [])
        chapter_id = request_data.get("chapter_id")
        
        if not text.strip():
            raise HTTPException(status_code=400, detail="æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        # æ„å»ºAIæç¤ºè¯
        prompt = f"""è¯·å°†ä»¥ä¸‹å°è¯´æ–‡æœ¬é‡æ–°åˆ†æ®µï¼Œæ­£ç¡®åˆ†ç¦»å¯¹è¯å’Œæ—ç™½ã€‚

è§„åˆ™ï¼š
1. "æŸæŸè¯´ï¼š"è¿™ç§æè¿°æ€§æ–‡å­—åº”è¯¥å½’ä¸ºã€æ—ç™½ã€‘
2. å¼•å·""å†…çš„å†…å®¹æ‰æ˜¯è¯¥è§’è‰²çš„ã€å¯¹è¯ã€‘
3. å…¶ä»–æè¿°æ€§æ–‡å­—éƒ½å½’ä¸ºã€æ—ç™½ã€‘

å·²çŸ¥è§’è‰²ï¼š{', '.join(characters)}

è¯·å°†æ–‡æœ¬åˆ†æ®µï¼Œæ¯æ®µæ ‡æ˜è¯´è¯äººï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[è¯´è¯äºº]ï¼šæ–‡æœ¬å†…å®¹

åŸæ–‡ï¼š
{text}

é‡æ–°åˆ†æ®µç»“æœï¼š"""

        logger.info(f"AIé‡æ–°åˆ†æ®µè¯·æ±‚ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}")
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨AIæœåŠ¡ï¼Œæš‚æ—¶ç”¨æ¨¡æ‹Ÿç»“æœ
        # TODO: é›†æˆå®é™…çš„AIæ¨¡å‹è°ƒç”¨
        
        # æ¨¡æ‹ŸAIåˆ†æ®µç»“æœ - æ­£ç¡®å¤„ç†å¯¹è¯åˆ†ç¦»
        segments = []
        
        # ç®€å•çš„è§„åˆ™å¤„ç†ï¼Œå…ˆä½œä¸ºå ä½ç¬¦
        import re
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        segment_id = 1
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¯¹è¯
            if 'ï¼š' in sentence and '"' in sentence:
                # åˆ†ç¦»æè¿°å’Œå¯¹è¯
                parts = sentence.split('ï¼š', 1)
                if len(parts) == 2:
                    desc_part = parts[0].strip() + 'ï¼š'
                    dialog_part = parts[1].strip()
                    
                    # æ·»åŠ æè¿°éƒ¨åˆ†ï¼ˆæ—ç™½ï¼‰
                    segments.append({
                        "segment_id": segment_id,
                        "speaker": "æ—ç™½",
                        "text": desc_part,
                        "voice_name": "ç³»ç»Ÿæ—ç™½",
                        "parameters": {"timeStep": 0.5, "pWeight": 0.5, "tWeight": 0.5}
                    })
                    segment_id += 1
                    
                    # æå–å¼•å·å†…çš„å†…å®¹
                    dialog_match = re.search(r'"([^"]*)"', dialog_part)
                    if dialog_match:
                        dialog_text = dialog_match.group(1)
                        
                        # å°è¯•ä»æè¿°ä¸­æå–è§’è‰²å
                        speaker_name = "æ—ç™½"
                        for char in characters:
                            if char in desc_part:
                                speaker_name = char
                                break
                        
                        segments.append({
                            "segment_id": segment_id,
                            "speaker": speaker_name,
                            "text": f'"{dialog_text}"',
                            "voice_name": speaker_name,
                            "parameters": {"timeStep": 0.5, "pWeight": 0.5, "tWeight": 0.5}
                        })
                        segment_id += 1
                    
                    # å¤„ç†å¼•å·åçš„å…¶ä»–å†…å®¹
                    remaining = re.sub(r'"[^"]*"', '', dialog_part).strip()
                    if remaining:
                        segments.append({
                            "segment_id": segment_id,
                            "speaker": "æ—ç™½",
                            "text": remaining,
                            "voice_name": "ç³»ç»Ÿæ—ç™½",
                            "parameters": {"timeStep": 0.5, "pWeight": 0.5, "tWeight": 0.5}
                        })
                        segment_id += 1
            else:
                # æ™®é€šæè¿°æ€§æ–‡å­—ï¼Œå½’ä¸ºæ—ç™½
                segments.append({
                    "segment_id": segment_id,
                    "speaker": "æ—ç™½",
                    "text": sentence.strip() + "ã€‚",
                    "voice_name": "ç³»ç»Ÿæ—ç™½",
                    "parameters": {"timeStep": 0.5, "pWeight": 0.5, "tWeight": 0.5}
                })
                segment_id += 1
        
        logger.info(f"AIé‡æ–°åˆ†æ®µå®Œæˆï¼Œç”Ÿæˆ {len(segments)} ä¸ªç‰‡æ®µ")
        
        return {
            "success": True,
            "data": {
                "segments": segments,
                "total_segments": len(segments),
                "processing_info": {
                    "method": "ai_resegment",
                    "original_length": len(text),
                    "characters_found": len(characters)
                }
            },
            "message": f"AIé‡æ–°åˆ†æ®µå®Œæˆï¼Œç”Ÿæˆ {len(segments)} ä¸ªç‰‡æ®µ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"AIé‡æ–°åˆ†æ®µå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AIé‡æ–°åˆ†æ®µå¤±è´¥: {str(e)}")


# ==================== æ™ºèƒ½æ£€æµ‹ç›¸å…³API ====================

class DetectionRequest(BaseModel):
    """æ™ºèƒ½æ£€æµ‹è¯·æ±‚æ¨¡å‹"""
    use_ai: bool = True
    auto_fix: bool = False

class SingleSegmentDetectionRequest(BaseModel):
    """å•æ®µè½æ£€æµ‹è¯·æ±‚æ¨¡å‹"""
    segment_text: str
    segment_index: Optional[int] = 0


@router.post("/detect/segment")
async def detect_single_segment(
    request: SingleSegmentDetectionRequest,
    db: Session = Depends(get_db)
):
    """
    ğŸ”¥ æ–°å¢ï¼šå•æ®µè½æ™ºèƒ½æ£€æµ‹
    ä¸“é—¨ç”¨äºæ£€æµ‹å•ä¸ªæ®µè½æ˜¯å¦éœ€è¦æ‹†åˆ†ï¼ˆå¦‚"å¤ªç›‘å‡å°–ç€å—“å­å–Šé“ï¼šé™›ä¸‹ï¼æ¥šå†›å·²é€¼è¿‘å‡½è°·å…³ï¼"ï¼‰
    
    Args:
        request: å•æ®µè½æ£€æµ‹è¯·æ±‚
            - segment_text: è¦æ£€æµ‹çš„æ®µè½æ–‡æœ¬
            - segment_index: æ®µè½ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æ£€æµ‹ç»“æœï¼ŒåŒ…å«æ‹†åˆ†å»ºè®®
    """
    try:
        # åˆ›å»ºæ£€æµ‹æœåŠ¡
        detection_service = IntelligentDetectionService()
        
        # æ‰§è¡Œå•æ®µè½æ£€æµ‹
        issues = await detection_service.detect_single_segment_issues(
            request.segment_text, 
            request.segment_index
        )
        
        return {
            "success": True,
            "segment_text": request.segment_text,
            "issues": [
                {
                    "issue_type": issue.issue_type,
                    "severity": issue.severity,
                    "segment_index": issue.segment_index,
                    "description": issue.description,
                    "suggestion": issue.suggestion,
                    "context": issue.context,
                    "fixable": issue.fixable,
                    "fix_data": issue.fix_data
                } for issue in issues
            ],
            "total_issues": len(issues),
            "split_needed": len(issues) > 0
        }
        
    except Exception as e:
        logger.error(f"å•æ®µè½æ£€æµ‹å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å•æ®µè½æ£€æµ‹å¤±è´¥: {str(e)}")


@router.post("/detect/{chapter_id}")
async def detect_chapter_issues(
    chapter_id: int,
    request: DetectionRequest = Body(default=DetectionRequest()),
    db: Session = Depends(get_db)
):
    """
    æ‰§è¡Œç« èŠ‚å†…å®¹æ™ºèƒ½æ£€æµ‹
    æ£€æµ‹æ™ºèƒ½å‡†å¤‡åå¯èƒ½å­˜åœ¨çš„é—®é¢˜
    
    Args:
        chapter_id: ç« èŠ‚ID
        request: æ£€æµ‹è¯·æ±‚å‚æ•°
            - use_ai: æ˜¯å¦ä½¿ç”¨AIæ£€æµ‹
            - auto_fix: æ˜¯å¦è‡ªåŠ¨ä¿®å¤å¯ä¿®å¤çš„é—®é¢˜
    
    Returns:
        æ£€æµ‹ç»“æœï¼ŒåŒ…å«é—®é¢˜åˆ—è¡¨å’Œç»Ÿè®¡ä¿¡æ¯
    """
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆæ™ºèƒ½å‡†å¤‡
        if not chapter.analysis_results:
            raise HTTPException(status_code=400, detail="ç« èŠ‚å°šæœªå®Œæˆæ™ºèƒ½å‡†å¤‡ï¼Œæ— æ³•è¿›è¡Œæ£€æµ‹")
        
        # åˆ›å»ºæ£€æµ‹æœåŠ¡
        detection_service = IntelligentDetectionService()
        
        # æ‰§è¡Œæ£€æµ‹
        detection_result = await detection_service.detect_chapter_issues(
            chapter_id=chapter_id,
            enable_ai_detection=request.use_ai
        )
        
        # å¦‚æœå¯ç”¨è‡ªåŠ¨ä¿®å¤
        fixed_segments = None
        fix_logs = []
        if request.auto_fix and detection_result.issues:
            # è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªåŠ¨ä¿®å¤é€»è¾‘ï¼Œç›®å‰å…ˆè·³è¿‡
            pass
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        critical_count = sum(1 for issue in detection_result.issues if issue.severity == 'critical')
        warning_count = sum(1 for issue in detection_result.issues if issue.severity == 'warning')
        info_count = sum(1 for issue in detection_result.issues if issue.severity == 'info')
        fixable_count = sum(1 for issue in detection_result.issues if issue.fixable)
        
        # è½¬æ¢DetectionResultå¯¹è±¡ä¸ºå­—å…¸æ ¼å¼
        result_dict = {
            "chapter_id": detection_result.chapter_id,
            "total_issues": detection_result.total_issues,
            "issues_by_severity": detection_result.issues_by_severity,
            "fixable_issues": detection_result.fixable_issues,
            "fixable_count": fixable_count,
            "stats": {
                "critical_count": critical_count,
                "warning_count": warning_count,
                "info_count": info_count,
                "total_count": len(detection_result.issues)
            },
            "issues": [{
                "issue_type": issue.issue_type,
                "severity": issue.severity,
                "segment_index": issue.segment_index,
                "description": issue.description,
                "suggestion": issue.suggestion,
                "context": issue.context,
                "fixable": issue.fixable,
                "fix_data": issue.fix_data
            } for issue in detection_result.issues],
            "detection_time": detection_result.detection_time
        }
        
        return {
            "success": True,
            "detection_result": result_dict,
            "auto_fix_applied": request.auto_fix and len(fix_logs) > 0,
            "fix_logs": fix_logs,
            "fixed_segments": fixed_segments,
            "message": f"æ£€æµ‹å®Œæˆï¼Œå‘ç° {detection_result.total_issues} ä¸ªé—®é¢˜"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ™ºèƒ½æ£€æµ‹å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ™ºèƒ½æ£€æµ‹å¤±è´¥: {str(e)}")


@router.get("/detect/result/{chapter_id}")
async def get_detection_result(
    chapter_id: int,
    db: Session = Depends(get_db)
):
    """
    è·å–ç« èŠ‚çš„æœ€æ–°æ£€æµ‹ç»“æœ
    
    Args:
        chapter_id: ç« èŠ‚ID
    
    Returns:
        æœ€æ–°çš„æ£€æµ‹ç»“æœ
    """
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æµ‹ç»“æœï¼ˆè¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºä»æ•°æ®åº“å­˜å‚¨çš„æ£€æµ‹ç»“æœä¸­è·å–ï¼‰
        # ç›®å‰å…ˆè¿”å›åŸºæœ¬ä¿¡æ¯ï¼Œåç»­å¯ä»¥æ·»åŠ æ£€æµ‹ç»“æœçš„æŒä¹…åŒ–å­˜å‚¨
        
        return {
            "success": True,
            "chapter_id": chapter_id,
            "has_detection_result": False,
            "message": "æš‚æ— æ£€æµ‹ç»“æœï¼Œè¯·å…ˆæ‰§è¡Œæ£€æµ‹"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ£€æµ‹ç»“æœå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ£€æµ‹ç»“æœå¤±è´¥: {str(e)}")


@router.post("/detect/fix/{chapter_id}")
async def apply_detection_fixes(
    chapter_id: int,
    fix_data: Dict[str, Any],
    db: Session = Depends(get_db)
):
    """
    åº”ç”¨æ£€æµ‹é—®é¢˜çš„ä¿®å¤
    
    Args:
        chapter_id: ç« èŠ‚ID
        fix_data: ä¿®å¤æ•°æ®
            - issues: è¦ä¿®å¤çš„é—®é¢˜åˆ—è¡¨ï¼ˆå•ä¸ªé—®é¢˜ä¿®å¤ï¼‰
            - fixed_segments: ä¿®å¤åçš„ç‰‡æ®µæ•°æ®ï¼ˆæ‰¹é‡ä¿®å¤ï¼‰
    
    Returns:
        ä¿®å¤ç»“æœ
    """
    try:
        # è·å–ç« èŠ‚
        chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
        if not chapter:
            raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
        
        # æ”¯æŒä¸¤ç§ä¿®å¤æ¨¡å¼ï¼šå•ä¸ªé—®é¢˜ä¿®å¤å’Œæ‰¹é‡ä¿®å¤
        issues_to_fix = fix_data.get('issues', [])
        fixed_segments = fix_data.get('fixed_segments', [])
        
        # å¦‚æœæ˜¯å•ä¸ªé—®é¢˜ä¿®å¤æ¨¡å¼
        if issues_to_fix and not fixed_segments:
            logger.info(f"å¼€å§‹å•ä¸ªé—®é¢˜ä¿®å¤ï¼Œç« èŠ‚ID: {chapter_id}, é—®é¢˜æ•°é‡: {len(issues_to_fix)}")
            
            # è·å–å½“å‰ç« èŠ‚çš„åˆ†æç»“æœ
            analysis_result = db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter_id
            ).order_by(AnalysisResult.created_at.desc()).first()
            
            if not analysis_result:
                logger.error(f"æœªæ‰¾åˆ°ç« èŠ‚ {chapter_id} çš„åˆ†æç»“æœ")
                raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç« èŠ‚çš„åˆ†æç»“æœ")
            
            if not analysis_result.synthesis_plan:
                logger.error(f"ç« èŠ‚ {chapter_id} çš„åˆ†æç»“æœä¸­æ²¡æœ‰åˆæˆè®¡åˆ’")
                raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç« èŠ‚çš„åˆæˆè®¡åˆ’")
            
            # ç¡®ä¿synthesis_planæ˜¯å­—å…¸ç±»å‹
            if not isinstance(analysis_result.synthesis_plan, dict):
                logger.error(f"ç« èŠ‚ {chapter_id} çš„åˆæˆè®¡åˆ’ä¸æ˜¯å­—å…¸ç±»å‹: {type(analysis_result.synthesis_plan)}")
                raise HTTPException(status_code=400, detail="åˆæˆè®¡åˆ’æ ¼å¼é”™è¯¯")
            
            # åº”ç”¨å•ä¸ªé—®é¢˜çš„ä¿®å¤
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è·å–segmentsæ•°æ®ï¼Œæ”¯æŒä¸¤ç§æ•°æ®ç»“æ„
            current_segments = analysis_result.synthesis_plan.get('synthesis_plan', [])
            if not current_segments:
                # å…¼å®¹æ—§çš„æ•°æ®ç»“æ„
                current_segments = analysis_result.synthesis_plan.get('segments', [])
            logger.info(f"å½“å‰ç‰‡æ®µæ•°é‡: {len(current_segments)}")

            # å¦‚æœæ²¡æœ‰ç‰‡æ®µæ•°æ®ï¼Œåˆ™æ— éœ€å°è¯•ä¿®å¤ä»»ä½•é—®é¢˜
            if not current_segments:
                error_detail = f"ç« èŠ‚ {chapter_id} çš„åˆæˆè®¡åˆ’ä¸­æ²¡æœ‰ç‰‡æ®µæ•°æ®ï¼Œæ— æ³•åº”ç”¨ä¿®å¤ã€‚è¯·å…ˆæ‰§è¡Œç« èŠ‚åˆ†æä»¥ç”Ÿæˆç‰‡æ®µæ•°æ®ã€‚"
                logger.warning(error_detail)
                raise HTTPException(status_code=400, detail=error_detail)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ä¿®å¤çš„é—®é¢˜
            if not issues_to_fix:
                logger.warning(f"ç« èŠ‚ {chapter_id} æ²¡æœ‰æä¾›å¯ä¿®å¤çš„é—®é¢˜")
                return {
                    "success": False,
                    "message": "æ²¡æœ‰æä¾›å¯ä¿®å¤çš„é—®é¢˜",
                        "details": "è¯·æ£€æŸ¥ä¿®å¤æ•°æ®"
                    }

            fixed_count = 0
            
            for i, issue in enumerate(issues_to_fix):
                logger.info(f"å¤„ç†é—®é¢˜ {i+1}: {issue}")
                
                if issue.get('fixable') and issue.get('fix_data'):
                    segment_index = issue.get('segment_index')
                    logger.info(f"é—®é¢˜çš„ç‰‡æ®µç´¢å¼•: {segment_index}")
                    if segment_index is None:
                        logger.warning(f"é—®é¢˜ {i+1} ç¼ºå°‘ç‰‡æ®µç´¢å¼•")
                        continue
                    
                    if 0 <= segment_index < len(current_segments):
                        # åº”ç”¨ä¿®å¤æ•°æ®
                        fix_data_content = issue.get('fix_data', {})
                        if fix_data_content:
                            logger.info(f"åº”ç”¨ä¿®å¤æ•°æ®åˆ°ç‰‡æ®µ {segment_index}: {fix_data_content}")
                            
                            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ¹æ®actionç±»å‹æ­£ç¡®åº”ç”¨ä¿®å¤
                            action = fix_data_content.get('action')
                            success = False
                            
                            # ğŸ”¥ ç§»é™¤assign_voice_typeï¼šä¸å†æ£€æµ‹è¯­éŸ³ç±»å‹é—®é¢˜
                            if action == 'assign_voice_type':
                                # å·²ç§»é™¤ï¼švoice_typeä¸å½±å“æ ¸å¿ƒåˆæˆåŠŸèƒ½
                                success = True
                                logger.info(f"è·³è¿‡è¯­éŸ³ç±»å‹è®¾ç½®ï¼ˆå·²ç§»é™¤æ£€æµ‹ï¼‰")
                            
                            elif action == 'clean_special_chars':
                                # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
                                text = current_segments[segment_index]['text']
                                chars_to_clean = fix_data_content.get('chars', [])
                                for char in chars_to_clean:
                                    text = text.replace(char, '')
                                current_segments[segment_index]['text'] = text.strip()
                                success = True
                                logger.info(f"æ¸…ç†ç‰¹æ®Šå­—ç¬¦: {chars_to_clean}")
                            
                            elif action == 'set_character':
                                # è®¾ç½®è§’è‰²
                                character = fix_data_content.get('character', '')
                                text_type = fix_data_content.get('text_type', 'dialogue')
                                current_segments[segment_index]['speaker'] = character
                                current_segments[segment_index]['character'] = character  # å…¼å®¹æ€§
                                current_segments[segment_index]['text_type'] = text_type
                                if text_type == 'dialogue' and character:
                                    if not current_segments[segment_index].get('voice_type'):
                                        current_segments[segment_index]['voice_type'] = 'default'
                                success = True
                                logger.info(f"è®¾ç½®è§’è‰²: '{character}', æ–‡æœ¬ç±»å‹: {text_type}")
                            
                            elif action == 'set_narration':
                                # è®¾ç½®ä¸ºæ—ç™½
                                current_segments[segment_index]['text_type'] = 'narration'
                                current_segments[segment_index]['speaker'] = ''
                                current_segments[segment_index]['character'] = ''
                                current_segments[segment_index]['voice_type'] = ''
                                success = True
                                logger.info("è®¾ç½®ä¸ºæ—ç™½")
                            
                            else:
                                logger.warning(f"æœªçŸ¥çš„ä¿®å¤åŠ¨ä½œ: {action}")
                                success = False
                            
                            if success:
                                fixed_count += 1
                        else:
                            logger.warning(f"é—®é¢˜ {i+1} çš„ä¿®å¤æ•°æ®ä¸ºç©º")
                    else:
                        logger.warning(f"é—®é¢˜ {i+1} çš„ç‰‡æ®µç´¢å¼• {segment_index} è¶…å‡ºèŒƒå›´ [0, {len(current_segments)-1}]")
                else:
                    logger.warning(f"é—®é¢˜ {i+1} ä¸å¯ä¿®å¤æˆ–ç¼ºå°‘ä¿®å¤æ•°æ®")
            
            if fixed_count == 0:
                error_detail = "æ²¡æœ‰æˆåŠŸä¿®å¤ä»»ä½•é—®é¢˜ã€‚è¯·æ£€æŸ¥æä¾›çš„ä¿®å¤æ•°æ®ï¼ˆå¦‚ segment_index æ˜¯å¦æ­£ç¡®ï¼Œfixable æ˜¯å¦ä¸º Trueï¼Œfix_data æ˜¯å¦æœ‰æ•ˆï¼‰ã€‚"
                logger.error(error_detail)
                raise HTTPException(status_code=400, detail=error_detail)
            
            # æ›´æ–°åˆ†æç»“æœ
            import json
            from datetime import datetime
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®æ›´æ–°synthesis_planæ•°æ®ç»“æ„
            synthesis_plan_copy = analysis_result.synthesis_plan.copy() if analysis_result.synthesis_plan else {}
            
            # æ›´æ–°synthesis_planä¸­çš„segmentsæ•°æ®
            if 'synthesis_plan' in synthesis_plan_copy:
                synthesis_plan_copy['synthesis_plan'] = current_segments
            else:
                # å…¼å®¹æ—§çš„æ•°æ®ç»“æ„
                synthesis_plan_copy['segments'] = current_segments
            
            # æ›´æ–°synthesis_planæœ¬èº«
            analysis_result.synthesis_plan = synthesis_plan_copy
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ­£ç¡®æ›´æ–°final_configï¼Œä¿æŒä¸æ£€æµ‹é€»è¾‘ä¸€è‡´çš„æ•°æ®ç»“æ„å’Œæ ¼å¼
            if analysis_result.final_config:
                try:
                    # å¦‚æœå·²æœ‰final_configï¼Œè§£æå¹¶æ›´æ–°
                    if isinstance(analysis_result.final_config, str):
                        final_config_data = json.loads(analysis_result.final_config)
                    else:
                        final_config_data = analysis_result.final_config
                    
                    # ç¡®ä¿æœ‰æ­£ç¡®çš„æ•°æ®ç»“æ„
                    if 'synthesis_json' not in final_config_data:
                        final_config_data['synthesis_json'] = {}
                    
                    # æ›´æ–°synthesis_planæ•°æ®
                    final_config_data['synthesis_json']['synthesis_plan'] = current_segments
                    final_config_data['synthesis_json']['total_segments'] = len(current_segments)
                    final_config_data['last_updated'] = datetime.now().isoformat()
                    final_config_data['updated_by'] = 'detection_fix'
                    
                    # ä¿å­˜ä¸ºJSONå­—ç¬¦ä¸²
                    analysis_result.final_config = json.dumps(final_config_data, ensure_ascii=False)
                    logger.info(f"[ä¿®å¤åŒæ­¥] å·²æ›´æ–°final_configæ•°æ®ç»“æ„ï¼Œæ®µè½æ•°: {len(current_segments)}")
                    
                except Exception as e:
                    logger.error(f"[ä¿®å¤åŒæ­¥] æ›´æ–°final_configå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°åˆ›å»ºæ–°ç»“æ„")
                    # åˆ›å»ºæ–°çš„final_configç»“æ„
                    final_config_data = {
                        'synthesis_json': {
                            'synthesis_plan': current_segments,
                            'total_segments': len(current_segments)
                        },
                        'last_updated': datetime.now().isoformat(),
                        'updated_by': 'detection_fix'
                    }
                    analysis_result.final_config = json.dumps(final_config_data, ensure_ascii=False)
            else:
                # å¦‚æœæ²¡æœ‰final_configï¼Œåˆ›å»ºæ–°çš„ç»“æ„
                final_config_data = {
                    'synthesis_json': {
                        'synthesis_plan': current_segments,
                        'total_segments': len(current_segments)
                    },
                    'last_updated': datetime.now().isoformat(),
                    'updated_by': 'detection_fix'
                }
                analysis_result.final_config = json.dumps(final_config_data, ensure_ascii=False)
                logger.info(f"[ä¿®å¤åŒæ­¥] åˆ›å»ºæ–°final_configæ•°æ®ç»“æ„ï¼Œæ®µè½æ•°: {len(current_segments)}")
            db.commit()
            
            logger.info(f"å•ä¸ªé—®é¢˜ä¿®å¤æˆåŠŸï¼Œä¿®å¤äº† {fixed_count} ä¸ªé—®é¢˜")
            
            return {
                "success": True,
                "message": f"æˆåŠŸä¿®å¤ {fixed_count} ä¸ªé—®é¢˜",
                "data": {
                    "fixed_count": fixed_count,
                    "fixed_issues": fixed_count
                }
            }
        
        # æ‰¹é‡ä¿®å¤æ¨¡å¼
        elif fixed_segments:
            try:
                # è·å–AnalysisResult
                analysis_result = db.query(AnalysisResult).filter(
                    AnalysisResult.chapter_id == chapter_id
                ).order_by(AnalysisResult.created_at.desc()).first()
                
                if not analysis_result:
                    logger.error(f"æœªæ‰¾åˆ°ç« èŠ‚ {chapter_id} çš„åˆ†æç»“æœ")
                    raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°åˆ†æç»“æœ")
                
                # æ›´æ–°final_configä»¥æ ‡è®°ä¸ºç”¨æˆ·ç¼–è¾‘
                import json
                from datetime import datetime
                
                final_config = analysis_result.synthesis_plan.copy() if analysis_result.synthesis_plan else {}
                final_config.update({
                    'synthesis_plan': fixed_segments,
                    'total_segments': len(fixed_segments),
                    'last_updated': datetime.now().isoformat(),
                    'updated_by': 'detection_fix'
                })
                
                analysis_result.final_config = final_config
                db.commit()
                
                logger.info(f"æ‰¹é‡ä¿®å¤æˆåŠŸï¼Œæ›´æ–°äº† {len(fixed_segments)} ä¸ªç‰‡æ®µ")
                
                return {
                    "success": True,
                    "message": f"æˆåŠŸåº”ç”¨ä¿®å¤ï¼Œæ›´æ–°äº† {len(fixed_segments)} ä¸ªç‰‡æ®µ",
                    "data": {
                        "fixed_count": len(fixed_segments),
                        "updated_segments": len(fixed_segments)
                    }
                }
            except HTTPException:
                db.rollback()
                raise
            except Exception as e:
                db.rollback()
                logger.error(f"æ‰¹é‡ä¿®å¤å¤„ç†å¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail=f"æ‰¹é‡ä¿®å¤å¤„ç†å¤±è´¥: {str(e)}")
        else:
            raise HTTPException(status_code=400, detail="è¯·æä¾›è¦ä¿®å¤çš„é—®é¢˜åˆ—è¡¨æˆ–ä¿®å¤åçš„ç‰‡æ®µæ•°æ®")
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"åº”ç”¨ä¿®å¤å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åº”ç”¨ä¿®å¤å¤±è´¥: {str(e)}")