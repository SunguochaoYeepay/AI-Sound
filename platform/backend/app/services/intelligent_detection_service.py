from typing import List, Dict, Any, Optional
import re
from dataclasses import dataclass
from app.models.book_chapter import BookChapter
from app.models.analysis_result import AnalysisResult
from app.database import get_db
from sqlalchemy.orm import Session
import logging

logger = logging.getLogger(__name__)

@dataclass
class DetectionIssue:
    """æ£€æµ‹åˆ°çš„é—®é¢˜"""
    issue_type: str  # é—®é¢˜ç±»å‹
    severity: str    # ä¸¥é‡ç¨‹åº¦: 'low', 'medium', 'high'
    segment_index: int  # ç‰‡æ®µç´¢å¼•
    description: str    # é—®é¢˜æè¿°
    suggestion: str     # ä¿®å¤å»ºè®®
    context: Optional[str] = None  # ä¸Šä¸‹æ–‡ä¿¡æ¯
    fixable: bool = True  # æ˜¯å¦å¯è‡ªåŠ¨ä¿®å¤
    fix_data: Optional[Dict[str, Any]] = None  # ä¿®å¤æ•°æ®

@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœ"""
    chapter_id: int
    total_issues: int
    issues_by_severity: Dict[str, int]
    fixable_issues: int
    issues: List[DetectionIssue]
    detection_time: str
    
class IntelligentDetectionService:
    """æ™ºèƒ½æ£€æµ‹æœåŠ¡"""
    
    def __init__(self):
        self.issue_patterns = {
            'character_mismatch': {
                'description': 'è§’è‰²åç§°ä¸è¯­éŸ³é…ç½®ä¸åŒ¹é…',
                'severity': 'high',
                'fixable': True
            },
            'empty_content': {
                'description': 'ç‰‡æ®µå†…å®¹ä¸ºç©º',
                'severity': 'medium',
                'fixable': False
            },
            'long_segment': {
                'description': 'ç‰‡æ®µå†…å®¹è¿‡é•¿ï¼Œå¯èƒ½å½±å“åˆæˆè´¨é‡',
                'severity': 'low',
                'fixable': True
            },
            'special_characters': {
                'description': 'åŒ…å«ç‰¹æ®Šå­—ç¬¦æˆ–ç¬¦å·',
                'severity': 'medium',
                'fixable': True
            },
            'quoted_content_as_narration': {
                'description': 'å¼•å·å¯¹è¯å†…å®¹è¢«æ ‡è®°ä¸ºæ—ç™½',
                'severity': 'medium',
                'fixable': True
            },
            'narration_as_dialogue': {
                'description': 'æ—ç™½å†…å®¹è¢«æ ‡è®°ä¸ºå¯¹è¯',
                'severity': 'medium',
                'fixable': True
            }
            # ğŸ”¥ å·²ç§»é™¤ï¼š'inconsistent_voice_type' - è¯­éŸ³ç±»å‹æ£€æµ‹ä¸é‡è¦
        }
    
    async def detect_chapter_issues(self, chapter_id: int, enable_ai_detection: bool = True) -> DetectionResult:
        """æ£€æµ‹ç« èŠ‚é—®é¢˜"""
        db = next(get_db())
        try:
            # è·å–ç« èŠ‚å’Œåˆæˆè®¡åˆ’
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ValueError(f"Chapter {chapter_id} not found")
            
            analysis_result = db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter_id
            ).order_by(AnalysisResult.created_at.desc()).first()
            
            if not analysis_result:
                return DetectionResult(
                    chapter_id=chapter_id,
                    total_issues=0,
                    issues_by_severity={'low': 0, 'medium': 0, 'high': 0},
                    fixable_issues=0,
                    issues=[],
                    detection_time=self._get_current_time()
                )
            
            # ğŸ”¥ ä¿®å¤ï¼šä¼˜å…ˆè¯»å–ç”¨æˆ·ç¼–è¾‘åçš„æ•°æ®
            segments = []
            
            # é¦–å…ˆå°è¯•ä»final_configè¯»å–æœ€æ–°çš„ç”¨æˆ·ç¼–è¾‘æ•°æ®
            if analysis_result.final_config:
                try:
                    import json
                    final_config_data = json.loads(analysis_result.final_config)
                    if 'synthesis_json' in final_config_data and 'synthesis_plan' in final_config_data['synthesis_json']:
                        segments = final_config_data['synthesis_json']['synthesis_plan']
                        logger.info(f"[æ™ºèƒ½æ£€æµ‹] ä½¿ç”¨ç”¨æˆ·ç¼–è¾‘åçš„æ•°æ®è¿›è¡Œæ£€æµ‹ï¼Œæ®µè½æ•°: {len(segments)}")
                except Exception as e:
                    logger.warning(f"[æ™ºèƒ½æ£€æµ‹] è§£æfinal_configå¤±è´¥: {str(e)}")
            
            # å¦‚æœæ²¡æœ‰ç”¨æˆ·ç¼–è¾‘æ•°æ®ï¼Œä½¿ç”¨åŸå§‹synthesis_plan
            if not segments and analysis_result.synthesis_plan and 'synthesis_plan' in analysis_result.synthesis_plan:
                segments = analysis_result.synthesis_plan['synthesis_plan']
                logger.info(f"[æ™ºèƒ½æ£€æµ‹] ä½¿ç”¨åŸå§‹synthesis_planæ•°æ®è¿›è¡Œæ£€æµ‹ï¼Œæ®µè½æ•°: {len(segments)}")
            
            # å¦‚æœä»ç„¶æ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºç»“æœ
            if not segments:
                logger.warning(f"[æ™ºèƒ½æ£€æµ‹] ç« èŠ‚ {chapter_id} æ²¡æœ‰å¯ç”¨çš„åˆæˆè®¡åˆ’æ•°æ®")
                return DetectionResult(
                    chapter_id=chapter_id,
                    total_issues=0,
                    issues_by_severity={'low': 0, 'medium': 0, 'high': 0},
                    fixable_issues=0,
                    issues=[],
                    detection_time=self._get_current_time()
                )
            
            issues = []
            
            # ğŸ” è°ƒè¯•ï¼šæ˜¾ç¤ºæ®µè½æ•°æ®è¯¦æƒ…
            logger.info(f"[æ™ºèƒ½æ£€æµ‹] å‡†å¤‡æ£€æµ‹ç« èŠ‚ {chapter_id}ï¼Œæ®µè½æ•°: {len(segments)}")
            if segments:
                # æ˜¾ç¤ºå‰3ä¸ªæ®µè½çš„ç¤ºä¾‹æ•°æ®
                for i, segment in enumerate(segments[:3]):
                    logger.info(f"[æ™ºèƒ½æ£€æµ‹] æ®µè½{i+1}ç¤ºä¾‹: text='{segment.get('text', '')[:50]}...', "
                               f"speaker='{segment.get('speaker', '')}', "
                               f"text_type='{segment.get('text_type', '')}'")
            
            # åŸºç¡€æ£€æµ‹
            basic_issues = self._detect_basic_issues(segments)
            issues.extend(basic_issues)
            logger.info(f"[æ™ºèƒ½æ£€æµ‹] åŸºç¡€æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(basic_issues)} ä¸ªé—®é¢˜")
            
            # AIå¢å¼ºæ£€æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_ai_detection:
                logger.info(f"[æ™ºèƒ½æ£€æµ‹] å¼€å§‹AIå¢å¼ºæ£€æµ‹...")
                ai_issues = await self._detect_ai_issues(segments)
                issues.extend(ai_issues)
                logger.info(f"[æ™ºèƒ½æ£€æµ‹] AIå¢å¼ºæ£€æµ‹å®Œæˆï¼Œå‘ç° {len(ai_issues)} ä¸ªé—®é¢˜")
            else:
                logger.info(f"[æ™ºèƒ½æ£€æµ‹] AIå¢å¼ºæ£€æµ‹å·²ç¦ç”¨")
            
            # ç»Ÿè®¡ç»“æœ
            issues_by_severity = {'low': 0, 'medium': 0, 'high': 0}
            fixable_issues = 0
            
            for issue in issues:
                issues_by_severity[issue.severity] += 1
                if issue.fixable:
                    fixable_issues += 1
            
            logger.info(f"[æ™ºèƒ½æ£€æµ‹] ç« èŠ‚ {chapter_id} æ£€æµ‹å®Œæˆï¼Œæ€»è®¡å‘ç° {len(issues)} ä¸ªé—®é¢˜")
            
            return DetectionResult(
                chapter_id=chapter_id,
                total_issues=len(issues),
                issues_by_severity=issues_by_severity,
                fixable_issues=fixable_issues,
                issues=issues,
                detection_time=self._get_current_time()
            )
            
        finally:
            db.close()
    
    def _detect_basic_issues(self, segments: List[Dict[str, Any]]) -> List[DetectionIssue]:
        """åŸºç¡€é—®é¢˜æ£€æµ‹"""
        issues = []
        
        for index, segment in enumerate(segments):
            text = segment.get('text', '').strip()
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
            character = segment.get('speaker', '') or segment.get('character', '')  # ä¼˜å…ˆä½¿ç”¨speakerï¼Œfallbackåˆ°character
            voice_type = segment.get('voice_type', '')
            text_type = segment.get('text_type', 'dialogue')
            
            # æ£€æµ‹ç©ºå†…å®¹
            if not text:
                issues.append(DetectionIssue(
                    issue_type='empty_content',
                    severity='medium',
                    segment_index=index,
                    description='ç‰‡æ®µå†…å®¹ä¸ºç©º',
                    suggestion='è¯·æ·»åŠ æ–‡æœ¬å†…å®¹æˆ–åˆ é™¤æ­¤ç‰‡æ®µ',
                    context=f'ç‰‡æ®µ {index + 1}',
                    fixable=False
                ))
                continue
            
            # æ£€æµ‹è¿‡é•¿ç‰‡æ®µ
            if len(text) > 500:
                issues.append(DetectionIssue(
                    issue_type='long_segment',
                    severity='low',
                    segment_index=index,
                    description=f'ç‰‡æ®µå†…å®¹è¿‡é•¿ï¼ˆ{len(text)}å­—ç¬¦ï¼‰',
                    suggestion='å»ºè®®å°†é•¿ç‰‡æ®µåˆ†å‰²ä¸ºå¤šä¸ªè¾ƒçŸ­çš„ç‰‡æ®µ',
                    context=text[:50] + '...',
                    fixable=True,
                    fix_data={'action': 'split_segment', 'max_length': 200}
                ))
            
            # æ£€æµ‹ç‰¹æ®Šå­—ç¬¦
            special_chars = re.findall(r'[#@$%^&*()_+=\[\]{}|;:,.<>?/~`]', text)
            if special_chars:
                issues.append(DetectionIssue(
                    issue_type='special_characters',
                    severity='medium',
                    segment_index=index,
                    description=f'åŒ…å«ç‰¹æ®Šå­—ç¬¦: {", ".join(set(special_chars))}',
                    suggestion='å»ºè®®ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦',
                    context=text[:100],
                    fixable=True,
                    fix_data={'action': 'clean_special_chars', 'chars': special_chars}
                ))
            
            # æ£€æµ‹è§’è‰²é…ç½®é—®é¢˜
            if text_type == 'dialogue' and not character:
                issues.append(DetectionIssue(
                    issue_type='character_mismatch',
                    severity='high',
                    segment_index=index,
                    description='å¯¹è¯ç‰‡æ®µæœªæŒ‡å®šè§’è‰²',
                    suggestion='è¯·ä¸ºå¯¹è¯ç‰‡æ®µæŒ‡å®šè¯´è¯è§’è‰²',
                    context=text[:50],
                    fixable=True,
                    fix_data={'action': 'assign_character'}
                ))
            
            # ğŸ”¥ å·²ç§»é™¤è¯­éŸ³ç±»å‹æ£€æµ‹ï¼švoice_typeä¸å½±å“TTSåˆæˆ
            # ç”¨æˆ·åé¦ˆï¼šè¯­éŸ³ç±»å‹æ£€æµ‹ä¸é‡è¦ï¼Œå·²å®Œå…¨ç§»é™¤
        
        return issues
    
    async def _detect_ai_issues(self, segments: List[Dict[str, Any]]) -> List[DetectionIssue]:
        """AIå¢å¼ºæ£€æµ‹"""
        issues = []
        
        # ğŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨ç¼–ç¨‹è§„åˆ™è¿›è¡Œåˆæ­¥ç­›é€‰ï¼Œåªå¯¹å¯ç–‘æ®µè½ä½¿ç”¨å¤§æ¨¡å‹
        logger.info(f"[AIå¢å¼ºæ£€æµ‹] å¼€å§‹æ™ºèƒ½æ£€æµ‹ {len(segments)} ä¸ªæ®µè½")
        
        # åˆå§‹åŒ–ç¼–ç¨‹è§„åˆ™æ£€æµ‹å™¨ï¼ˆç”¨äºå¿«é€Ÿé¢„ç­›é€‰ï¼‰
        try:
            from app.detectors.character_detectors import ProgrammaticCharacterDetector
            rule_detector = ProgrammaticCharacterDetector()
            logger.info(f"[AIå¢å¼ºæ£€æµ‹] ç¼–ç¨‹è§„åˆ™æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"[AIå¢å¼ºæ£€æµ‹] ç¼–ç¨‹è§„åˆ™æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            rule_detector = None
        
        # å¤§æ¨¡å‹æ£€æµ‹å™¨ï¼ˆä»…åœ¨éœ€è¦æ—¶åˆå§‹åŒ–ï¼‰
        mixed_text_detector = None
        suspicious_segments = []
        
        for index, segment in enumerate(segments):
            text = segment.get('text', '').strip()
            text_type = segment.get('text_type', 'dialogue')
            
            if not text:
                continue
            
            # ï¿½ï¿½ ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ç¼–ç¨‹è§„åˆ™å¿«é€Ÿç­›é€‰å¯ç–‘æ®µè½  
            if rule_detector and len(text) > 15:  # åªæ£€æµ‹è¶³å¤Ÿé•¿çš„æ–‡æœ¬
                # ğŸ”¥ é™ä½é—¨æ§›ï¼šåªè¦æœ‰ä¸€å®šç‰¹å¾å°±äº¤ç»™å¤§æ¨¡å‹åˆ¤æ–­
                
                # 1. æœ‰è§’è‰²åŠ¨ä½œæè¿°ï¼ˆæ‰©å±•åŒ¹é…èŒƒå›´ï¼‰
                has_speaker_action = re.search(r'([ä¸€-é¾¯]{2,6})[^ï¼Œã€‚ï¼ï¼Ÿ]*?[è¯´é“è®²å«å–Šé—®ç­”å›å¤è¡¨ç¤ºæŠ¬ä¸¾ç‚¹å¤´æ‘‡å¤´çœ‹ç€][:ï¼š]', text)
                
                # 2. åŒ…å«å¼•å·å¯¹è¯
                has_quotes = '"' in text or '"' in text or '"' in text
                
                # 3. åŒ…å«åŠ¨ä½œæˆ–åœºæ™¯æè¿°
                has_narration_content = any(word in text for word in ['åªè§', 'æ­¤æ—¶', 'çªç„¶', 'ç„¶å', 'ä»–ä»¬', 'æŠ¬èµ·', 'çœ‹ç€', 'ç­‰ç€', 'æ„è¯†åˆ°', 'ä¸€èµ·', 'å‘', 'æ‰‘', 'éƒ½', 'è¡€çº¢', 'çœ¼ç›', 'å°¸ä½“', 'æ”¾è„±', 'ç‚¹å¤´', 'æ‘‡å¤´', 'èµ°å‘', 'æ„‰å¿«', 'èŠç€'])
                
                # 4. æ–‡æœ¬è¶³å¤Ÿé•¿å¯èƒ½åŒ…å«å¤šç§å†…å®¹ï¼ˆæé«˜é˜ˆå€¼ï¼‰
                is_potentially_mixed = len(text) > 40
                
                # ğŸ” è°ƒè¯•ä¿¡æ¯
                logger.info(f"[AIå¢å¼ºæ£€æµ‹] æ®µè½{index+1}ç‰¹å¾: é•¿åº¦={len(text)}, "
                           f"è§’è‰²åŠ¨ä½œ={bool(has_speaker_action)}, å¼•å·={has_quotes}, "
                           f"åœºæ™¯æè¿°={has_narration_content}, æ½œåœ¨æ··åˆ={is_potentially_mixed}, "
                           f"æ–‡æœ¬='{text[:50]}...'")
                
                # ğŸ¯ ä¸¥æ ¼å‡†å…¥é€»è¾‘ï¼šåªæ£€æµ‹æ˜æ˜¾çš„æ··åˆæ–‡æœ¬ï¼Œå¤§å¹…å‡å°‘LLMè°ƒç”¨
                should_check = False
                reason = ""
                
                # ğŸš« å¿«é€Ÿæ’é™¤ï¼šæ˜æ˜¾ä¸éœ€è¦æ£€æµ‹çš„æƒ…å†µ
                if len(text) < 20:
                    should_check = False
                    reason = "æ–‡æœ¬è¿‡çŸ­"
                
                # ğŸ”¥ æ ¸å¿ƒç­–ç•¥ï¼šåŒæ—¶åŒ…å«å¯¹è¯æ ‡è®°å’Œå™è¿°å†…å®¹çš„æ··åˆæ–‡æœ¬
                elif has_quotes and has_speaker_action and len(text) > 40:
                    # å¿…é¡»åŒæ—¶æœ‰ï¼šå¼•å· + è¯´è¯åŠ¨ä½œ + è¶³å¤Ÿé•¿åº¦
                    should_check = True
                    reason = "å¼•å·+è¯´è¯åŠ¨ä½œ+è¾ƒé•¿æ–‡æœ¬"
                
                # ğŸ”¥ æ˜ç¡®çš„æ··åˆæ ‡å¿—ï¼šåŒ…å«å®Œæ•´çš„å¯¹è¯æ ¼å¼ä½†å¾ˆé•¿
                elif ('è¯´é“ï¼š' in text or 'è¯´ï¼š' in text) and has_narration_content and len(text) > 50:
                    # å¿…é¡»åŒæ—¶æœ‰ï¼šå¯¹è¯æ ‡è®° + å™è¿°è¯æ±‡ + ä¸­ç­‰é•¿åº¦
                    should_check = True
                    reason = "å¯¹è¯æ ‡è®°+å™è¿°å†…å®¹+ä¸­ç­‰é•¿åº¦"
                
                # ğŸ”¥ è¶…é•¿æ–‡æœ¬ä¸”åŒ…å«å¤šç§å†…å®¹æ ‡å¿—
                elif len(text) > 120 and has_quotes and has_narration_content:
                    # å¿…é¡»åŒæ—¶æœ‰ï¼šè¶…é•¿ + å¼•å· + å™è¿°å†…å®¹
                    should_check = True
                    reason = "è¶…é•¿æ–‡æœ¬+å¼•å·+å™è¿°å†…å®¹"
                
                # ğŸ”¥ å¤æ‚æ··åˆï¼šå¤šä¸ªåŠ¨ä½œ+å¼•å·+è¶³å¤Ÿé•¿åº¦
                elif (len([word for word in ['æŠ¬èµ·', 'çœ‹ç€', 'èµ°å‘', 'èŠç€', 'ç‚¹å¤´', 'æ‘‡å¤´', 'æ¨å¼€', 'é—¯å‡º', 'æ‰‘è¿‡'] if word in text]) >= 2 
                      and has_quotes and len(text) > 45):
                    should_check = True
                    reason = "å¤šä¸ªåŠ¨ä½œ+å¼•å·+é•¿æ–‡æœ¬"
                
                if should_check:
                    suspicious_segments.append((index, text))
                    logger.info(f"[AIå¢å¼ºæ£€æµ‹] âœ… æ®µè½{index+1}æ ‡è®°ä¸ºå¯ç–‘ ({reason}): '{text[:50]}...'")
                else:
                    logger.info(f"[AIå¢å¼ºæ£€æµ‹] âŒ æ®µè½{index+1}æœªæ ‡è®°ä¸ºå¯ç–‘: ä¸ç¬¦åˆä»»ä½•æ£€æµ‹ç­–ç•¥")
        
        # ğŸ”¥ ç¬¬äºŒæ­¥ï¼šåªå¯¹å¯ç–‘æ®µè½ä½¿ç”¨å¤§æ¨¡å‹æ·±åº¦åˆ†æ
        if suspicious_segments:
            logger.info(f"[AIå¢å¼ºæ£€æµ‹] å‘ç° {len(suspicious_segments)} ä¸ªå¯ç–‘æ®µè½ï¼Œå¯ç”¨å¤§æ¨¡å‹æ·±åº¦åˆ†æ (å…±{len(segments)}ä¸ªæ®µè½ï¼Œç­›é€‰ç‡ï¼š{len(suspicious_segments)/len(segments)*100:.1f}%)")
            
            try:
                from app.detectors.ollama_character_detector import OllamaCharacterDetector
                mixed_text_detector = OllamaCharacterDetector()
                logger.info(f"[AIå¢å¼ºæ£€æµ‹] å¤§æ¨¡å‹æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"[AIå¢å¼ºæ£€æµ‹] å¤§æ¨¡å‹æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡æ·±åº¦åˆ†æ: {str(e)}")
            
            if mixed_text_detector:
                import time
                start_time = time.time()
                processed_count = 0
                
                for index, text in suspicious_segments:
                    try:
                        # ä½¿ç”¨å¤§æ¨¡å‹åˆ†æè¿™ä¸ªå¯ç–‘æ®µè½
                        analysis_result = await mixed_text_detector._analyze_single_text(text)
                        detected_segments = analysis_result.get('segments', [])
                        processed_count += 1
                        
                        # å¦‚æœæ£€æµ‹å‡ºå¤šä¸ªæ®µè½ï¼Œè¯´æ˜è¿™æ˜¯æ··åˆæ–‡æœ¬
                        if len(detected_segments) > 1:
                            issues.append(DetectionIssue(
                                issue_type='segment_split_needed',
                                severity='medium',
                                segment_index=index,
                                description=f'æ£€æµ‹åˆ°æ··åˆæ–‡æœ¬ï¼Œå»ºè®®æ‹†åˆ†ä¸º {len(detected_segments)} ä¸ªæ®µè½',
                                suggestion='ç‚¹å‡»æŸ¥çœ‹æ‹†åˆ†å»ºè®®',
                                context=text[:100] + ('...' if len(text) > 100 else ''),
                                fixable=True,
                                fix_data={
                                    'action': 'split_segment', 
                                    'suggested_segments': detected_segments
                                }
                            ))
                            logger.info(f"[AIå¢å¼ºæ£€æµ‹] å‘ç°æ··åˆæ–‡æœ¬ (æ®µè½{index+1}): {len(detected_segments)}ä¸ªå­æ®µè½")
                        
                    except Exception as e:
                        logger.warning(f"[AIå¢å¼ºæ£€æµ‹] æ®µè½{index+1}æ·±åº¦åˆ†æå¤±è´¥: {str(e)}")
                
                # æ€§èƒ½ç»Ÿè®¡
                end_time = time.time()
                total_time = end_time - start_time
                avg_time = total_time / processed_count if processed_count > 0 else 0
                logger.info(f"[AIå¢å¼ºæ£€æµ‹] å¤§æ¨¡å‹åˆ†æå®Œæˆï¼Œå¤„ç†{processed_count}ä¸ªæ®µè½ï¼Œè€—æ—¶{total_time:.2f}ç§’ï¼Œå¹³å‡{avg_time:.2f}ç§’/æ®µè½")
        
        # åŸæœ‰çš„åŸºç¡€AIæ£€æµ‹é€»è¾‘
        for index, segment in enumerate(segments):
            text = segment.get('text', '').strip()
            text_type = segment.get('text_type', 'dialogue')
            
            if not text:
                continue
                
            # æ£€æµ‹å¼•å·å†…å®¹ä½†æ ‡è®°ä¸ºæ—ç™½
            if text_type == 'narration' and ('"' in text or '"' in text or '"' in text):
                quote_content = re.search(r'["""]([^"""]*?)["""]', text)
                if quote_content and len(quote_content.group(1).strip()) > 5:
                    issues.append(DetectionIssue(
                        issue_type='quoted_content_as_narration',
                        severity='medium',
                        segment_index=index,
                        description='åŒ…å«å¼•å·å¯¹è¯å†…å®¹ä½†è¢«æ ‡è®°ä¸ºæ—ç™½',
                        suggestion='å»ºè®®å°†å¼•å·å†…å®¹åˆ†ç¦»ä¸ºç‹¬ç«‹çš„å¯¹è¯ç‰‡æ®µ',
                        context=quote_content.group(1)[:50],
                        fixable=True,
                        fix_data={
                            'action': 'split_quoted_content',
                            'quoted_text': quote_content.group(1)
                        }
                    ))
            
            # æ£€æµ‹æ—ç™½å†…å®¹ä½†æ ‡è®°ä¸ºå¯¹è¯
            if text_type == 'dialogue' and not any(char in text for char in ['"', '"', '"', 'è¯´', 'é“', 'é—®', 'ç­”']):
                # ç®€å•çš„æ—ç™½ç‰¹å¾æ£€æµ‹
                narration_indicators = ['åªè§', 'çªç„¶', 'æ­¤æ—¶', 'ç„¶å', 'æ¥ç€', 'äºæ˜¯', 'è¿™æ—¶']
                if any(indicator in text for indicator in narration_indicators):
                    issues.append(DetectionIssue(
                        issue_type='narration_as_dialogue',
                        severity='medium',
                        segment_index=index,
                        description='ç–‘ä¼¼æ—ç™½å†…å®¹è¢«æ ‡è®°ä¸ºå¯¹è¯',
                        suggestion='å»ºè®®å°†æ­¤ç‰‡æ®µæ ‡è®°ä¸ºæ—ç™½',
                        context=text[:50],
                        fixable=True,
                        fix_data={'action': 'change_to_narration'}
                    ))
        
        logger.info(f"[AIå¢å¼ºæ£€æµ‹] æ™ºèƒ½æ£€æµ‹å®Œæˆï¼Œå‘ç° {len([i for i in issues if i.issue_type == 'segment_split_needed'])} ä¸ªæ··åˆæ–‡æœ¬é—®é¢˜")
        return issues
    
    async def detect_single_segment_issues(self, segment_text: str, segment_index: int = 0) -> List[DetectionIssue]:
        """ğŸ”¥ å•æ®µè½æ™ºèƒ½æ£€æµ‹ - ä¸“é—¨ç”¨äºæ‹†åˆ†æ··åˆæ–‡æœ¬"""
        issues = []
        
        try:
            # ğŸ”¥ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œå•æ®µè½æ£€æµ‹
            try:
                from app.detectors.ollama_character_detector import OllamaCharacterDetector
                character_detector = OllamaCharacterDetector()
                logger.info(f"[å•æ®µè½æ£€æµ‹] ä½¿ç”¨Ollama AIè¿›è¡Œæ™ºèƒ½æ£€æµ‹")
                
                # ä½¿ç”¨å¤§æ¨¡å‹åˆ†æå•æ®µè½
                analysis_result = await character_detector._analyze_single_text(segment_text)
                detected_segments = analysis_result.get('segments', [])
                
            except Exception as e:
                logger.warning(f"[å•æ®µè½æ£€æµ‹] å¤§æ¨¡å‹æ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°ç¼–ç¨‹è§„åˆ™: {str(e)}")
                # å›é€€åˆ°ç¼–ç¨‹è§„åˆ™æ£€æµ‹å™¨
                from app.detectors.character_detectors import ProgrammaticCharacterDetector
                character_detector = ProgrammaticCharacterDetector()
                analysis_result = character_detector.analyze_text_segments(segment_text)
                detected_segments = analysis_result.get('segments', [])
            
            # å¦‚æœæ£€æµ‹å‡ºå¤šä¸ªæ®µè½ï¼Œè¯´æ˜éœ€è¦æ‹†åˆ†
            if len(detected_segments) > 1:
                issues.append(DetectionIssue(
                    issue_type='segment_split_needed',
                    severity='medium',
                    segment_index=segment_index,
                    description=f'æ£€æµ‹åˆ°æ··åˆæ–‡æœ¬ï¼Œå»ºè®®æ‹†åˆ†ä¸º {len(detected_segments)} ä¸ªæ®µè½',
                    suggestion='ç‚¹å‡»æŸ¥çœ‹æ‹†åˆ†å»ºè®®',
                    context=segment_text[:100] + ('...' if len(segment_text) > 100 else ''),
                    fixable=True,
                    fix_data={
                        'action': 'split_segment',
                        'suggested_segments': detected_segments
                    }
                ))
            
            logger.info(f"[å•æ®µè½æ£€æµ‹] æ£€æµ‹æ®µè½é•¿åº¦: {len(segment_text)}, æ£€æµ‹åˆ°æ®µè½æ•°: {len(detected_segments)}, {'éœ€è¦æ‹†åˆ†' if len(detected_segments) > 1 else 'æ— éœ€æ‹†åˆ†'}")
            
        except Exception as e:
            logger.error(f"[å•æ®µè½æ£€æµ‹] æ£€æµ‹å¤±è´¥: {str(e)}")
        
        return issues
    
    async def apply_fixes(self, chapter_id: int, issue_indices: Optional[List[int]] = None) -> Dict[str, Any]:
        """åº”ç”¨ä¿®å¤"""
        db = next(get_db())
        try:
            # è·å–æ£€æµ‹ç»“æœ
            detection_result = await self.detect_chapter_issues(chapter_id, enable_ai_detection=True)
            
            if not detection_result.issues:
                return {'success': True, 'message': 'æ²¡æœ‰å‘ç°éœ€è¦ä¿®å¤çš„é—®é¢˜', 'fixed_count': 0}
            
            # è·å–åˆæˆè®¡åˆ’
            analysis_result = db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter_id
            ).first()
            
            if not analysis_result or not analysis_result.synthesis_plan or 'synthesis_plan' not in analysis_result.synthesis_plan:
                return {'success': False, 'message': 'æœªæ‰¾åˆ°åˆæˆè®¡åˆ’'}
            
            segments = analysis_result.synthesis_plan['synthesis_plan'].copy()
            fixed_count = 0
            
            # ç¡®å®šè¦ä¿®å¤çš„é—®é¢˜
            issues_to_fix = detection_result.issues
            if issue_indices is not None:
                issues_to_fix = [issue for i, issue in enumerate(detection_result.issues) if i in issue_indices]
            
            # æŒ‰ç‰‡æ®µç´¢å¼•æ’åºï¼Œä»åå¾€å‰å¤„ç†ï¼ˆé¿å…ç´¢å¼•å˜åŒ–ï¼‰
            issues_to_fix.sort(key=lambda x: x.segment_index, reverse=True)
            
            for issue in issues_to_fix:
                if not issue.fixable or not issue.fix_data:
                    continue
                
                success = self._apply_single_fix(segments, issue)
                if success:
                    fixed_count += 1
            
            # æ›´æ–°åˆæˆè®¡åˆ’
            analysis_result.synthesis_plan['synthesis_plan'] = segments
            db.commit()
            
            return {
                'success': True,
                'message': f'æˆåŠŸä¿®å¤ {fixed_count} ä¸ªé—®é¢˜',
                'fixed_count': fixed_count
            }
            
        except Exception as e:
            db.rollback()
            return {'success': False, 'message': f'ä¿®å¤å¤±è´¥: {str(e)}'}
        finally:
            db.close()
    
    def _apply_single_fix(self, segments: List[Dict[str, Any]], issue: DetectionIssue) -> bool:
        """åº”ç”¨å•ä¸ªä¿®å¤"""
        try:
            segment_index = issue.segment_index
            if segment_index >= len(segments):
                return False
            
            fix_data = issue.fix_data
            action = fix_data.get('action')
            
            if action == 'clean_special_chars':
                # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
                text = segments[segment_index]['text']
                for char in fix_data.get('chars', []):
                    text = text.replace(char, '')
                segments[segment_index]['text'] = text.strip()
                return True
            
            elif action == 'split_segment':
                # åˆ†å‰²é•¿ç‰‡æ®µ
                text = segments[segment_index]['text']
                max_length = fix_data.get('max_length', 200)
                
                if len(text) > max_length:
                    # ç®€å•æŒ‰å¥å·åˆ†å‰²
                    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
                    new_segments = []
                    current_text = ''
                    
                    for sentence in sentences:
                        if len(current_text + sentence) > max_length and current_text:
                            # åˆ›å»ºæ–°ç‰‡æ®µ
                            new_segment = segments[segment_index].copy()
                            new_segment['text'] = current_text.strip()
                            new_segments.append(new_segment)
                            current_text = sentence
                        else:
                            current_text += sentence + 'ã€‚' if sentence else ''
                    
                    if current_text.strip():
                        new_segment = segments[segment_index].copy()
                        new_segment['text'] = current_text.strip()
                        new_segments.append(new_segment)
                    
                    # æ›¿æ¢åŸç‰‡æ®µ
                    if new_segments:
                        segments[segment_index:segment_index+1] = new_segments
                        return True
            
            elif action == 'change_to_narration':
                # æ”¹ä¸ºæ—ç™½
                segments[segment_index]['text_type'] = 'narration'
                segments[segment_index]['character'] = ''
                segments[segment_index]['voice_type'] = ''
                return True
            
            elif action == 'split_quoted_content':
                # åˆ†ç¦»å¼•å·å†…å®¹
                text = segments[segment_index]['text']
                quoted_text = fix_data.get('quoted_text', '')
                
                if quoted_text in text:
                    # åˆ†ç¦»ä¸ºä¸¤ä¸ªç‰‡æ®µ
                    before_quote = text.split(quoted_text)[0].strip()
                    
                    new_segments = []
                    
                    # æ—ç™½éƒ¨åˆ†
                    if before_quote:
                        narration_segment = segments[segment_index].copy()
                        narration_segment['text'] = before_quote
                        narration_segment['text_type'] = 'narration'
                        narration_segment['character'] = ''
                        narration_segment['voice_type'] = ''
                        new_segments.append(narration_segment)
                    
                    # å¯¹è¯éƒ¨åˆ†
                    dialogue_segment = segments[segment_index].copy()
                    dialogue_segment['text'] = quoted_text
                    dialogue_segment['text_type'] = 'dialogue'
                    new_segments.append(dialogue_segment)
                    
                    # æ›¿æ¢åŸç‰‡æ®µ
                    segments[segment_index:segment_index+1] = new_segments
                    return True
            
            return False
            
        except Exception as e:
            print(f"Fix application error: {e}")
            return False
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    def get_issue_statistics(self, issues: List[DetectionIssue]) -> Dict[str, Any]:
        """è·å–é—®é¢˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total': len(issues),
            'by_severity': {'low': 0, 'medium': 0, 'high': 0},
            'by_type': {},
            'fixable': 0
        }
        
        for issue in issues:
            stats['by_severity'][issue.severity] += 1
            stats['by_type'][issue.issue_type] = stats['by_type'].get(issue.issue_type, 0) + 1
            if issue.fixable:
                stats['fixable'] += 1
        
        return stats
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.utcnow().isoformat()
        
    def detect_issues(self, chapter: BookChapter, use_ai: bool = True) -> Dict[str, Any]:
        """æ£€æµ‹ç« èŠ‚é—®é¢˜
        
        Args:
            chapter: ç« èŠ‚å¯¹è±¡
            use_ai: æ˜¯å¦ä½¿ç”¨AIå¢å¼ºæ£€æµ‹
            
        Returns:
            Dict[str, Any]: æ£€æµ‹ç»“æœå­—å…¸
        """
        try:
            # è·å–åˆ†æç»“æœ
            synthesis_plan = self._get_synthesis_plan(chapter)
            if not synthesis_plan:
                return {
                    "success": False,
                    "error": "ç« èŠ‚æœªå®Œæˆåˆ†ææˆ–åˆæˆè®¡åˆ’ä¸å­˜åœ¨"
                }
                
            segments = synthesis_plan.get('synthesis_plan', [])
            if not segments:
                return {
                    "success": False,
                    "error": "åˆæˆè®¡åˆ’ä¸ºç©º"
                }
            
            # æ‰§è¡ŒåŸºç¡€æ£€æµ‹
            basic_issues = self._detect_basic_issues(segments)
            
            # æ‰§è¡ŒAIå¢å¼ºæ£€æµ‹
            ai_issues = []
            if use_ai:
                ai_issues = self._detect_ai_issues(segments)
            
            # åˆå¹¶é—®é¢˜åˆ—è¡¨
            all_issues = basic_issues + ai_issues
            
            # ç»Ÿè®¡é—®é¢˜
            stats = self.get_issue_statistics(all_issues)
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼è¿”å›
            issues_dict = [{
                "issue_type": issue.issue_type,
                "severity": issue.severity,
                "segment_index": issue.segment_index,
                "description": issue.description,
                "suggestion": issue.suggestion,
                "auto_fixable": hasattr(issue, 'auto_fixable') and issue.auto_fixable
            } for issue in all_issues]
            
            return {
                "success": True,
                "issues": issues_dict,
                "stats": stats
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
            
    def _get_synthesis_plan(self, chapter: BookChapter) -> Optional[Dict[str, Any]]:
        """è·å–ç« èŠ‚çš„åˆæˆè®¡åˆ’"""
        db = next(get_db())
        try:
            analysis_result = db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter.id
            ).first()
            
            if analysis_result and analysis_result.synthesis_plan:
                return analysis_result.synthesis_plan
            return None
        finally:
            db.close()
            
    def auto_fix_issues(self, segments: List[Dict[str, Any]], issues: List[DetectionIssue]) -> tuple:
        """è‡ªåŠ¨ä¿®å¤æ£€æµ‹åˆ°çš„é—®é¢˜
        
        Args:
            segments: åˆæˆè®¡åˆ’ç‰‡æ®µ
            issues: æ£€æµ‹åˆ°çš„é—®é¢˜åˆ—è¡¨
            
        Returns:
            tuple: (ä¿®å¤åçš„ç‰‡æ®µ, ä¿®å¤æ—¥å¿—)
        """
        fixed_segments = segments.copy()
        fix_logs = []
        
        # æŒ‰ç‰‡æ®µç´¢å¼•æ’åºï¼Œä»åå¾€å‰å¤„ç†ï¼ˆé¿å…ç´¢å¼•å˜åŒ–ï¼‰
        fixable_issues = [issue for issue in issues if hasattr(issue, 'auto_fixable') and issue.auto_fixable]
        fixable_issues.sort(key=lambda x: x.segment_index, reverse=True)
        
        for issue in fixable_issues:
            try:
                segment_index = issue.segment_index
                if segment_index >= len(fixed_segments):
                    continue
                
                # æ ¹æ®é—®é¢˜ç±»å‹åº”ç”¨ä¸åŒçš„ä¿®å¤
                if issue.issue_type == 'special_characters':
                    # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
                    text = fixed_segments[segment_index]['text']
                    special_chars = re.findall(r'[#@$%^&*()_+=\[\]{}|;:,.<>?/~`]', text)
                    for char in special_chars:
                        text = text.replace(char, '')
                    fixed_segments[segment_index]['text'] = text.strip()
                    fix_logs.append(f"å·²æ¸…ç†ç‰‡æ®µ {segment_index + 1} ä¸­çš„ç‰¹æ®Šå­—ç¬¦")
                
                elif issue.issue_type == 'long_segment':
                    # åˆ†å‰²é•¿ç‰‡æ®µ
                    text = fixed_segments[segment_index]['text']
                    max_length = 200
                    
                    if len(text) > max_length:
                        # ç®€å•æŒ‰å¥å·åˆ†å‰²
                        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
                        new_segments = []
                        current_text = ''
                        
                        for sentence in sentences:
                            if len(current_text + sentence) > max_length and current_text:
                                # åˆ›å»ºæ–°ç‰‡æ®µ
                                new_segment = fixed_segments[segment_index].copy()
                                new_segment['text'] = current_text.strip()
                                new_segments.append(new_segment)
                                current_text = sentence
                            else:
                                current_text += sentence + 'ã€‚' if sentence else ''
                        
                        if current_text.strip():
                            new_segment = fixed_segments[segment_index].copy()
                            new_segment['text'] = current_text.strip()
                            new_segments.append(new_segment)
                        
                        # æ›¿æ¢åŸç‰‡æ®µ
                        if new_segments:
                            fixed_segments[segment_index:segment_index+1] = new_segments
                            fix_logs.append(f"å·²å°†ç‰‡æ®µ {segment_index + 1} åˆ†å‰²ä¸º {len(new_segments)} ä¸ªç‰‡æ®µ")
                
                elif issue.issue_type == 'narration_as_dialogue':
                    # å°†å¯¹è¯æ”¹ä¸ºæ—ç™½
                    fixed_segments[segment_index]['text_type'] = 'narration'
                    fixed_segments[segment_index]['character'] = ''
                    fixed_segments[segment_index]['voice_type'] = ''
                    fix_logs.append(f"å·²å°†ç‰‡æ®µ {segment_index + 1} ä»å¯¹è¯æ”¹ä¸ºæ—ç™½")
                
            except Exception as e:
                fix_logs.append(f"ä¿®å¤ç‰‡æ®µ {segment_index + 1} æ—¶å‡ºé”™: {str(e)}")
        
        return fixed_segments, fix_logs
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().isoformat()