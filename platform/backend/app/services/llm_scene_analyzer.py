#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½åœºæ™¯åˆ†æå™¨
ä½¿ç”¨Ollama HTTP APIè¿›è¡Œæ·±åº¦åœºæ™¯ç†è§£å’Œåˆ†æ
"""

import os
import json
import logging
import asyncio
import aiohttp
from datetime import datetime
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
from tenacity import retry, stop_after_attempt, wait_exponential
import re
import time

from app.services.sequential_timeline_generator import SceneInfo
from app.services.intelligent_scene_analyzer import SceneAnalysisResult, intelligent_scene_analyzer

logger = logging.getLogger(__name__)

@dataclass
class SceneAnalysis:
    """å•ä¸ªåœºæ™¯åˆ†æç»“æœ"""
    location: str
    keywords: List[str]
    confidence: float

@dataclass
class SceneAnalysisResult:
    """åœºæ™¯åˆ†æç»“æœ"""
    analyzed_scenes: List[SceneAnalysis]
    confidence_score: float
    processing_time: float
    raw_response: str

class OllamaLLMSceneAnalyzer:
    """åŸºäºOllama HTTP APIçš„æ™ºèƒ½åœºæ™¯åˆ†æå™¨"""
    
    def __init__(self):
        # Ollamaé…ç½®
        self.ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.model_name = os.getenv("OLLAMA_MODEL", "qwen2.5:14b")  # ğŸ”¥ æ”¹ç”¨ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
        
        logger.info(f"[LLM_ANALYZER] åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {self.model_name}, URL: {self.ollama_base_url}")

    async def check_ollama_status(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.ollama_base_url}/api/tags", timeout=5) as response:
                    if response.status == 200:
                        data = await response.json()
                        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹
                        models = [model.get("name", "") for model in data.get("models", [])]
                        logger.info(f"Ollamaå¯ç”¨æ¨¡å‹: {models}")
                        return len(models) > 0
                    return False
        except Exception as e:
            logger.error(f"æ£€æŸ¥OllamaçŠ¶æ€å¤±è´¥: {e}")
            return False

    async def check_model_available(self, model_name: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å¯ç”¨"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.ollama_base_url}/api/tags", timeout=5) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [model.get("name", "") for model in data.get("models", [])]
                        # æ£€æŸ¥æ¨¡å‹åç§°åŒ¹é…ï¼ˆæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼‰
                        for model in models:
                            if model_name.split(":")[0] in model:
                                logger.info(f"æ‰¾åˆ°åŒ¹é…æ¨¡å‹: {model}")
                                return True
                        logger.warning(f"æœªæ‰¾åˆ°æ¨¡å‹ {model_name}ï¼Œå¯ç”¨æ¨¡å‹: {models}")
                        return False
                    return False
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§å¤±è´¥: {e}")
            return False

    async def analyze_text_scenes_with_llm(self, text: str) -> SceneAnalysisResult:
        """ä½¿ç”¨LLMåˆ†ææ–‡æœ¬ä¸­çš„åœºæ™¯ç¯å¢ƒ"""
        logger.info(f"[LLM_ANALYZER] å¼€å§‹åˆ†ææ–‡æœ¬ï¼Œé•¿åº¦: {len(text)}å­—ç¬¦")
        
        try:
            # æ£€æµ‹æ˜¯å¦ä¸ºæ‰¹é‡åˆ†æï¼ˆåŒ…å«å¤šä¸ªæ®µè½çš„æ ¼å¼ï¼‰
            is_batch_analysis = self._is_batch_analysis_text(text)
            
            if is_batch_analysis:
                logger.info("[LLM_ANALYZER] æ£€æµ‹åˆ°æ‰¹é‡åˆ†ææ ¼å¼ï¼Œä½¿ç”¨æ‰¹é‡æç¤ºè¯")
                prompt = self._create_batch_analysis_prompt(text)
            else:
                logger.info("[LLM_ANALYZER] ä½¿ç”¨å•æ®µè½åˆ†ææç¤ºè¯")
                prompt = self._create_single_analysis_prompt(text)
            
            # ç›´æ¥ä½¿ç”¨HTTP APIè°ƒç”¨Ollama
            start_time = time.time()
            
            payload = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "num_predict": 1000 if is_batch_analysis else 500,
                }
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.ollama_base_url}/api/chat",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"[LLM_ANALYZER] Ollamaè¯·æ±‚å¤±è´¥ {response.status}: {error_text}")
                        return SceneAnalysisResult(
                            analyzed_scenes=[],
                            confidence_score=0.0,
                            processing_time=time.time() - start_time,
                            raw_response=f"HTTPé”™è¯¯: {response.status}"
                        )
                    
                    data = await response.json()
                    analysis_time = time.time() - start_time
                    
                    if not data or 'message' not in data:
                        logger.error("[LLM_ANALYZER] LLMå“åº”æ ¼å¼é”™è¯¯")
                        return SceneAnalysisResult(
                            analyzed_scenes=[],
                            confidence_score=0.0,
                            processing_time=analysis_time,
                            raw_response="å“åº”æ ¼å¼é”™è¯¯"
                        )
                    
                    response_text = data['message']['content'].strip()
                    logger.info(f"[LLM_ANALYZER] LLMåˆ†æå®Œæˆï¼Œè€—æ—¶: {analysis_time:.2f}s")
                    logger.info(f"[LLM_ANALYZER] LLMå“åº”: {response_text[:200]}...")
                    
                    # è§£æå“åº”
                    if is_batch_analysis:
                        scenes = self._parse_batch_llm_response(response_text)
                    else:
                        scenes = self._parse_single_llm_response(response_text)
                    
                    # è®¡ç®—ç½®ä¿¡åº¦
                    confidence = self._calculate_confidence(scenes, response_text)
                    
                    logger.info(f"[LLM_ANALYZER] è§£æå®Œæˆ: {len(scenes)}ä¸ªåœºæ™¯ï¼Œç½®ä¿¡åº¦: {confidence:.2f}")
                    
                    return SceneAnalysisResult(
                        analyzed_scenes=scenes,
                        confidence_score=confidence,
                        processing_time=analysis_time,
                        raw_response=response_text
                    )
            
        except Exception as e:
            logger.error(f"[LLM_ANALYZER] åˆ†æå¤±è´¥: {str(e)}")
            return SceneAnalysisResult(
                analyzed_scenes=[],
                confidence_score=0.0,
                processing_time=0.0,
                raw_response=f"åˆ†æå¤±è´¥: {str(e)}"
            )
    
    def _is_batch_analysis_text(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºæ‰¹é‡åˆ†ææ–‡æœ¬"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ®µè½ç¼–å·å’Œæ—¶é—´èŒƒå›´çš„æ ¼å¼
        batch_patterns = [
            r'æ®µè½\d+\([^)]+\):',  # æ®µè½1(0.0-15.2s):
            r'è¯·åˆ†æä»¥ä¸‹ç« èŠ‚',      # æ‰¹é‡åˆ†æçš„å¼€å¤´
            r'\d+\.\d+-\d+\.\d+s', # æ—¶é—´èŒƒå›´æ ¼å¼
        ]
        
        for pattern in batch_patterns:
            if re.search(pattern, text):
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªæ®µè½ï¼ˆç®€å•è®¡æ•°ï¼‰
        segment_count = len(re.findall(r'æ®µè½\d+', text))
        return segment_count > 1

    def _create_batch_analysis_prompt(self, text: str) -> str:
        """åˆ›å»ºæ‰¹é‡åˆ†æçš„æç¤ºè¯"""
        return f"""è¯·åˆ†æä»¥ä¸‹ç« èŠ‚ä¸­æ¯ä¸ªæ®µè½çš„ç¯å¢ƒå£°éŸ³ï¼Œæå–ç¯å¢ƒä¸­çš„å£°éŸ³å…ƒç´ ï¼š

{text}

è¯·æŒ‰æ®µè½é¡ºåºè¿”å›ç»“æœï¼Œæ¯ä¸ªæ®µè½ä¸€è¡Œï¼Œæ ¼å¼ï¼š
æ®µè½1: ["å…³é”®è¯1", "å…³é”®è¯2"]
æ®µè½2: ["å…³é”®è¯3", "å…³é”®è¯4"]

è¦æ±‚ï¼š
- åªæå–ç¯å¢ƒå£°éŸ³ï¼šé£å£°ã€é›¨å£°ã€é›·å£°ã€è™«é¸£ã€é¸Ÿå«ã€æ°´å£°ã€è„šæ­¥å£°ã€ç¿»ä¹¦å£°ç­‰
- ä¸è¦æå–äººç‰©åç§°æˆ–å¯¹è¯å†…å®¹
- æ¯ä¸ªæ®µè½æœ€å¤š3ä¸ªå…³é”®è¯
- å¦‚æœæ®µè½æ²¡æœ‰ç¯å¢ƒå£°éŸ³ï¼Œè¿”å›ç©ºæ•°ç»„[]
- ä¸è¦è§£é‡Šï¼Œç›´æ¥è¿”å›ç»“æœ"""

    def _create_single_analysis_prompt(self, text: str) -> str:
        """åˆ›å»ºå•æ®µè½åˆ†æçš„æç¤ºè¯"""
        return f"""ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ç¯å¢ƒå£°éŸ³å…ƒç´ ï¼š

{text}

è¿”å›æ ¼å¼ï¼š["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"]

è¦æ±‚ï¼š
- åªæå–ç¯å¢ƒå£°éŸ³ï¼šé£å£°ã€é›¨å£°ã€é›·å£°ã€è™«é¸£ã€é¸Ÿå«ã€æ°´å£°ã€è„šæ­¥å£°ã€ç¿»ä¹¦å£°ç­‰
- ä¸è¦æå–äººç‰©åç§°æˆ–å¯¹è¯å†…å®¹
- æœ€å¤š3ä¸ªå…³é”®è¯
- å¦‚æœæ²¡æœ‰ç¯å¢ƒå£°éŸ³ï¼Œè¿”å›[]
- ä¸è¦è§£é‡Šï¼Œç›´æ¥è¿”å›ç»“æœ"""
    
    def _parse_batch_llm_response(self, response_text: str) -> List[SceneAnalysis]:
        """è§£ææ‰¹é‡åˆ†æçš„LLMå“åº”"""
        scenes = []
        
        # æŸ¥æ‰¾æ®µè½æ ¼å¼çš„å“åº”
        lines = response_text.strip().split('\n')
        segment_pattern = r'æ®µè½(\d+):\s*(\[.*?\])'
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            match = re.search(segment_pattern, line)
            if match:
                segment_num = int(match.group(1))
                keywords_str = match.group(2)
                
                try:
                    # è§£æJSONæ•°ç»„
                    keywords = json.loads(keywords_str)
                    if isinstance(keywords, list) and keywords:
                        # æ¸…ç†å…³é”®è¯
                        clean_keywords = [kw.strip() for kw in keywords if kw.strip()]
                        if clean_keywords:
                            scenes.append(SceneAnalysis(
                                location="detected_environment",
                                keywords=clean_keywords[:3],  # æœ€å¤š3ä¸ª
                                confidence=0.9
                            ))
                            logger.info(f"[BATCH_PARSER] æ®µè½{segment_num}: {clean_keywords}")
                except json.JSONDecodeError:
                    logger.warning(f"[BATCH_PARSER] æ®µè½{segment_num}è§£æå¤±è´¥: {keywords_str}")
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½æ ¼å¼ï¼Œå°è¯•è§£æä¸ºæ•´ä½“ç»“æœ
        if not scenes:
            logger.info("[BATCH_PARSER] æœªæ‰¾åˆ°æ®µè½æ ¼å¼ï¼Œå°è¯•æ•´ä½“è§£æ")
            scenes = self._parse_single_llm_response(response_text)
        
        return scenes
    
    def _parse_single_llm_response(self, response_text: str) -> List[SceneAnalysis]:
        """è§£æå•æ®µè½åˆ†æçš„LLMå“åº”"""
        # å°è¯•æå–JSONæ•°ç»„
        json_pattern = r'\[([^\]]*)\]'
        matches = re.findall(json_pattern, response_text)
        
        for match in matches:
            try:
                # æ„å»ºå®Œæ•´çš„JSONå­—ç¬¦ä¸²
                json_str = f'[{match}]'
                keywords = json.loads(json_str)
                
                if isinstance(keywords, list) and keywords:
                    # æ¸…ç†å…³é”®è¯
                    clean_keywords = []
                    for kw in keywords:
                        if isinstance(kw, str) and kw.strip():
                            clean_keywords.append(kw.strip())
                    
                    if clean_keywords:
                        return [SceneAnalysis(
                            location="detected_environment",
                            keywords=clean_keywords[:3],  # æœ€å¤š3ä¸ª
                            confidence=0.9
                        )]
                        
            except json.JSONDecodeError:
                continue
        
        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æå–å…³é”®è¯
        logger.info("[SINGLE_PARSER] JSONè§£æå¤±è´¥ï¼Œå°è¯•å…³é”®è¯æå–")
        keywords = self._extract_keywords_from_text(response_text)
        
        if keywords:
            return [SceneAnalysis(
                location="detected_environment",
                keywords=keywords[:3],
                confidence=0.7  # å…³é”®è¯æå–çš„ç½®ä¿¡åº¦è¾ƒä½
            )]
        
        return []
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å¯èƒ½çš„å…³é”®è¯"""
        # å¸¸è§çš„å£°éŸ³å…³é”®è¯
        sound_keywords = [
            'é£å£°', 'é›¨å£°', 'é›·å£°', 'é¸Ÿé¸£', 'è™«é¸£', 'æ°´å£°', 'æµ·æµª', 'æºªæµ',
            'è„šæ­¥å£°', 'æ•²é—¨å£°', 'æ±½è½¦å£°', 'é£æœºå£°', 'éŸ³ä¹å£°', 'è¯´è¯å£°',
            'ç‹—å«', 'çŒ«å«', 'é©¬è¹„å£°', 'é’Ÿå£°', 'é“ƒå£°', 'å“­å£°', 'ç¬‘å£°',
            'ç«ç„°', 'çˆ†ç‚¸', 'ç¢°æ’', 'æ‘©æ“¦', 'æ»´æ°´', 'æµæ°´', 'ç€‘å¸ƒ'
        ]
        
        found_keywords = []
        text_lower = text.lower()
        
        for keyword in sound_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
                
        return found_keywords[:3]  # æœ€å¤šè¿”å›3ä¸ª

    def _calculate_confidence(self, scenes: List[SceneAnalysis], response_text: str) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        if not scenes:
            return 0.0
        
        # åŸºäºåœºæ™¯æ•°é‡å’Œå…³é”®è¯è´¨é‡è®¡ç®—ç½®ä¿¡åº¦
        total_confidence = sum(scene.confidence for scene in scenes)
        avg_confidence = total_confidence / len(scenes)
        
        # æ ¹æ®å“åº”è´¨é‡è°ƒæ•´
        if "json" in response_text.lower() or "[" in response_text:
            # å¦‚æœå“åº”åŒ…å«JSONæ ¼å¼ï¼Œæé«˜ç½®ä¿¡åº¦
            avg_confidence = min(avg_confidence * 1.1, 1.0)
        
        return round(avg_confidence, 2)

# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
llm_scene_analyzer = OllamaLLMSceneAnalyzer()