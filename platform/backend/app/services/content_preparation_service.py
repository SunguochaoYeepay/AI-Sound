"""
å°è¯´ç« èŠ‚åˆæˆè¯­éŸ³å‰å†…å®¹å‡†å¤‡æœåŠ¡
é‡æ„åçš„ç²¾ç®€ç‰ˆæœ¬ - ä¸»è¦è´Ÿè´£æµç¨‹æ§åˆ¶å’Œåè°ƒå„ä¸ªä¸“é—¨æœåŠ¡
"""

import asyncio
import re
import json
import logging
import hashlib
from typing import List, Dict, Any, Optional
from datetime import datetime
from sqlalchemy.orm import Session

from ..models import BookChapter, VoiceProfile
from ..exceptions import ServiceException
from .chapter_chunker import ChapterChunker
from .ai_tts_optimizer import AITTSOptimizer
from .intelligent_voice_mapper import IntelligentVoiceMapper

logger = logging.getLogger(__name__)


class ContentPreparationService:
    """å†…å®¹å‡†å¤‡æœåŠ¡ä¸»æ§åˆ¶å™¨ - é‡æ„åçš„ç²¾ç®€ç‰ˆæœ¬"""
    
    def __init__(self, db: Session):
        self.db = db
        self.chunker = ChapterChunker()
        self.tts_optimizer = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.voice_mapper = IntelligentVoiceMapper(db)
        self.ollama_detector = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    async def prepare_chapter_for_synthesis(
        self, 
        chapter_id: int,
        user_preferences: Dict = None
    ) -> Dict:
        """å‡†å¤‡ç« èŠ‚ç”¨äºè¯­éŸ³åˆæˆçš„å®Œæ•´æµç¨‹"""
        
        try:
            # 1. è·å–ç« èŠ‚æ•°æ®
            chapter = self.db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ServiceException(f"ç« èŠ‚ {chapter_id} ä¸å­˜åœ¨")
            
            logger.info(f"å¼€å§‹å‡†å¤‡ç« èŠ‚ {chapter_id}: {chapter.chapter_title}")
            
            # 2. æ›´æ–°ç« èŠ‚çŠ¶æ€ä¸ºå¤„ç†ä¸­
            chapter.analysis_status = 'analyzing'
            self.db.commit()
            
            # 3. é¢„å¤„ç†æ–‡æœ¬
            cleaned_text = self._clean_and_normalize(chapter.content)
            
            # 4. æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå†³å®šå¤„ç†ç­–ç•¥
            estimated_tokens = self._estimate_tokens(cleaned_text)
            processing_mode = "distributed" if estimated_tokens > 3000 else "single"
            
            logger.info(f"ç« èŠ‚å†…å®¹é•¿åº¦: {len(cleaned_text)} å­—ç¬¦, ä¼°ç®— {estimated_tokens} tokens, ä½¿ç”¨ {processing_mode} æ¨¡å¼")
            
            # 5. æ‰§è¡ŒAIåˆ†æ
            chapter_info = {
                "chapter_id": chapter.id,
                "chapter_title": chapter.chapter_title,
                "chapter_number": chapter.chapter_number,
                "processing_mode": processing_mode
            }
            
            # æ£€æŸ¥ç”¨æˆ·åå¥½ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨ç®€åŒ–æ¨¡å¼
            use_simple_mode = user_preferences and user_preferences.get("processing_mode") == "fast"
            # ğŸ”§ æ–°å¢ï¼šæ”¯æŒTTSä¼˜åŒ–æ¨¡å¼
            tts_optimization_mode = user_preferences and user_preferences.get("tts_optimization", "balanced")
            
            logger.info(f"ğŸ” å¤„ç†æ¨¡å¼å†³ç­–: processing_mode={processing_mode}, use_simple_mode={use_simple_mode}, tts_optimization={tts_optimization_mode}")
            
            if use_simple_mode:
                # ä½¿ç”¨ç®€åŒ–çš„æœ¬åœ°åˆ†æï¼Œä¸ä¾èµ–Ollama
                logger.info("âš¡ ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼Œè·³è¿‡Ollamaåˆ†æ")
                analysis_result = await self._simple_local_analysis(cleaned_text, chapter_info)
            else:
                # å¼ºåˆ¶ä½¿ç”¨AIåˆ†æï¼Œå¹¶å¢å¼ºé”™è¯¯å¤„ç†
                logger.info("ğŸ¤– å°è¯•ä½¿ç”¨Ollama AIè¿›è¡Œè§’è‰²åˆ†æ")
                try:
                    # å»¶è¿Ÿåˆå§‹åŒ–OllamaCharacterDetector
                    if self.ollama_detector is None:
                        logger.info("ğŸ“¦ åˆå§‹åŒ–OllamaCharacterDetector...")
                        try:
                            from ..detectors.ollama_character_detector import OllamaCharacterDetector
                            self.ollama_detector = OllamaCharacterDetector()
                            logger.info("âœ… OllamaCharacterDetectoråˆå§‹åŒ–æˆåŠŸ")
                        except ImportError as e:
                            logger.error(f"âŒ æ— æ³•å¯¼å…¥OllamaCharacterDetector: {str(e)}")
                            raise e
                    
                    # æ‰§è¡ŒAIåˆ†æ
                    logger.info(f"ğŸ”„ å¼€å§‹Ollamaåˆ†æï¼Œæ¨¡å¼: {processing_mode}")
                    if processing_mode == "single":
                        analysis_result = await self.ollama_detector.analyze_text(cleaned_text, chapter_info)
                        logger.info("âœ… Ollamaå•å—åˆ†æå®Œæˆ")
                    else:
                        analysis_result = await self._analyze_chapter_distributed(cleaned_text, chapter_info)
                        logger.info("âœ… Ollamaåˆ†å¸ƒå¼åˆ†æå®Œæˆ")
                        
                except Exception as e:
                    logger.error(f"âŒ Ollama AIåˆ†æå¤±è´¥: {str(e)}")
                    logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                    
                    # åªæœ‰åœ¨æ˜ç¡®ä¸å¯æ¢å¤çš„é”™è¯¯æ—¶æ‰é™çº§
                    if "attempted relative import" in str(e) or "No module named" in str(e):
                        logger.warning("ğŸ”§ æ£€æµ‹åˆ°å¯¼å…¥é”™è¯¯ï¼Œå°è¯•ä¿®å¤åé‡è¯•")
                        # è¿™é‡Œå¯ä»¥æ·»åŠ å¯¼å…¥ä¿®å¤é€»è¾‘
                    
                    logger.warning("â¬‡ï¸  é™çº§åˆ°æœ¬åœ°è§„åˆ™åˆ†æ")
                    analysis_result = await self._simple_local_analysis(cleaned_text, chapter_info)
            
            # 6. ç¡®ä¿æœ‰æ—ç™½è§’è‰²
            detected_characters = analysis_result.get('detected_characters', [])
            detected_characters = self._ensure_narrator_character(detected_characters)
            
            # 7. æ™ºèƒ½è¯­éŸ³æ˜ å°„
            voice_mapping = await self.voice_mapper.intelligent_voice_mapping(detected_characters, user_preferences)
            
            # 8. è½¬æ¢ä¸ºåˆæˆæ ¼å¼ï¼ˆåº”ç”¨TTSä¼˜åŒ–é…ç½®ï¼‰
            synthesis_json = self._adapt_to_synthesis_format(
                analysis_result, 
                voice_mapping,
                tts_optimization_mode=tts_optimization_mode
            )
            
            # 9. ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
            preparation_result = await self._save_preparation_result(
                chapter_id=chapter_id,
                analysis_result=analysis_result,
                synthesis_json=synthesis_json,
                voice_mapping=voice_mapping,
                processing_info={
                    "mode": processing_mode,
                    "total_segments": len(analysis_result.get('segments', [])),
                    "characters_found": len(detected_characters),
                    "estimated_tokens": estimated_tokens,
                    "use_simple_mode": use_simple_mode
                }
            )
            
            # 10. æ›´æ–°ç« èŠ‚çŠ¶æ€ä¸ºå®Œæˆ
            chapter.analysis_status = 'completed'
            chapter.synthesis_status = 'ready'
            self.db.commit()
            
            logger.info(f"ç« èŠ‚ {chapter_id} æ™ºèƒ½å‡†å¤‡å®Œæˆï¼Œå…±è¯†åˆ« {len(detected_characters)} ä¸ªè§’è‰²")
            
            # 11. è¿”å›ç»“æœ
            return {
                "synthesis_json": synthesis_json,
                "processing_info": {
                    "mode": processing_mode,
                    "total_segments": len(analysis_result.get('segments', [])),
                    "characters_found": len(detected_characters),
                    "estimated_tokens": estimated_tokens,
                    "narrator_added": any(char.get('name') == 'æ—ç™½' for char in detected_characters),
                    "voice_mapping": voice_mapping,
                    "saved_to_database": True,
                    "preparation_id": preparation_result.get("id")
                }
            }
            
        except Exception as e:
            # æ›´æ–°ç« èŠ‚çŠ¶æ€ä¸ºå¤±è´¥
            if chapter:
                chapter.analysis_status = 'failed'
                self.db.commit()
            
            logger.error(f"ç« èŠ‚ {chapter_id} æ™ºèƒ½å‡†å¤‡å¤±è´¥: {str(e)}")
            raise ServiceException(f"æ™ºèƒ½å‡†å¤‡å¤±è´¥: {str(e)}")

    async def _save_preparation_result(
        self,
        chapter_id: int,
        analysis_result: Dict,
        synthesis_json: Dict,
        voice_mapping: Dict,
        processing_info: Dict
    ) -> Dict:
        """ä¿å­˜æ™ºèƒ½å‡†å¤‡ç»“æœåˆ°æ•°æ®åº“"""
        
        try:
            # è®¡ç®—å†…å®¹å“ˆå¸Œç”¨äºå»é‡
            content_hash = hashlib.md5(
                json.dumps(analysis_result, sort_keys=True).encode('utf-8')
            ).hexdigest()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¼å…¥analysis_resultsæ¨¡å‹
            try:
                from ..models import AnalysisResult
            except ImportError:
                # å¦‚æœæ²¡æœ‰AnalysisResultæ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„å­˜å‚¨æ–¹æ¡ˆ
                logger.warning("AnalysisResultæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç« èŠ‚å­—æ®µå­˜å‚¨ç»“æœ")
                
                # å°†ç»“æœå­˜å‚¨åœ¨ç« èŠ‚çš„å­—æ®µä¸­
                chapter = self.db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
                if chapter:
                    # åˆ›å»ºå®Œæ•´çš„ç»“æœæ•°æ®
                    full_result = {
                        "analysis_result": analysis_result,
                        "synthesis_json": synthesis_json,
                        "voice_mapping": voice_mapping,
                        "processing_info": processing_info,
                        "content_hash": content_hash,
                        "created_at": datetime.utcnow().isoformat(),
                        "version": 1
                    }
                    
                    # å¦‚æœç« èŠ‚æœ‰character_analysis_resultå­—æ®µï¼Œå­˜å‚¨åœ¨é‚£é‡Œ
                    if hasattr(chapter, 'character_analysis_result'):
                        chapter.character_analysis_result = json.dumps(full_result, ensure_ascii=False)
                    
                    self.db.commit()
                    
                    return {
                        "id": f"chapter_{chapter_id}",
                        "storage_method": "chapter_field",
                        "content_hash": content_hash
                    }
            
            # å¦‚æœæœ‰AnalysisResultæ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†å­˜å‚¨
            # ç”±äºAnalysisResultçš„session_idæ˜¯å¿…éœ€çš„ï¼Œä½†æˆ‘ä»¬çš„æ™ºèƒ½å‡†å¤‡ä¸å±äºç‰¹å®šé¡¹ç›®ä¼šè¯
            # æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„session_idæˆ–ä½¿ç”¨å…¶ä»–å­˜å‚¨æ–¹å¼
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒç« èŠ‚çš„æ™ºèƒ½å‡†å¤‡ç»“æœ
            existing_result = self.db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter_id,
                AnalysisResult.status == 'completed'
            ).order_by(AnalysisResult.created_at.desc()).first()
            
            if existing_result:
                logger.info(f"ç« èŠ‚ {chapter_id} å·²æœ‰æ™ºèƒ½å‡†å¤‡ç»“æœï¼Œæ›´æ–°ç°æœ‰è®°å½•")
                # æ›´æ–°ç°æœ‰è®°å½•
                existing_result.original_analysis = analysis_result
                existing_result.synthesis_plan = synthesis_json
                existing_result.final_config = {
                    "synthesis_json": synthesis_json,
                    "voice_mapping": voice_mapping,
                    "processing_info": processing_info
                }
                existing_result.updated_at = datetime.utcnow()
                self.db.commit()
                
                return {
                    "id": existing_result.id,
                    "storage_method": "analysis_result_updated",
                    "content_hash": content_hash
                }
            
            # æå–è§’è‰²ä¿¡æ¯
            detected_characters = []
            if synthesis_json.get('characters'):
                detected_characters = synthesis_json['characters']
            
            # åˆ›å»ºæ–°çš„åˆ†æç»“æœè®°å½•
            # æ™ºèƒ½å‡†å¤‡ä¸ä¾èµ–é¡¹ç›®sessionï¼Œsession_idè®¾ä¸ºNone
            
            new_result = AnalysisResult(
                session_id=None,  # æ™ºèƒ½å‡†å¤‡ç‹¬ç«‹äºé¡¹ç›®session
                chapter_id=chapter_id,
                original_analysis=analysis_result,
                detected_characters=detected_characters,
                synthesis_plan=synthesis_json,
                final_config={
                    "synthesis_json": synthesis_json,
                    "voice_mapping": voice_mapping,
                    "processing_info": processing_info
                },
                status='completed',
                processing_time=processing_info.get('processing_time', 0),
                confidence_score=85,  # é»˜è®¤ç½®ä¿¡åº¦
                created_at=datetime.utcnow(),
                completed_at=datetime.utcnow()
            )
            
            self.db.add(new_result)
            self.db.commit()
            self.db.refresh(new_result)
            
            logger.info(f"æ™ºèƒ½å‡†å¤‡ç»“æœå·²ä¿å­˜åˆ°AnalysisResultè¡¨ï¼ŒID: {new_result.id}")
            
            return {
                "id": new_result.id,
                "storage_method": "analysis_result_created",
                "content_hash": content_hash
            }
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ™ºèƒ½å‡†å¤‡ç»“æœå¤±è´¥: {str(e)}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
            return {
                "id": None,
                "storage_method": "failed",
                "error": str(e)
            }
    
    async def _simple_local_analysis(self, content: str, chapter_info: Dict) -> Dict:
        """ç®€åŒ–çš„æœ¬åœ°åˆ†æï¼Œä¸ä¾èµ–å¤–éƒ¨AIæœåŠ¡"""
        import re
        
        # åŸºæœ¬çš„å¯¹è¯æ£€æµ‹
        dialogue_patterns = [
            r'"([^"]*)"',  # åŒå¼•å·å¯¹è¯
            r'"([^"]*)"',  # ä¸­æ–‡åŒå¼•å·
            r'ã€Œ([^ã€]*)ã€',  # æ—¥å¼å¼•å·
            r'ã€([^ã€]*)ã€',  # æ—¥å¼ä¹¦åå·
        ]
        
        segments = []
        detected_characters = set()
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n')
        segment_id = 1
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # æ£€æµ‹æ˜¯å¦åŒ…å«å¯¹è¯
            has_dialogue = False
            for pattern in dialogue_patterns:
                if re.search(pattern, paragraph):
                    has_dialogue = True
                    # æå–å¯¹è¯å†…å®¹
                    matches = re.findall(pattern, paragraph)
                    for match in matches:
                        if len(match.strip()) > 2:  # è¿‡æ»¤å¤ªçŸ­çš„å¯¹è¯
                            detected_characters.add(f"è§’è‰²{len(detected_characters) + 1}")
                    break
            
            # åˆ›å»ºæ®µè½
            segments.append({
                'text': paragraph,
                'speaker': f"è§’è‰²{segment_id % 3 + 1}" if has_dialogue else 'æ—ç™½',
                'text_type': 'dialogue' if has_dialogue else 'narration',
                'confidence': 0.8 if has_dialogue else 0.9,
                'detection_method': 'simple_local'
            })
            segment_id += 1
        
        # ç¡®ä¿è‡³å°‘æœ‰æ—ç™½è§’è‰²
        if 'æ—ç™½' not in detected_characters:
            detected_characters.add('æ—ç™½')
        
        # æ„å»ºè§’è‰²åˆ—è¡¨
        character_list = []
        for i, char_name in enumerate(detected_characters):
            character_list.append({
                'name': char_name,
                'confidence': 0.8,
                'source': 'simple_local',
                'recommended_config': {
                    'gender': 'female' if i % 2 == 0 else 'male',
                    'personality': 'gentle' if char_name == 'æ—ç™½' else 'normal'
                }
            })
        
        return {
            'segments': segments,
            'detected_characters': character_list,
            'analysis_metadata': {
                'total_segments': len(segments),
                'total_characters': len(character_list),
                'processing_mode': 'simple_local',
                'method': 'rule_based'
            }
        }
    
    async def _analyze_chapter_distributed(self, chapter_content: str, chapter_info: Dict) -> Dict:
        """åˆ†å¸ƒå¼åˆ†æç« èŠ‚"""
        
        # 1. æ™ºèƒ½åˆ†å—
        chunks = self.chunker.chunk_chapter(chapter_content)
        logger.info(f"ç« èŠ‚åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªåˆ†å—")
        
        # 2. å¹¶è¡Œåˆ†æå¤šä¸ªåˆ†å—
        chunk_results = await self._analyze_chunks_parallel(chunks, chapter_info)
        
        # 3. åˆå¹¶åˆ†æç»“æœ
        merged_result = await self._merge_chunk_results(chunk_results, chapter_info)
        
        return merged_result
    
    async def _analyze_chunks_parallel(self, chunks: List[Dict], chapter_info: Dict) -> List[Dict]:
        """å¹¶è¡Œåˆ†æå¤šä¸ªåˆ†å—"""
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(3)  # æœ€å¤§å¹¶å‘æ•°
        
        async def analyze_chunk_with_semaphore(chunk, index):
            async with semaphore:
                chunk_info = {
                    **chapter_info,
                    "chunk_index": index,
                    "total_chunks": len(chunks),
                    "is_chunk": True
                }
                logger.info(f"å¼€å§‹åˆ†æåˆ†å— {index + 1}/{len(chunks)}")
                
                try:
                    result = await self.ollama_detector.analyze_text(chunk["content"], chunk_info)
                    logger.info(f"åˆ†å— {index + 1} åˆ†æå®Œæˆ")
                    return result
                except Exception as e:
                    logger.error(f"åˆ†å— {index + 1} åˆ†æå¤±è´¥: {str(e)}")
                    return self._create_fallback_result(chunk, chunk_info)
        
        # å¹¶è¡Œæ‰§è¡Œåˆ†æ
        tasks = [
            analyze_chunk_with_semaphore(chunk, i) 
            for i, chunk in enumerate(chunks)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"åˆ†å— {i} åˆ†æå¼‚å¸¸: {result}")
                valid_results.append(self._create_fallback_result(chunks[i], chapter_info))
            else:
                valid_results.append(result)
        
        return valid_results
    
    async def _merge_chunk_results(self, chunk_results: List[Dict], chapter_info: Dict) -> Dict:
        """åˆå¹¶åˆ†å—åˆ†æç»“æœ"""
        
        # åˆå¹¶æ‰€æœ‰æ®µè½
        all_segments = []
        segment_order = 1
        
        for result in chunk_results:
            for segment in result.get('segments', []):
                segment['order'] = segment_order
                all_segments.append(segment)
                segment_order += 1
        
        # åˆå¹¶è§’è‰²ä¿¡æ¯ï¼ˆå»é‡ï¼‰
        all_characters = {}
        for result in chunk_results:
            for character in result.get('detected_characters', []):
                char_name = character['name']
                if char_name not in all_characters:
                    all_characters[char_name] = character
                else:
                    # åˆå¹¶è§’è‰²ä¿¡æ¯ï¼ˆå–ç½®ä¿¡åº¦æ›´é«˜çš„ï¼‰
                    existing = all_characters[char_name]
                    if character.get('confidence', 0) > existing.get('confidence', 0):
                        all_characters[char_name] = character
        
        return {
            'segments': all_segments,
            'detected_characters': list(all_characters.values()),
            'analysis_metadata': {
                'total_chunks': len(chunk_results),
                'total_segments': len(all_segments),
                'total_characters': len(all_characters),
                'processing_mode': 'distributed'
            }
        }
    
    def _create_fallback_result(self, chunk: Dict, chapter_info: Dict) -> Dict:
        """åˆ›å»ºé™çº§ç»“æœ"""
        return {
            'segments': [{
                'text': chunk['content'],
                'speaker': 'æ—ç™½',
                'text_type': 'narration',
                'confidence': 0.5,
                'detection_method': 'fallback'
            }],
            'detected_characters': [{
                'name': 'æ—ç™½',
                'confidence': 1.0,
                'source': 'fallback',
                'recommended_config': {
                    'gender': 'neutral',
                    'personality': 'calm'
                }
            }],
            'processing_stats': {
                'total_segments': 1,
                'characters_found': 1,
                'analysis_method': 'fallback'
            }
        }
    
    def _ensure_narrator_character(self, detected_characters: List[Dict]) -> List[Dict]:
        """ç¡®ä¿è§’è‰²åˆ—è¡¨ä¸­åŒ…å«æ—ç™½è§’è‰²"""
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ—ç™½è§’è‰²
        has_narrator = any(char['name'] == 'æ—ç™½' for char in detected_characters)
        
        if not has_narrator:
            # è‡ªåŠ¨æ·»åŠ æ—ç™½è§’è‰²
            narrator_character = {
                'name': 'æ—ç™½',
                'confidence': 1.0,
                'recommended_config': {
                    'gender': 'neutral',
                    'age_range': 'adult',
                    'personality': 'calm',
                    'voice_style': 'professional'
                },
                'source': 'system_generated',
                'description': 'ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ çš„æ—ç™½è§’è‰²ï¼Œç”¨äºå™è¿°æ€§æ–‡æœ¬'
            }
            detected_characters.append(narrator_character)
            logger.info("ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ æ—ç™½è§’è‰²")
        
        return detected_characters
    
    def _adapt_to_synthesis_format(
        self, 
        analysis_result: Dict, 
        voice_mapping: Dict[str, int],
        available_voices: List[Dict] = None,
        tts_optimization_mode: str = "balanced"
    ) -> Dict:
        """é€‚é…ä¸ºç°æœ‰åˆæˆç³»ç»Ÿçš„JSONæ ¼å¼"""
        
        # æ„å»ºvoice_idåˆ°voice_nameçš„æ˜ å°„
        voice_id_to_name = {}
        if available_voices:
            voice_id_to_name = {v['id']: v['name'] for v in available_voices}
        
        # æ ¼å¼åŒ–è§’è‰²ä¿¡æ¯
        characters = []
        for character in analysis_result['detected_characters']:
            char_name = character['name']
            voice_id = voice_mapping.get(char_name)
            if voice_id:
                characters.append({
                    "name": char_name,
                    "voice_id": voice_id,
                    "voice_name": voice_id_to_name.get(voice_id, f"Voice_{voice_id}")
                })
        
        # æ ¼å¼åŒ–åˆæˆè®¡åˆ’
        synthesis_plan = []
        segment_id = 1
        
        for segment in analysis_result['segments']:
            # è·å–è¯­éŸ³ä¿¡æ¯
            voice_id = voice_mapping.get(segment['speaker'])
            voice_name = voice_id_to_name.get(voice_id, f"Voice_{voice_id}") if voice_id else "æœªåˆ†é…"
            
            # ğŸ¯ æ™ºèƒ½TTSå‚æ•°é…ç½® - åŸºäºè§’è‰²å’Œæ–‡æœ¬å†…å®¹
            if not self.tts_optimizer:
                self.tts_optimizer = AITTSOptimizer(self.ollama_detector)
                # æ ¹æ®ä¼˜åŒ–æ¨¡å¼é…ç½®TTSåˆ†æ
                if tts_optimization_mode == "fast":
                    self.tts_optimizer.set_enable_ai_analysis(False)
                    logger.info("ğŸš€ TTSä¼˜åŒ–å™¨è®¾ç½®ä¸ºå¿«é€Ÿæ¨¡å¼ï¼ˆç¦ç”¨AIåˆ†æï¼‰")
                elif tts_optimization_mode == "quality":
                    self.tts_optimizer.set_enable_ai_analysis(True)
                    logger.info("ğŸ¯ TTSä¼˜åŒ–å™¨è®¾ç½®ä¸ºè´¨é‡æ¨¡å¼ï¼ˆå¯ç”¨AIåˆ†æï¼‰")
                else:  # balanced
                    self.tts_optimizer.set_enable_ai_analysis(True)
                    logger.info("âš–ï¸ TTSä¼˜åŒ–å™¨è®¾ç½®ä¸ºå¹³è¡¡æ¨¡å¼")
            
            tts_params = self.tts_optimizer.get_smart_tts_params(segment, analysis_result.get('detected_characters', []))
            
            synthesis_plan.append({
                "segment_id": segment_id,
                "text": segment['text'],  # ğŸ”’ åŸæ–‡ä¸å˜
                "speaker": segment['speaker'],
                "voice_id": voice_id,
                "voice_name": voice_name,
                "parameters": tts_params
            })
            segment_id += 1
        
        # å®Œå…¨åŒ¹é…ç°æœ‰ç³»ç»Ÿæ ¼å¼
        return {
            "project_info": {
                "novel_type": "æ™ºèƒ½æ£€æµ‹",
                "analysis_time": datetime.now().isoformat(),
                "total_segments": len(synthesis_plan),
                "ai_model": "optimized-smart-analysis",
                "detected_characters": len(characters)
            },
            "synthesis_plan": synthesis_plan,
            "characters": characters
        }
    

    
    def _clean_and_normalize(self, text: str) -> str:
        """æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬"""
        # åŸºæœ¬æ¸…ç†ï¼Œä¿æŒåŸæ–‡å®Œæ•´æ€§
        text = text.strip()
        # ç»Ÿä¸€æ¢è¡Œç¬¦
        text = re.sub(r'\r\n', '\n', text)
        # ç§»é™¤å¤šä½™ç©ºè¡Œï¼ˆä¿ç•™æ®µè½ç»“æ„ï¼‰
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬tokenæ•°é‡"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        return int(chinese_chars * 1.5 + english_words)
    


    async def get_content_stats(self, chapter_id: int, db: Session) -> Dict:
        """
        è·å–ç« èŠ‚å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            from app.models import BookChapter
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ValueError("ç« èŠ‚ä¸å­˜åœ¨")
            
            # åŸºæœ¬ç»Ÿè®¡
            content = chapter.content or ""
            word_count = len(content.replace(' ', '').replace('\n', ''))
            
            # ä½¿ç”¨ChapterChunkerè¿›è¡Œåˆ†å—åˆ†æ
            chunker = ChapterChunker()
            chunks = chunker.chunk_chapter(content)
            
            # ä¼°ç®—å¤„ç†æ—¶é—´å’Œå»ºè®®
            estimated_time = len(chunks) * 2  # æ¯ä¸ªchunkå¤§çº¦2ç§’
            processing_recommendation = "fast" if len(chunks) <= 5 else "detailed" if len(chunks) <= 15 else "distributed"
            
            return {
                "chapter_id": chapter_id,
                "chapter_title": chapter.chapter_title,
                "word_count": word_count,
                "chunk_count": len(chunks),
                "estimated_processing_time": estimated_time,
                "processing_recommendation": processing_recommendation,
                "content_preview": content[:200] + "..." if len(content) > 200 else content
            }
            
        except Exception as e:
            logger.error(f"è·å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {str(e)}")
            raise

    async def get_synthesis_preview(self, chapter_id: int, db: Session) -> Dict:
        """
        è·å–ç« èŠ‚è¯­éŸ³åˆæˆé¢„è§ˆ
        """
        try:
            from app.models import BookChapter
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ValueError("ç« èŠ‚ä¸å­˜åœ¨")
            
            content = chapter.content or ""
            
            # å¿«é€Ÿè§’è‰²æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            import re
            dialogue_patterns = [
                r'"([^"]*)"',  # åŒå¼•å·å¯¹è¯
                r'"([^"]*)"',  # ä¸­æ–‡åŒå¼•å·
                r'ã€Œ([^ã€]*)ã€',  # æ—¥å¼å¼•å·
            ]
            
            dialogues = []
            for pattern in dialogue_patterns:
                matches = re.findall(pattern, content)
                dialogues.extend(matches)
            
            # ä¼°ç®—è§’è‰²æ•°é‡ï¼ˆç®€åŒ–ï¼‰
            estimated_characters = min(len(set(dialogues[:10])), 8) if dialogues else 1
            
            # ä½¿ç”¨ChapterChunkerè¿›è¡Œåˆ†å—
            chunker = ChapterChunker()
            chunks = chunker.chunk_chapter(content)
            
            return {
                "chapter_id": chapter_id,
                "chapter_title": chapter.chapter_title,
                "estimated_characters": estimated_characters,
                "dialogue_count": len(dialogues),
                "chunk_count": len(chunks),
                "sample_dialogues": dialogues[:5],  # å‰5ä¸ªå¯¹è¯ç¤ºä¾‹
                "processing_complexity": "simple" if len(chunks) <= 3 else "moderate" if len(chunks) <= 10 else "complex"
            }
            
        except Exception as e:
            logger.error(f"è·å–åˆæˆé¢„è§ˆå¤±è´¥: {str(e)}")
            raise

    async def get_preparation_status(self, chapter_id: int, db: Session) -> Dict:
        """
        è·å–ç« èŠ‚å‡†å¤‡çŠ¶æ€
        """
        try:
            from app.models import BookChapter
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ValueError("ç« èŠ‚ä¸å­˜åœ¨")
            
            # æ£€æŸ¥åˆ†æçŠ¶æ€
            is_analyzed = getattr(chapter, 'analysis_status', 'pending') == 'completed'
            is_synthesis_ready = getattr(chapter, 'synthesis_status', 'pending') in ['ready', 'completed']
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³çš„åˆæˆé…ç½®ï¼ˆä»AnalysisResultè¡¨ä¸­æŸ¥è¯¢ï¼‰
            try:
                from ..models import AnalysisResult
                latest_result = self.db.query(AnalysisResult).filter(
                    AnalysisResult.chapter_id == chapter_id,
                    AnalysisResult.status == 'completed'
                ).order_by(AnalysisResult.created_at.desc()).first()
                
                has_synthesis_config = bool(latest_result)
                has_complete_synthesis_config = False
                
                if latest_result:
                    # æ£€æŸ¥æ˜¯å¦æœ‰synthesis_planæˆ–final_config
                    has_synthesis_plan = bool(latest_result.synthesis_plan)
                    has_final_config = bool(latest_result.final_config)
                    
                    # å¦‚æœæœ‰final_configï¼Œæ£€æŸ¥å…¶ä¸­æ˜¯å¦åŒ…å«synthesis_json
                    if has_final_config and latest_result.final_config:
                        try:
                            final_config = latest_result.final_config
                            if isinstance(final_config, str):
                                import json
                                final_config = json.loads(final_config)
                            has_complete_synthesis_config = bool(final_config.get('synthesis_json'))
                        except:
                            has_complete_synthesis_config = has_synthesis_plan
                    else:
                        has_complete_synthesis_config = has_synthesis_plan
                        
            except ImportError:
                # å¦‚æœæ²¡æœ‰AnalysisResultæ¨¡å‹ï¼Œå›é€€åˆ°ç« èŠ‚å­—æ®µæ£€æŸ¥
                analysis_result = getattr(chapter, 'analysis_result', None)
                has_synthesis_config = bool(analysis_result)
                has_complete_synthesis_config = False
                
                if has_synthesis_config and analysis_result:
                    try:
                        import json
                        if isinstance(analysis_result, str):
                            result_data = json.loads(analysis_result)
                        else:
                            result_data = analysis_result
                        
                        has_complete_synthesis_config = bool(result_data.get('synthesis_json'))
                    except:
                        has_complete_synthesis_config = False
            
            return {
                "chapter_id": chapter_id,
                "chapter_title": chapter.chapter_title,
                "is_analyzed": is_analyzed,
                "is_synthesis_ready": is_synthesis_ready,
                "has_synthesis_config": has_synthesis_config,
                "has_complete_synthesis_config": has_complete_synthesis_config,
                "analysis_status": getattr(chapter, 'analysis_status', 'pending'),
                "synthesis_status": getattr(chapter, 'synthesis_status', 'pending'),
                "last_updated": chapter.updated_at.isoformat() if getattr(chapter, 'updated_at', None) else None,
                "preparation_complete": is_analyzed and is_synthesis_ready and has_complete_synthesis_config
            }
            
        except Exception as e:
            logger.error(f"è·å–å‡†å¤‡çŠ¶æ€å¤±è´¥: {str(e)}")
            raise 