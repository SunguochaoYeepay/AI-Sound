"""
å°è¯´ç« èŠ‚åˆæˆè¯­éŸ³å‰å†…å®¹å‡†å¤‡æœåŠ¡
å®ç°æ™ºèƒ½è§’è‰²è¯†åˆ«ã€æƒ…ç»ªåˆ†æã€å‚æ•°é…ç½®ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import re
import json
import logging
import hashlib
from typing import List, Dict, Any, Optional
from datetime import datetime
from sqlalchemy.orm import Session

from ..models import BookChapter, VoiceProfile
from ..exceptions import ServiceException

logger = logging.getLogger(__name__)


class ChapterChunker:
    """ç« èŠ‚æ™ºèƒ½åˆ†å—å™¨ - è§£å†³å¤§æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶"""
    
    def __init__(self, max_tokens: int = 3000):
        self.max_tokens = max_tokens
        self.overlap_tokens = 200  # é‡å tokenæ•°ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
    
    def chunk_chapter(self, chapter_content: str) -> List[Dict]:
        """æ™ºèƒ½åˆ†å—ç« èŠ‚å†…å®¹"""
        # 1. æŒ‰è‡ªç„¶æ®µè½åˆ†å‰²
        paragraphs = self._split_by_paragraphs(chapter_content)
        
        # 2. ä¼°ç®—tokenæ•°é‡
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for para in paragraphs:
            para_tokens = self._estimate_tokens(para)
            
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…é•¿ï¼Œéœ€è¦å¼ºåˆ¶åˆ†å‰²
            if para_tokens > self.max_tokens:
                if current_chunk:
                    chunks.append(self._create_chunk(current_chunk))
                    current_chunk = []
                    current_tokens = 0
                
                # å¼ºåˆ¶åˆ†å‰²è¶…é•¿æ®µè½
                sub_chunks = self._force_split_paragraph(para)
                chunks.extend(sub_chunks)
                continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°å»ºchunk
            if current_tokens + para_tokens > self.max_tokens:
                chunks.append(self._create_chunk(current_chunk))
                
                # ä¿æŒé‡å ä¸Šä¸‹æ–‡
                overlap_paras = self._get_overlap_context(current_chunk)
                current_chunk = overlap_paras + [para]
                current_tokens = sum(self._estimate_tokens(p) for p in current_chunk)
            else:
                current_chunk.append(para)
                current_tokens += para_tokens
        
        # å¤„ç†æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append(self._create_chunk(current_chunk))
        
        return chunks
    
    def _split_by_paragraphs(self, text: str) -> List[str]:
        """æŒ‰è‡ªç„¶æ®µè½åˆ†å‰²æ–‡æœ¬"""
        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
        paragraphs = re.split(r'\n\s*\n', text.strip())
        # è¿‡æ»¤ç©ºæ®µè½
        return [p.strip() for p in paragraphs if p.strip()]
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆä¸­æ–‡æŒ‰å­—ç¬¦æ•°ä¼°ç®—ï¼‰"""
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦ * 1.5 + è‹±æ–‡å•è¯æ•°
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        return int(chinese_chars * 1.5 + english_words)
    
    def _create_chunk(self, paragraphs: List[str]) -> Dict:
        """åˆ›å»ºåˆ†å—æ•°æ®"""
        content = "\n\n".join(paragraphs)
        return {
            "content": content,
            "paragraph_count": len(paragraphs),
            "estimated_tokens": self._estimate_tokens(content),
            "chunk_type": "normal"
        }
    
    def _force_split_paragraph(self, paragraph: str) -> List[Dict]:
        """å¼ºåˆ¶åˆ†å‰²è¶…é•¿æ®µè½"""
        # æŒ‰å¥å·åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', paragraph)
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            sentence = sentence.strip() + "ã€‚"  # æ¢å¤å¥å·
            sentence_tokens = self._estimate_tokens(sentence)
            
            if current_tokens + sentence_tokens > self.max_tokens:
                if current_chunk:
                    chunks.append(self._create_chunk(current_chunk))
                current_chunk = [sentence]
                current_tokens = sentence_tokens
            else:
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
        
        if current_chunk:
            chunks.append(self._create_chunk(current_chunk))
        
        return chunks
    
    def _get_overlap_context(self, paragraphs: List[str]) -> List[str]:
        """è·å–é‡å ä¸Šä¸‹æ–‡"""
        if not paragraphs:
            return []
        
        # å–æœ€å1-2ä¸ªæ®µè½ä½œä¸ºé‡å ä¸Šä¸‹æ–‡
        overlap_tokens = 0
        overlap_paras = []
        
        for para in reversed(paragraphs):
            para_tokens = self._estimate_tokens(para)
            if overlap_tokens + para_tokens <= self.overlap_tokens:
                overlap_paras.insert(0, para)
                overlap_tokens += para_tokens
            else:
                break
        
        return overlap_paras


class ContentPreparationService:
    """å†…å®¹å‡†å¤‡æœåŠ¡ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, db: Session):
        self.db = db
        self.chunker = ChapterChunker()
        self.ollama_detector = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    async def prepare_chapter_for_synthesis(
        self, 
        chapter_id: int,
        user_preferences: Dict = None
    ) -> Dict:
        """å‡†å¤‡ç« èŠ‚ç”¨äºè¯­éŸ³åˆæˆçš„å®Œæ•´æµç¨‹"""
        
        try:
            # 1. è·å–ç« èŠ‚æ•°æ®
            chapter = self.db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ServiceException(f"ç« èŠ‚ {chapter_id} ä¸å­˜åœ¨")
            
            logger.info(f"å¼€å§‹å‡†å¤‡ç« èŠ‚ {chapter_id}: {chapter.chapter_title}")
            
            # 2. æ›´æ–°ç« èŠ‚çŠ¶æ€ä¸ºå¤„ç†ä¸­
            chapter.analysis_status = 'analyzing'
            self.db.commit()
            
            # 3. é¢„å¤„ç†æ–‡æœ¬
            cleaned_text = self._clean_and_normalize(chapter.content)
            
            # 4. æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå†³å®šå¤„ç†ç­–ç•¥
            estimated_tokens = self._estimate_tokens(cleaned_text)
            processing_mode = "distributed" if estimated_tokens > 3000 else "single"
            
            logger.info(f"ç« èŠ‚å†…å®¹é•¿åº¦: {len(cleaned_text)} å­—ç¬¦, ä¼°ç®— {estimated_tokens} tokens, ä½¿ç”¨ {processing_mode} æ¨¡å¼")
            
            # 5. æ‰§è¡ŒAIåˆ†æ
            chapter_info = {
                "chapter_id": chapter.id,
                "chapter_title": chapter.chapter_title,
                "chapter_number": chapter.chapter_number,
                "processing_mode": processing_mode
            }
            
            # æ£€æŸ¥ç”¨æˆ·åå¥½ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨ç®€åŒ–æ¨¡å¼
            use_simple_mode = user_preferences and user_preferences.get("processing_mode") == "fast"
            
            if use_simple_mode:
                # ä½¿ç”¨ç®€åŒ–çš„æœ¬åœ°åˆ†æï¼Œä¸ä¾èµ–Ollama
                analysis_result = await self._simple_local_analysis(cleaned_text, chapter_info)
            else:
                # å°è¯•ä½¿ç”¨AIåˆ†æï¼Œå¦‚æœå¤±è´¥åˆ™é™çº§åˆ°ç®€åŒ–æ¨¡å¼
                try:
                    # å»¶è¿Ÿåˆå§‹åŒ–OllamaCharacterDetector
                    if self.ollama_detector is None:
                        from ..api.v1.chapters import OllamaCharacterDetector
                        self.ollama_detector = OllamaCharacterDetector()
                    
                    if processing_mode == "single":
                        analysis_result = await self.ollama_detector.analyze_text(cleaned_text, chapter_info)
                    else:
                        analysis_result = await self._analyze_chapter_distributed(cleaned_text, chapter_info)
                        
                except Exception as e:
                    logger.warning(f"AIåˆ†æå¤±è´¥ï¼Œé™çº§åˆ°æœ¬åœ°åˆ†æ: {str(e)}")
                    analysis_result = await self._simple_local_analysis(cleaned_text, chapter_info)
            
            # 6. ç¡®ä¿æœ‰æ—ç™½è§’è‰²
            detected_characters = analysis_result.get('detected_characters', [])
            detected_characters = self._ensure_narrator_character(detected_characters)
            
            # 7. æ™ºèƒ½è¯­éŸ³æ˜ å°„
            voice_mapping = await self._intelligent_voice_mapping(detected_characters, user_preferences)
            
            # 8. è½¬æ¢ä¸ºåˆæˆæ ¼å¼
            synthesis_json = self._adapt_to_synthesis_format(
                analysis_result, 
                voice_mapping
            )
            
            # 9. ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
            preparation_result = await self._save_preparation_result(
                chapter_id=chapter_id,
                analysis_result=analysis_result,
                synthesis_json=synthesis_json,
                voice_mapping=voice_mapping,
                processing_info={
                    "mode": processing_mode,
                    "total_segments": len(analysis_result.get('segments', [])),
                    "characters_found": len(detected_characters),
                    "estimated_tokens": estimated_tokens,
                    "use_simple_mode": use_simple_mode
                }
            )
            
            # 10. æ›´æ–°ç« èŠ‚çŠ¶æ€ä¸ºå®Œæˆ
            chapter.analysis_status = 'completed'
            chapter.synthesis_status = 'ready'
            self.db.commit()
            
            logger.info(f"ç« èŠ‚ {chapter_id} æ™ºèƒ½å‡†å¤‡å®Œæˆï¼Œå…±è¯†åˆ« {len(detected_characters)} ä¸ªè§’è‰²")
            
            # 11. è¿”å›ç»“æœ
            return {
                "synthesis_json": synthesis_json,
                "processing_info": {
                    "mode": processing_mode,
                    "total_segments": len(analysis_result.get('segments', [])),
                    "characters_found": len(detected_characters),
                    "estimated_tokens": estimated_tokens,
                    "narrator_added": any(char.get('name') == 'æ—ç™½' for char in detected_characters),
                    "voice_mapping": voice_mapping,
                    "saved_to_database": True,
                    "preparation_id": preparation_result.get("id")
                }
            }
            
        except Exception as e:
            # æ›´æ–°ç« èŠ‚çŠ¶æ€ä¸ºå¤±è´¥
            if chapter:
                chapter.analysis_status = 'failed'
                self.db.commit()
            
            logger.error(f"ç« èŠ‚ {chapter_id} æ™ºèƒ½å‡†å¤‡å¤±è´¥: {str(e)}")
            raise ServiceException(f"æ™ºèƒ½å‡†å¤‡å¤±è´¥: {str(e)}")

    async def _save_preparation_result(
        self,
        chapter_id: int,
        analysis_result: Dict,
        synthesis_json: Dict,
        voice_mapping: Dict,
        processing_info: Dict
    ) -> Dict:
        """ä¿å­˜æ™ºèƒ½å‡†å¤‡ç»“æœåˆ°æ•°æ®åº“"""
        
        try:
            # è®¡ç®—å†…å®¹å“ˆå¸Œç”¨äºå»é‡
            content_hash = hashlib.md5(
                json.dumps(analysis_result, sort_keys=True).encode('utf-8')
            ).hexdigest()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¼å…¥analysis_resultsæ¨¡å‹
            try:
                from ..models import AnalysisResult
            except ImportError:
                # å¦‚æœæ²¡æœ‰AnalysisResultæ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„å­˜å‚¨æ–¹æ¡ˆ
                logger.warning("AnalysisResultæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç« èŠ‚å­—æ®µå­˜å‚¨ç»“æœ")
                
                # å°†ç»“æœå­˜å‚¨åœ¨ç« èŠ‚çš„å­—æ®µä¸­
                chapter = self.db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
                if chapter:
                    # åˆ›å»ºå®Œæ•´çš„ç»“æœæ•°æ®
                    full_result = {
                        "analysis_result": analysis_result,
                        "synthesis_json": synthesis_json,
                        "voice_mapping": voice_mapping,
                        "processing_info": processing_info,
                        "content_hash": content_hash,
                        "created_at": datetime.utcnow().isoformat(),
                        "version": 1
                    }
                    
                    # å¦‚æœç« èŠ‚æœ‰character_analysis_resultå­—æ®µï¼Œå­˜å‚¨åœ¨é‚£é‡Œ
                    if hasattr(chapter, 'character_analysis_result'):
                        chapter.character_analysis_result = json.dumps(full_result, ensure_ascii=False)
                    
                    self.db.commit()
                    
                    return {
                        "id": f"chapter_{chapter_id}",
                        "storage_method": "chapter_field",
                        "content_hash": content_hash
                    }
            
            # å¦‚æœæœ‰AnalysisResultæ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†å­˜å‚¨
            # ç”±äºAnalysisResultçš„session_idæ˜¯å¿…éœ€çš„ï¼Œä½†æˆ‘ä»¬çš„æ™ºèƒ½å‡†å¤‡ä¸å±äºç‰¹å®šé¡¹ç›®ä¼šè¯
            # æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„session_idæˆ–ä½¿ç”¨å…¶ä»–å­˜å‚¨æ–¹å¼
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒç« èŠ‚çš„æ™ºèƒ½å‡†å¤‡ç»“æœ
            existing_result = self.db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter_id,
                AnalysisResult.status == 'completed'
            ).order_by(AnalysisResult.created_at.desc()).first()
            
            if existing_result:
                logger.info(f"ç« èŠ‚ {chapter_id} å·²æœ‰æ™ºèƒ½å‡†å¤‡ç»“æœï¼Œæ›´æ–°ç°æœ‰è®°å½•")
                # æ›´æ–°ç°æœ‰è®°å½•
                existing_result.original_analysis = analysis_result
                existing_result.synthesis_plan = synthesis_json
                existing_result.final_config = {
                    "synthesis_json": synthesis_json,
                    "voice_mapping": voice_mapping,
                    "processing_info": processing_info
                }
                existing_result.updated_at = datetime.utcnow()
                self.db.commit()
                
                return {
                    "id": existing_result.id,
                    "storage_method": "analysis_result_updated",
                    "content_hash": content_hash
                }
            
            # æå–è§’è‰²ä¿¡æ¯
            detected_characters = []
            if synthesis_json.get('characters'):
                detected_characters = synthesis_json['characters']
            
            # åˆ›å»ºæ–°çš„åˆ†æç»“æœè®°å½•
            # ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„session_idæ¥æ ‡è¯†æ™ºèƒ½å‡†å¤‡
            special_session_id = 999999  # ç‰¹æ®Šçš„session_idç”¨äºæ™ºèƒ½å‡†å¤‡
            
            new_result = AnalysisResult(
                session_id=special_session_id,  # ä½¿ç”¨ç‰¹æ®Šsession_id
                chapter_id=chapter_id,
                original_analysis=analysis_result,
                detected_characters=detected_characters,
                synthesis_plan=synthesis_json,
                final_config={
                    "synthesis_json": synthesis_json,
                    "voice_mapping": voice_mapping,
                    "processing_info": processing_info
                },
                status='completed',
                processing_time=processing_info.get('processing_time', 0),
                confidence_score=85,  # é»˜è®¤ç½®ä¿¡åº¦
                created_at=datetime.utcnow(),
                completed_at=datetime.utcnow()
            )
            
            self.db.add(new_result)
            self.db.commit()
            self.db.refresh(new_result)
            
            logger.info(f"æ™ºèƒ½å‡†å¤‡ç»“æœå·²ä¿å­˜åˆ°AnalysisResultè¡¨ï¼ŒID: {new_result.id}")
            
            return {
                "id": new_result.id,
                "storage_method": "analysis_result_created",
                "content_hash": content_hash
            }
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ™ºèƒ½å‡†å¤‡ç»“æœå¤±è´¥: {str(e)}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
            return {
                "id": None,
                "storage_method": "failed",
                "error": str(e)
            }
    
    async def _simple_local_analysis(self, content: str, chapter_info: Dict) -> Dict:
        """ç®€åŒ–çš„æœ¬åœ°åˆ†æï¼Œä¸ä¾èµ–å¤–éƒ¨AIæœåŠ¡"""
        import re
        
        # åŸºæœ¬çš„å¯¹è¯æ£€æµ‹
        dialogue_patterns = [
            r'"([^"]*)"',  # åŒå¼•å·å¯¹è¯
            r'"([^"]*)"',  # ä¸­æ–‡åŒå¼•å·
            r'ã€Œ([^ã€]*)ã€',  # æ—¥å¼å¼•å·
            r'ã€([^ã€]*)ã€',  # æ—¥å¼ä¹¦åå·
        ]
        
        segments = []
        detected_characters = set()
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n')
        segment_id = 1
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # æ£€æµ‹æ˜¯å¦åŒ…å«å¯¹è¯
            has_dialogue = False
            for pattern in dialogue_patterns:
                if re.search(pattern, paragraph):
                    has_dialogue = True
                    # æå–å¯¹è¯å†…å®¹
                    matches = re.findall(pattern, paragraph)
                    for match in matches:
                        if len(match.strip()) > 2:  # è¿‡æ»¤å¤ªçŸ­çš„å¯¹è¯
                            detected_characters.add(f"è§’è‰²{len(detected_characters) + 1}")
                    break
            
            # åˆ›å»ºæ®µè½
            segments.append({
                'text': paragraph,
                'speaker': f"è§’è‰²{segment_id % 3 + 1}" if has_dialogue else 'æ—ç™½',
                'text_type': 'dialogue' if has_dialogue else 'narration',
                'confidence': 0.8 if has_dialogue else 0.9,
                'detection_method': 'simple_local'
            })
            segment_id += 1
        
        # ç¡®ä¿è‡³å°‘æœ‰æ—ç™½è§’è‰²
        if 'æ—ç™½' not in detected_characters:
            detected_characters.add('æ—ç™½')
        
        # æ„å»ºè§’è‰²åˆ—è¡¨
        character_list = []
        for i, char_name in enumerate(detected_characters):
            character_list.append({
                'name': char_name,
                'confidence': 0.8,
                'source': 'simple_local',
                'recommended_config': {
                    'gender': 'female' if i % 2 == 0 else 'male',
                    'personality': 'gentle' if char_name == 'æ—ç™½' else 'normal'
                }
            })
        
        return {
            'segments': segments,
            'detected_characters': character_list,
            'analysis_metadata': {
                'total_segments': len(segments),
                'total_characters': len(character_list),
                'processing_mode': 'simple_local',
                'method': 'rule_based'
            }
        }
    
    async def _analyze_chapter_distributed(self, chapter_content: str, chapter_info: Dict) -> Dict:
        """åˆ†å¸ƒå¼åˆ†æç« èŠ‚"""
        
        # 1. æ™ºèƒ½åˆ†å—
        chunks = self.chunker.chunk_chapter(chapter_content)
        logger.info(f"ç« èŠ‚åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªåˆ†å—")
        
        # 2. å¹¶è¡Œåˆ†æå¤šä¸ªåˆ†å—
        chunk_results = await self._analyze_chunks_parallel(chunks, chapter_info)
        
        # 3. åˆå¹¶åˆ†æç»“æœ
        merged_result = await self._merge_chunk_results(chunk_results, chapter_info)
        
        return merged_result
    
    async def _analyze_chunks_parallel(self, chunks: List[Dict], chapter_info: Dict) -> List[Dict]:
        """å¹¶è¡Œåˆ†æå¤šä¸ªåˆ†å—"""
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(3)  # æœ€å¤§å¹¶å‘æ•°
        
        async def analyze_chunk_with_semaphore(chunk, index):
            async with semaphore:
                chunk_info = {
                    **chapter_info,
                    "chunk_index": index,
                    "total_chunks": len(chunks),
                    "is_chunk": True
                }
                logger.info(f"å¼€å§‹åˆ†æåˆ†å— {index + 1}/{len(chunks)}")
                
                try:
                    result = await self.ollama_detector.analyze_text(chunk["content"], chunk_info)
                    logger.info(f"åˆ†å— {index + 1} åˆ†æå®Œæˆ")
                    return result
                except Exception as e:
                    logger.error(f"åˆ†å— {index + 1} åˆ†æå¤±è´¥: {str(e)}")
                    return self._create_fallback_result(chunk, chunk_info)
        
        # å¹¶è¡Œæ‰§è¡Œåˆ†æ
        tasks = [
            analyze_chunk_with_semaphore(chunk, i) 
            for i, chunk in enumerate(chunks)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"åˆ†å— {i} åˆ†æå¼‚å¸¸: {result}")
                valid_results.append(self._create_fallback_result(chunks[i], chapter_info))
            else:
                valid_results.append(result)
        
        return valid_results
    
    async def _merge_chunk_results(self, chunk_results: List[Dict], chapter_info: Dict) -> Dict:
        """åˆå¹¶åˆ†å—åˆ†æç»“æœ"""
        
        # åˆå¹¶æ‰€æœ‰æ®µè½
        all_segments = []
        segment_order = 1
        
        for result in chunk_results:
            for segment in result.get('segments', []):
                segment['order'] = segment_order
                all_segments.append(segment)
                segment_order += 1
        
        # åˆå¹¶è§’è‰²ä¿¡æ¯ï¼ˆå»é‡ï¼‰
        all_characters = {}
        for result in chunk_results:
            for character in result.get('detected_characters', []):
                char_name = character['name']
                if char_name not in all_characters:
                    all_characters[char_name] = character
                else:
                    # åˆå¹¶è§’è‰²ä¿¡æ¯ï¼ˆå–ç½®ä¿¡åº¦æ›´é«˜çš„ï¼‰
                    existing = all_characters[char_name]
                    if character.get('confidence', 0) > existing.get('confidence', 0):
                        all_characters[char_name] = character
        
        return {
            'segments': all_segments,
            'detected_characters': list(all_characters.values()),
            'analysis_metadata': {
                'total_chunks': len(chunk_results),
                'total_segments': len(all_segments),
                'total_characters': len(all_characters),
                'processing_mode': 'distributed'
            }
        }
    
    def _create_fallback_result(self, chunk: Dict, chapter_info: Dict) -> Dict:
        """åˆ›å»ºé™çº§ç»“æœ"""
        return {
            'segments': [{
                'text': chunk['content'],
                'speaker': 'æ—ç™½',
                'text_type': 'narration',
                'confidence': 0.5,
                'detection_method': 'fallback'
            }],
            'detected_characters': [{
                'name': 'æ—ç™½',
                'confidence': 1.0,
                'source': 'fallback',
                'recommended_config': {
                    'gender': 'neutral',
                    'personality': 'calm'
                }
            }],
            'processing_stats': {
                'total_segments': 1,
                'characters_found': 1,
                'analysis_method': 'fallback'
            }
        }
    
    def _ensure_narrator_character(self, detected_characters: List[Dict]) -> List[Dict]:
        """ç¡®ä¿è§’è‰²åˆ—è¡¨ä¸­åŒ…å«æ—ç™½è§’è‰²"""
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ—ç™½è§’è‰²
        has_narrator = any(char['name'] == 'æ—ç™½' for char in detected_characters)
        
        if not has_narrator:
            # è‡ªåŠ¨æ·»åŠ æ—ç™½è§’è‰²
            narrator_character = {
                'name': 'æ—ç™½',
                'confidence': 1.0,
                'recommended_config': {
                    'gender': 'neutral',
                    'age_range': 'adult',
                    'personality': 'calm',
                    'voice_style': 'professional'
                },
                'source': 'system_generated',
                'description': 'ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ çš„æ—ç™½è§’è‰²ï¼Œç”¨äºå™è¿°æ€§æ–‡æœ¬'
            }
            detected_characters.append(narrator_character)
            logger.info("ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ æ—ç™½è§’è‰²")
        
        return detected_characters
    
    def _adapt_to_synthesis_format(
        self, 
        analysis_result: Dict, 
        voice_mapping: Dict[str, int],
        available_voices: List[Dict] = None
    ) -> Dict:
        """é€‚é…ä¸ºç°æœ‰åˆæˆç³»ç»Ÿçš„JSONæ ¼å¼"""
        
        # æ„å»ºvoice_idåˆ°voice_nameçš„æ˜ å°„
        voice_id_to_name = {}
        if available_voices:
            voice_id_to_name = {v['id']: v['name'] for v in available_voices}
        
        # æ ¼å¼åŒ–è§’è‰²ä¿¡æ¯
        characters = []
        for character in analysis_result['detected_characters']:
            char_name = character['name']
            voice_id = voice_mapping.get(char_name)
            if voice_id:
                characters.append({
                    "name": char_name,
                    "voice_id": voice_id,
                    "voice_name": voice_id_to_name.get(voice_id, f"Voice_{voice_id}")
                })
        
        # æ ¼å¼åŒ–åˆæˆè®¡åˆ’
        synthesis_plan = []
        segment_id = 1
        
        for segment in analysis_result['segments']:
            # è·å–è¯­éŸ³ä¿¡æ¯
            voice_id = voice_mapping.get(segment['speaker'])
            voice_name = voice_id_to_name.get(voice_id, f"Voice_{voice_id}") if voice_id else "æœªåˆ†é…"
            
            synthesis_plan.append({
                "segment_id": segment_id,
                "text": segment['text'],  # ğŸ”’ åŸæ–‡ä¸å˜
                "speaker": segment['speaker'],
                "voice_id": voice_id,
                "voice_name": voice_name,
                "parameters": {
                    "timeStep": 32,  # é»˜è®¤å‚æ•°
                    "pWeight": 1.4,
                    "tWeight": 3.0
                }
            })
            segment_id += 1
        
        # å®Œå…¨åŒ¹é…ç°æœ‰ç³»ç»Ÿæ ¼å¼
        return {
            "project_info": {
                "novel_type": "æ™ºèƒ½æ£€æµ‹",
                "analysis_time": datetime.now().isoformat(),
                "total_segments": len(synthesis_plan),
                "ai_model": "optimized-smart-analysis",
                "detected_characters": len(characters)
            },
            "synthesis_plan": synthesis_plan,
            "characters": characters
        }
    
    def _clean_and_normalize(self, text: str) -> str:
        """æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬"""
        # åŸºæœ¬æ¸…ç†ï¼Œä¿æŒåŸæ–‡å®Œæ•´æ€§
        text = text.strip()
        # ç»Ÿä¸€æ¢è¡Œç¬¦
        text = re.sub(r'\r\n', '\n', text)
        # ç§»é™¤å¤šä½™ç©ºè¡Œï¼ˆä¿ç•™æ®µè½ç»“æ„ï¼‰
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬tokenæ•°é‡"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        return int(chinese_chars * 1.5 + english_words)
    
    async def _intelligent_voice_mapping(
        self, 
        detected_characters: List[Dict], 
        user_preferences: Dict = None
    ) -> Dict[str, int]:
        """æ™ºèƒ½è¯­éŸ³åŒ¹é…"""
        
        # è·å–å¯ç”¨è¯­éŸ³
        available_voices = await self._get_available_voices()
        voice_mapping = {}
        
        # ç®€å•çš„åŒ¹é…é€»è¾‘ï¼ˆå¯ä»¥åç»­ä¼˜åŒ–ï¼‰
        for i, character in enumerate(detected_characters):
            char_name = character['name']
            
            # ä¸ºæ—ç™½è§’è‰²ç‰¹æ®Šå¤„ç†
            if char_name == 'æ—ç™½':
                narrator_voice = self._get_narrator_voice_mapping(available_voices)
                if narrator_voice:
                    voice_mapping[char_name] = narrator_voice
                continue
            
            # å…¶ä»–è§’è‰²ç®€å•åˆ†é…
            if i < len(available_voices):
                voice_mapping[char_name] = available_voices[i]['id']
        
        return voice_mapping
    
    def _get_narrator_voice_mapping(self, available_voices: List[Dict]) -> Optional[int]:
        """ä¸ºæ—ç™½è§’è‰²é€‰æ‹©åˆé€‚çš„è¯­éŸ³"""
        
        # ä¼˜å…ˆé€‰æ‹©æ ‡è®°ä¸º"æ—ç™½"æˆ–"ä¸­æ€§"çš„è¯­éŸ³
        for voice in available_voices:
            if voice.get('type') == 'neutral' or 'æ—ç™½' in voice.get('name', ''):
                return voice.get('id')
        
        # å…¶æ¬¡é€‰æ‹©å¥³æ€§æ¸©å’Œå£°éŸ³
        for voice in available_voices:
            if voice.get('type') == 'female' and 'æ¸©æŸ”' in voice.get('name', ''):
                return voice.get('id')
        
        # æœ€åé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨å£°éŸ³
        if available_voices:
            return available_voices[0].get('id')
        
        return None
    
    async def _get_available_voices(self) -> List[Dict]:
        """è·å–å¯ç”¨è¯­éŸ³åˆ—è¡¨"""
        try:
            from app.models import VoiceProfile
            voices = self.db.query(VoiceProfile).filter(VoiceProfile.status == 'active').all()
            return [
                {
                    'id': voice.id,
                    'name': voice.name,
                    'voice_type': voice.type,
                    'description': voice.description or ""
                }
                for voice in voices
            ]
        except Exception as e:
            logger.error(f"è·å–å¯ç”¨è¯­éŸ³å¤±è´¥: {str(e)}")
            return []

    async def get_content_stats(self, chapter_id: int, db: Session) -> Dict:
        """
        è·å–ç« èŠ‚å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            from app.models import BookChapter
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ValueError("ç« èŠ‚ä¸å­˜åœ¨")
            
            # åŸºæœ¬ç»Ÿè®¡
            content = chapter.content or ""
            word_count = len(content.replace(' ', '').replace('\n', ''))
            
            # ä½¿ç”¨ChapterChunkerè¿›è¡Œåˆ†å—åˆ†æ
            chunker = ChapterChunker()
            chunks = chunker.chunk_chapter(content)
            
            # ä¼°ç®—å¤„ç†æ—¶é—´å’Œå»ºè®®
            estimated_time = len(chunks) * 2  # æ¯ä¸ªchunkå¤§çº¦2ç§’
            processing_recommendation = "fast" if len(chunks) <= 5 else "detailed" if len(chunks) <= 15 else "distributed"
            
            return {
                "chapter_id": chapter_id,
                "chapter_title": chapter.chapter_title,
                "word_count": word_count,
                "chunk_count": len(chunks),
                "estimated_processing_time": estimated_time,
                "processing_recommendation": processing_recommendation,
                "content_preview": content[:200] + "..." if len(content) > 200 else content
            }
            
        except Exception as e:
            logger.error(f"è·å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {str(e)}")
            raise

    async def get_synthesis_preview(self, chapter_id: int, db: Session) -> Dict:
        """
        è·å–ç« èŠ‚è¯­éŸ³åˆæˆé¢„è§ˆ
        """
        try:
            from app.models import BookChapter
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ValueError("ç« èŠ‚ä¸å­˜åœ¨")
            
            content = chapter.content or ""
            
            # å¿«é€Ÿè§’è‰²æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            import re
            dialogue_patterns = [
                r'"([^"]*)"',  # åŒå¼•å·å¯¹è¯
                r'"([^"]*)"',  # ä¸­æ–‡åŒå¼•å·
                r'ã€Œ([^ã€]*)ã€',  # æ—¥å¼å¼•å·
            ]
            
            dialogues = []
            for pattern in dialogue_patterns:
                matches = re.findall(pattern, content)
                dialogues.extend(matches)
            
            # ä¼°ç®—è§’è‰²æ•°é‡ï¼ˆç®€åŒ–ï¼‰
            estimated_characters = min(len(set(dialogues[:10])), 8) if dialogues else 1
            
            # ä½¿ç”¨ChapterChunkerè¿›è¡Œåˆ†å—
            chunker = ChapterChunker()
            chunks = chunker.chunk_chapter(content)
            
            return {
                "chapter_id": chapter_id,
                "chapter_title": chapter.chapter_title,
                "estimated_characters": estimated_characters,
                "dialogue_count": len(dialogues),
                "chunk_count": len(chunks),
                "sample_dialogues": dialogues[:5],  # å‰5ä¸ªå¯¹è¯ç¤ºä¾‹
                "processing_complexity": "simple" if len(chunks) <= 3 else "moderate" if len(chunks) <= 10 else "complex"
            }
            
        except Exception as e:
            logger.error(f"è·å–åˆæˆé¢„è§ˆå¤±è´¥: {str(e)}")
            raise

    async def get_preparation_status(self, chapter_id: int, db: Session) -> Dict:
        """
        è·å–ç« èŠ‚å‡†å¤‡çŠ¶æ€
        """
        try:
            from app.models import BookChapter
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ValueError("ç« èŠ‚ä¸å­˜åœ¨")
            
            # æ£€æŸ¥åˆ†æçŠ¶æ€
            is_analyzed = getattr(chapter, 'analysis_status', 'pending') == 'completed'
            is_synthesis_ready = getattr(chapter, 'synthesis_status', 'pending') in ['ready', 'completed']
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³çš„åˆæˆé…ç½®ï¼ˆä»AnalysisResultè¡¨ä¸­æŸ¥è¯¢ï¼‰
            try:
                from ..models import AnalysisResult
                latest_result = self.db.query(AnalysisResult).filter(
                    AnalysisResult.chapter_id == chapter_id,
                    AnalysisResult.status == 'completed'
                ).order_by(AnalysisResult.created_at.desc()).first()
                
                has_synthesis_config = bool(latest_result)
                has_complete_synthesis_config = False
                
                if latest_result:
                    # æ£€æŸ¥æ˜¯å¦æœ‰synthesis_planæˆ–final_config
                    has_synthesis_plan = bool(latest_result.synthesis_plan)
                    has_final_config = bool(latest_result.final_config)
                    
                    # å¦‚æœæœ‰final_configï¼Œæ£€æŸ¥å…¶ä¸­æ˜¯å¦åŒ…å«synthesis_json
                    if has_final_config and latest_result.final_config:
                        try:
                            final_config = latest_result.final_config
                            if isinstance(final_config, str):
                                import json
                                final_config = json.loads(final_config)
                            has_complete_synthesis_config = bool(final_config.get('synthesis_json'))
                        except:
                            has_complete_synthesis_config = has_synthesis_plan
                    else:
                        has_complete_synthesis_config = has_synthesis_plan
                        
            except ImportError:
                # å¦‚æœæ²¡æœ‰AnalysisResultæ¨¡å‹ï¼Œå›é€€åˆ°ç« èŠ‚å­—æ®µæ£€æŸ¥
                analysis_result = getattr(chapter, 'analysis_result', None)
                has_synthesis_config = bool(analysis_result)
                has_complete_synthesis_config = False
                
                if has_synthesis_config and analysis_result:
                    try:
                        import json
                        if isinstance(analysis_result, str):
                            result_data = json.loads(analysis_result)
                        else:
                            result_data = analysis_result
                        
                        has_complete_synthesis_config = bool(result_data.get('synthesis_json'))
                    except:
                        has_complete_synthesis_config = False
            
            return {
                "chapter_id": chapter_id,
                "chapter_title": chapter.chapter_title,
                "is_analyzed": is_analyzed,
                "is_synthesis_ready": is_synthesis_ready,
                "has_synthesis_config": has_synthesis_config,
                "has_complete_synthesis_config": has_complete_synthesis_config,
                "analysis_status": getattr(chapter, 'analysis_status', 'pending'),
                "synthesis_status": getattr(chapter, 'synthesis_status', 'pending'),
                "last_updated": chapter.updated_at.isoformat() if getattr(chapter, 'updated_at', None) else None,
                "preparation_complete": is_analyzed and is_synthesis_ready and has_complete_synthesis_config
            }
            
        except Exception as e:
            logger.error(f"è·å–å‡†å¤‡çŠ¶æ€å¤±è´¥: {str(e)}")
            raise 