"""
å°è¯´ç« èŠ‚åˆæˆè¯­éŸ³å‰å†…å®¹å‡†å¤‡æœåŠ¡
é‡æ„åçš„ç²¾ç®€ç‰ˆæœ¬ - ä¸»è¦è´Ÿè´£æµç¨‹æ§åˆ¶å’Œåè°ƒå„ä¸ªä¸“é—¨æœåŠ¡
"""

import asyncio
import re
import json
import logging
import hashlib
import os
from typing import List, Dict, Any, Optional
from datetime import datetime
from sqlalchemy.orm import Session

from ..models import BookChapter, VoiceProfile
from ..exceptions import ServiceException
from .chapter_chunker import ChapterChunker
from .ai_tts_optimizer import AITTSOptimizer
from .intelligent_voice_mapper import IntelligentVoiceMapper

logger = logging.getLogger(__name__)


class ContentPreparationService:
    """å†…å®¹å‡†å¤‡æœåŠ¡ä¸»æ§åˆ¶å™¨ - é‡æ„åçš„ç²¾ç®€ç‰ˆæœ¬"""
    
    def __init__(self, db: Session):
        self.db = db
        self.chunker = ChapterChunker()
        self.tts_optimizer = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.voice_mapper = IntelligentVoiceMapper(db)
        self.ollama_detector = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    async def prepare_chapter_for_synthesis(
        self, 
        chapter_id: int,
        user_preferences: Dict = None
    ) -> Dict:
        """å‡†å¤‡ç« èŠ‚ç”¨äºè¯­éŸ³åˆæˆçš„å®Œæ•´æµç¨‹"""
        
        try:
            # 1. è·å–ç« èŠ‚æ•°æ®
            chapter = self.db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ServiceException(f"ç« èŠ‚ {chapter_id} ä¸å­˜åœ¨")
            
            logger.info(f"å¼€å§‹å‡†å¤‡ç« èŠ‚ {chapter_id}: {chapter.chapter_title}")
            
            # 2. æ›´æ–°ç« èŠ‚çŠ¶æ€ä¸ºå¤„ç†ä¸­
            chapter.analysis_status = 'analyzing'
            self.db.commit()
            
            # 3. é¢„å¤„ç†æ–‡æœ¬
            cleaned_text = self._clean_and_normalize(chapter.content)
            
            # 4. æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå†³å®šå¤„ç†ç­–ç•¥
            estimated_tokens = self._estimate_tokens(cleaned_text)
            processing_mode = "distributed" if estimated_tokens > 3000 else "single"
            
            logger.info(f"ç« èŠ‚å†…å®¹é•¿åº¦: {len(cleaned_text)} å­—ç¬¦, ä¼°ç®— {estimated_tokens} tokens, ä½¿ç”¨ {processing_mode} æ¨¡å¼")
            
            # 5. æ‰§è¡ŒAIåˆ†æ
            chapter_info = {
                "chapter_id": chapter.id,
                "chapter_title": chapter.chapter_title,
                "chapter_number": chapter.chapter_number,
                "processing_mode": processing_mode
            }
            
            # ğŸ”§ æ–°å¢ï¼šæ”¯æŒTTSä¼˜åŒ–æ¨¡å¼
            tts_optimization_mode = user_preferences and user_preferences.get("tts_optimization", "balanced")
            
            logger.info(f"ğŸ” å¤„ç†æ¨¡å¼å†³ç­–: processing_mode={processing_mode}, tts_optimization={tts_optimization_mode}")
            
            # ğŸš€ å¼ºåˆ¶ä½¿ç”¨å®Œæ•´AIåˆ†æï¼Œåˆ é™¤åƒåœ¾ç®€åŒ–æ¨¡å¼
            logger.info("ğŸ¤– ä½¿ç”¨å®Œæ•´AIæ™ºèƒ½åˆ†ææ¨¡å¼")
            try:
                # å»¶è¿Ÿåˆå§‹åŒ–OllamaCharacterDetector
                if self.ollama_detector is None:
                    logger.info("ğŸ“¦ åˆå§‹åŒ–OllamaCharacterDetector...")
                    try:
                        from ..detectors.ollama_character_detector import OllamaCharacterDetector
                        self.ollama_detector = OllamaCharacterDetector()
                        logger.info("âœ… OllamaCharacterDetectoråˆå§‹åŒ–æˆåŠŸ")
                    except ImportError as e:
                        logger.error(f"âŒ æ— æ³•å¯¼å…¥OllamaCharacterDetector: {str(e)}")
                        raise ServiceException(f"AIåˆ†æç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                
                # æ‰§è¡ŒAIåˆ†æ
                logger.info(f"ğŸ”„ å¼€å§‹æ™ºèƒ½åˆ†æï¼Œæ¨¡å¼: {processing_mode}")
                if processing_mode == "single":
                    analysis_result = await self.ollama_detector.analyze_text(cleaned_text, chapter_info)
                    logger.info("âœ… æ™ºèƒ½å•å—åˆ†æå®Œæˆ")
                else:
                    analysis_result = await self._analyze_chapter_distributed(cleaned_text, chapter_info)
                    logger.info("âœ… æ™ºèƒ½åˆ†å¸ƒå¼åˆ†æå®Œæˆ")
                    
            except Exception as e:
                logger.error(f"âŒ AIæ™ºèƒ½åˆ†æå¤±è´¥: {str(e)}")
                raise ServiceException(f"AIæ™ºèƒ½åˆ†æå¤±è´¥: {str(e)}")
            
            # 6. ç¡®ä¿æœ‰æ—ç™½è§’è‰²
            detected_characters = analysis_result.get('detected_characters', [])
            detected_characters = self._ensure_narrator_character(detected_characters)
            
            # 7. æ™ºèƒ½è¯­éŸ³æ˜ å°„
            voice_mapping = await self.voice_mapper.intelligent_voice_mapping(detected_characters, user_preferences)
            
            # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥voice mappingç»“æœ
            logger.info(f"ğŸ”Š Voice mappingç»“æœ: {voice_mapping}")
            logger.info(f"ğŸ­ æ‰€æœ‰è§’è‰²: {[char.get('name') for char in detected_characters]}")
            for char_name, voice_id in voice_mapping.items():
                logger.info(f"ğŸµ {char_name} -> voice_id: {voice_id}")
            
            # ğŸ”§ æ£€æŸ¥æ—ç™½è§’è‰²çš„voice mappingçŠ¶æ€
            if 'æ—ç™½' in voice_mapping:
                logger.info(f"âœ… æ—ç™½è§’è‰²å·²åˆ†é…voice_id: {voice_mapping['æ—ç™½']}")
            else:
                logger.warning("âŒ æ—ç™½è§’è‰²æœªåˆ†é…voice_idï¼Œéœ€è¦ç”¨æˆ·æ‰‹åŠ¨åˆ†é…")
            
            # 8. è½¬æ¢ä¸ºåˆæˆæ ¼å¼ï¼ˆåº”ç”¨TTSä¼˜åŒ–é…ç½®ï¼‰
            # è®¾ç½®å½“å‰ç« èŠ‚IDï¼Œç”¨äºå…³è”è§’è‰²é…éŸ³åº“
            self.current_chapter_id = chapter_id
            synthesis_json = await self._adapt_to_synthesis_format(
                analysis_result, 
                voice_mapping,
                tts_optimization_mode=tts_optimization_mode
            )
            
            # ğŸ”¥ æ–°å¢ï¼šæœ€ç»ˆå®Œæ•´æ€§æ ¡éªŒ - ç¡®ä¿synthesis_planè¦†ç›–äº†åŸæ–‡æ‰€æœ‰å†…å®¹
            final_completeness = self._validate_synthesis_completeness(cleaned_text, synthesis_json)
            if not final_completeness:
                logger.warning("æœ€ç»ˆåˆæˆè®¡åˆ’å®Œæ•´æ€§æ ¡éªŒå¤±è´¥ï¼Œå¯èƒ½å­˜åœ¨å†…å®¹ä¸¢å¤±")
                # è®°å½•è¯¦ç»†çš„å·®å¼‚ä¿¡æ¯ç”¨äºè°ƒè¯•
                self._log_completeness_details(cleaned_text, synthesis_json)
            
            # 9. ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
            preparation_result = await self._save_preparation_result(
                chapter_id=chapter_id,
                analysis_result=analysis_result,
                synthesis_json=synthesis_json,
                voice_mapping=voice_mapping,
                processing_info={
                    "mode": processing_mode,
                    "total_segments": len(analysis_result.get('segments', [])),
                    "characters_found": len(detected_characters),
                    "estimated_tokens": estimated_tokens,
                    "analysis_method": analysis_result.get('analysis_metadata', {}).get('method', 'ai_enhanced'),
                    "final_completeness_validated": final_completeness  # ğŸ”¥ æ–°å¢ï¼šè®°å½•æœ€ç»ˆå®Œæ•´æ€§æ ¡éªŒç»“æœ
                }
            )
            
            # 10. æ›´æ–°ç« èŠ‚çŠ¶æ€ä¸ºå®Œæˆ
            chapter.analysis_status = 'completed'
            chapter.synthesis_status = 'ready'
            self.db.commit()
            
            logger.info(f"ç« èŠ‚ {chapter_id} æ™ºèƒ½å‡†å¤‡å®Œæˆï¼Œå…±è¯†åˆ« {len(detected_characters)} ä¸ªè§’è‰²")
            
            # 10.5. æ›´æ–°ä¹¦ç±è§’è‰²æ±‡æ€»ï¼ˆé«˜æ€§èƒ½ï¼‰
            try:
                from ..models import Book
                book = self.db.query(Book).filter(Book.id == chapter.book_id).first()
                if book:
                    # ğŸ­ æå–è§’è‰²ä¿¡æ¯ç”¨äºæ±‡æ€» - ä¿®å¤æ•°æ®æ ¼å¼
                    characters_for_summary = []
                    for char in detected_characters:
                        char_name = char.get('name', '').strip()
                        if not char_name:
                            continue
                            
                        # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿æ—ç™½è§’è‰²çš„æ•°æ®å®Œæ•´æ€§
                        char_data = {
                            'name': char_name,
                            'gender': char.get('gender', char.get('recommended_config', {}).get('gender', '')),
                            'age': char.get('age', char.get('recommended_config', {}).get('age_range', '')),
                            'personality': char.get('personality', char.get('recommended_config', {}).get('personality', '')),
                            'description': char.get('description', f"æ™ºèƒ½æ£€æµ‹åˆ°çš„{char_name}è§’è‰²"),
                            'appearances': 1  # æœ¬ç« èŠ‚å‡ºç°1æ¬¡
                        }
                        characters_for_summary.append(char_data)
                        
                        logger.debug(f"ğŸ“ å‡†å¤‡æ›´æ–°è§’è‰²æ±‡æ€»: {char_name} -> {char_data}")
                    
                    # æ›´æ–°ä¹¦ç±è§’è‰²æ±‡æ€»
                    logger.info(f"ğŸ­ å¼€å§‹æ›´æ–°ä¹¦ç± {book.id} çš„è§’è‰²æ±‡æ€»ï¼Œæœ¬ç« è¯†åˆ«è§’è‰²: {[c['name'] for c in characters_for_summary]}")
                    book.update_character_summary(characters_for_summary, chapter_id)
                    self.db.commit()
                    
                    # ğŸ“Š éªŒè¯æ›´æ–°ç»“æœ
                    updated_summary = book.get_character_summary()
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ™ºèƒ½åˆ†æå®Œæˆåè‡ªåŠ¨åŒæ­¥å·²æœ‰çš„è§’è‰²é…ç½®
                    if updated_summary and 'voice_mappings' in updated_summary:
                        voice_mappings = updated_summary['voice_mappings']
                        if voice_mappings:
                            logger.info(f"ğŸ”„ [æ™ºèƒ½åˆ†æååŒæ­¥] å‘ç°ä¹¦ç±å·²æœ‰è§’è‰²é…ç½®ï¼Œè‡ªåŠ¨åŒæ­¥åˆ°æ–°ç« èŠ‚: {voice_mappings}")
                            
                            # å¯¼å…¥åŒæ­¥å‡½æ•°å¹¶æ‰§è¡Œ
                            try:
                                from ..api.v1.books import _sync_character_voice_to_synthesis_plans
                                updated_chapters = await _sync_character_voice_to_synthesis_plans(
                                    book.id, voice_mappings, self.db
                                )
                                logger.info(f"âœ… [æ™ºèƒ½åˆ†æååŒæ­¥] æˆåŠŸåŒæ­¥ {updated_chapters} ä¸ªç« èŠ‚çš„è§’è‰²é…ç½®")
                            except Exception as sync_error:
                                logger.error(f"âŒ [æ™ºèƒ½åˆ†æååŒæ­¥] åŒæ­¥å¤±è´¥: {sync_error}")
                        else:
                            logger.info(f"ğŸ“‹ [æ™ºèƒ½åˆ†æååŒæ­¥] ä¹¦ç±æš‚æ— è§’è‰²é…ç½®ï¼Œè·³è¿‡åŒæ­¥")
                    else:
                        logger.info(f"ğŸ“‹ [æ™ºèƒ½åˆ†æååŒæ­¥] ä¹¦ç±è§’è‰²æ±‡æ€»æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡åŒæ­¥")
                    all_characters = [char['name'] for char in updated_summary.get('characters', [])]
                    logger.info(f"âœ… ä¹¦ç±è§’è‰²æ±‡æ€»æ›´æ–°å®Œæˆï¼Œå½“å‰æ‰€æœ‰è§’è‰²: {all_characters}")
                    
                    # ğŸ” ç‰¹åˆ«æ£€æŸ¥æ—ç™½è§’è‰²
                    narrator_exists = any(char['name'] == 'æ—ç™½' for char in updated_summary.get('characters', []))
                    if narrator_exists:
                        logger.info("ğŸ­ âœ… æ—ç™½è§’è‰²å·²æˆåŠŸæ·»åŠ åˆ°ä¹¦ç±è§’è‰²æ±‡æ€»")
                    else:
                        logger.warning("ğŸ­ âŒ æ—ç™½è§’è‰²æœªèƒ½æ·»åŠ åˆ°ä¹¦ç±è§’è‰²æ±‡æ€»ï¼Œéœ€è¦æ‰‹åŠ¨æ£€æŸ¥")
                else:
                    logger.warning(f"æœªæ‰¾åˆ°ç« èŠ‚ {chapter_id} å¯¹åº”çš„ä¹¦ç±")
            except Exception as e:
                logger.error(f"æ›´æ–°ä¹¦ç±è§’è‰²æ±‡æ€»å¤±è´¥: {str(e)}")
                import traceback
                logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
            
            # 11. è¿”å›ç»“æœ
            return {
                "synthesis_json": synthesis_json,
                "processing_info": {
                    "mode": processing_mode,
                    "total_segments": len(analysis_result.get('segments', [])),
                    "characters_found": len(detected_characters),
                    "estimated_tokens": estimated_tokens,
                    "narrator_added": any(char.get('name') == 'æ—ç™½' for char in detected_characters),
                    "voice_mapping": voice_mapping,
                    "saved_to_database": True,
                    "preparation_id": preparation_result.get("id")
                }
            }
            
        except Exception as e:
            # æ›´æ–°ç« èŠ‚çŠ¶æ€ä¸ºå¤±è´¥
            if chapter:
                chapter.analysis_status = 'failed'
                self.db.commit()
            
            logger.error(f"ç« èŠ‚ {chapter_id} æ™ºèƒ½å‡†å¤‡å¤±è´¥: {str(e)}")
            raise ServiceException(f"æ™ºèƒ½å‡†å¤‡å¤±è´¥: {str(e)}")

    async def _save_preparation_result(
        self,
        chapter_id: int,
        analysis_result: Dict,
        synthesis_json: Dict,
        voice_mapping: Dict,
        processing_info: Dict
    ) -> Dict:
        """ä¿å­˜æ™ºèƒ½å‡†å¤‡ç»“æœåˆ°æ•°æ®åº“"""
        
        try:
            # è®¡ç®—å†…å®¹å“ˆå¸Œç”¨äºå»é‡
            content_hash = hashlib.md5(
                json.dumps(analysis_result, sort_keys=True).encode('utf-8')
            ).hexdigest()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¼å…¥analysis_resultsæ¨¡å‹
            try:
                from ..models import AnalysisResult
            except ImportError:
                # å¦‚æœæ²¡æœ‰AnalysisResultæ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„å­˜å‚¨æ–¹æ¡ˆ
                logger.warning("AnalysisResultæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç« èŠ‚å­—æ®µå­˜å‚¨ç»“æœ")
                
                # å°†ç»“æœå­˜å‚¨åœ¨ç« èŠ‚çš„å­—æ®µä¸­
                chapter = self.db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
                if chapter:
                    # åˆ›å»ºå®Œæ•´çš„ç»“æœæ•°æ®
                    full_result = {
                        "analysis_result": analysis_result,
                        "synthesis_json": synthesis_json,
                        "voice_mapping": voice_mapping,
                        "processing_info": processing_info,
                        "content_hash": content_hash,
                        "created_at": datetime.utcnow().isoformat(),
                        "version": 1
                    }
                    
                    # å¦‚æœç« èŠ‚æœ‰analysis_resultså­—æ®µï¼Œå­˜å‚¨åœ¨é‚£é‡Œ
                    if chapter.analysis_results:
                        chapter.analysis_results[0].original_analysis = json.dumps(full_result, ensure_ascii=False)
                    
                    self.db.commit()
                    
                    return {
                        "id": f"chapter_{chapter_id}",
                        "storage_method": "chapter_field",
                        "content_hash": content_hash
                    }
            
            # å¦‚æœæœ‰AnalysisResultæ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†å­˜å‚¨
            # ç”±äºAnalysisResultçš„session_idæ˜¯å¿…éœ€çš„ï¼Œä½†æˆ‘ä»¬çš„æ™ºèƒ½å‡†å¤‡ä¸å±äºç‰¹å®šé¡¹ç›®ä¼šè¯
            # æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„session_idæˆ–ä½¿ç”¨å…¶ä»–å­˜å‚¨æ–¹å¼
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒç« èŠ‚çš„æ™ºèƒ½å‡†å¤‡ç»“æœ
            existing_result = self.db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter_id,
                AnalysisResult.status == 'completed'
            ).order_by(AnalysisResult.created_at.desc()).first()
            
            if existing_result:
                logger.info(f"ç« èŠ‚ {chapter_id} å·²æœ‰æ™ºèƒ½å‡†å¤‡ç»“æœï¼Œæ›´æ–°ç°æœ‰è®°å½•")
                # æ›´æ–°ç°æœ‰è®°å½•
                existing_result.original_analysis = analysis_result
                existing_result.synthesis_plan = synthesis_json
                existing_result.final_config = {
                    "synthesis_json": synthesis_json,
                    "voice_mapping": voice_mapping,
                    "processing_info": processing_info
                }
                existing_result.updated_at = datetime.utcnow()
                self.db.commit()
                
                return {
                    "id": existing_result.id,
                    "storage_method": "analysis_result_updated",
                    "content_hash": content_hash
                }
            
            # æå–è§’è‰²ä¿¡æ¯
            detected_characters = []
            if synthesis_json.get('characters'):
                detected_characters = synthesis_json['characters']
            
            # åˆ›å»ºæ–°çš„åˆ†æç»“æœè®°å½•
            # æ™ºèƒ½å‡†å¤‡ä¸ä¾èµ–é¡¹ç›®sessionï¼Œsession_idè®¾ä¸ºNone
            
            new_result = AnalysisResult(
                session_id=None,  # æ™ºèƒ½å‡†å¤‡ç‹¬ç«‹äºé¡¹ç›®session
                chapter_id=chapter_id,
                original_analysis=analysis_result,
                detected_characters=detected_characters,
                synthesis_plan=synthesis_json,
                final_config={
                    "synthesis_json": synthesis_json,
                    "voice_mapping": voice_mapping,
                    "processing_info": processing_info
                },
                status='completed',
                processing_time=processing_info.get('processing_time', 0),
                confidence_score=85,  # é»˜è®¤ç½®ä¿¡åº¦
                created_at=datetime.utcnow(),
                completed_at=datetime.utcnow()
            )
            
            self.db.add(new_result)
            self.db.commit()
            self.db.refresh(new_result)
            
            logger.info(f"æ™ºèƒ½å‡†å¤‡ç»“æœå·²ä¿å­˜åˆ°AnalysisResultè¡¨ï¼ŒID: {new_result.id}")
            
            return {
                "id": new_result.id,
                "storage_method": "analysis_result_created",
                "content_hash": content_hash
            }
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ™ºèƒ½å‡†å¤‡ç»“æœå¤±è´¥: {str(e)}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
            return {
                "id": None,
                "storage_method": "failed",
                "error": str(e)
            }
    

    
    async def _analyze_chapter_distributed(self, chapter_content: str, chapter_info: Dict) -> Dict:
        """åˆ†å¸ƒå¼åˆ†æç« èŠ‚"""
        
        # 1. æ™ºèƒ½åˆ†å—
        chunks = self.chunker.chunk_chapter(chapter_content)
        logger.info(f"ç« èŠ‚åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªåˆ†å—")
        
        # 2. å¹¶è¡Œåˆ†æå¤šä¸ªåˆ†å—
        chunk_results = await self._analyze_chunks_parallel(chunks, chapter_info)
        
        # 3. åˆå¹¶åˆ†æç»“æœ
        merged_result = await self._merge_chunk_results(chunk_results, chapter_info)
        
        return merged_result
    
    async def _analyze_chunks_parallel(self, chunks: List[Dict], chapter_info: Dict) -> List[Dict]:
        """å¹¶è¡Œåˆ†æå¤šä¸ªåˆ†å—"""
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(3)  # æœ€å¤§å¹¶å‘æ•°
        
        async def analyze_chunk_with_semaphore(chunk, index):
            async with semaphore:
                chunk_info = {
                    **chapter_info,
                    "chunk_index": index,
                    "total_chunks": len(chunks),
                    "is_chunk": True
                }
                logger.info(f"å¼€å§‹åˆ†æåˆ†å— {index + 1}/{len(chunks)}")
                
                try:
                    result = await self.ollama_detector.analyze_text(chunk["content"], chunk_info)
                    logger.info(f"åˆ†å— {index + 1} åˆ†æå®Œæˆ")
                    return result
                except Exception as e:
                    logger.error(f"åˆ†å— {index + 1} AIåˆ†æå¤±è´¥: {str(e)}")
                    raise e
        
        # å¹¶è¡Œæ‰§è¡Œåˆ†æ
        tasks = [
            analyze_chunk_with_semaphore(chunk, i) 
            for i, chunk in enumerate(chunks)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ£€æŸ¥å¼‚å¸¸ç»“æœ
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"åˆ†å— {i} AIåˆ†æå¼‚å¸¸: {result}")
                raise ServiceException(f"åˆ†å¸ƒå¼AIåˆ†æå¤±è´¥ï¼Œåˆ†å— {i} é”™è¯¯: {result}")
        
        return results
    
    async def _merge_chunk_results(self, chunk_results: List[Dict], chapter_info: Dict) -> Dict:
        """åˆå¹¶åˆ†å—åˆ†æç»“æœ"""
        
        # åˆå¹¶æ‰€æœ‰æ®µè½
        all_segments = []
        segment_order = 1
        
        for result in chunk_results:
            for segment in result.get('segments', []):
                segment['order'] = segment_order
                all_segments.append(segment)
                segment_order += 1
        
        # åˆå¹¶è§’è‰²ä¿¡æ¯ï¼ˆå»é‡ï¼‰
        all_characters = {}
        for result in chunk_results:
            for character in result.get('detected_characters', []):
                char_name = character['name']
                if char_name not in all_characters:
                    all_characters[char_name] = character
                else:
                    # åˆå¹¶è§’è‰²ä¿¡æ¯ï¼ˆå–ç½®ä¿¡åº¦æ›´é«˜çš„ï¼‰
                    existing = all_characters[char_name]
                    if character.get('confidence', 0) > existing.get('confidence', 0):
                        all_characters[char_name] = character
        
        return {
            'segments': all_segments,
            'detected_characters': list(all_characters.values()),
            'analysis_metadata': {
                'total_chunks': len(chunk_results),
                'total_segments': len(all_segments),
                'total_characters': len(all_characters),
                'processing_mode': 'distributed'
            }
        }
    

    
    def _ensure_narrator_character(self, detected_characters: List[Dict]) -> List[Dict]:
        """ç¡®ä¿è§’è‰²åˆ—è¡¨ä¸­åŒ…å«æ—ç™½è§’è‰²"""
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ—ç™½è§’è‰²
        has_narrator = any(char['name'] == 'æ—ç™½' for char in detected_characters)
        
        if not has_narrator:
            # è‡ªåŠ¨æ·»åŠ æ—ç™½è§’è‰²
            narrator_character = {
                'name': 'æ—ç™½',
                'confidence': 1.0,
                'recommended_config': {
                    'gender': 'neutral',
                    'age_range': 'adult',
                    'personality': 'calm',
                    'voice_style': 'professional'
                },
                'source': 'system_generated',
                'description': 'ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ çš„æ—ç™½è§’è‰²ï¼Œç”¨äºå™è¿°æ€§æ–‡æœ¬'
            }
            detected_characters.append(narrator_character)
            logger.info("ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ æ—ç™½è§’è‰²")
        
        return detected_characters
    
    async def _ai_reanalyze_unknown_segments(self, segments: List[Dict], detected_characters: List[Dict]) -> List[Dict]:
        """ğŸ¤– AIäºŒæ¬¡åˆ†æï¼šä¸“é—¨å¤„ç†æœªçŸ¥è§’è‰²çš„segment"""
        unknown_segments = []
        known_character_names = [char.get('name', '') for char in detected_characters if char.get('name')]
        
        # æ”¶é›†æ‰€æœ‰æœªçŸ¥è§’è‰²çš„segment
        for i, segment in enumerate(segments):
            if segment.get('speaker') == 'æœªçŸ¥è§’è‰²':
                unknown_segments.append({
                    'index': i,
                    'segment': segment,
                    'context_before': segments[max(0, i-2):i],  # å‰2ä¸ªsegmentä½œä¸ºä¸Šä¸‹æ–‡
                    'context_after': segments[i+1:min(len(segments), i+3)]  # å2ä¸ªsegmentä½œä¸ºä¸Šä¸‹æ–‡
                })
        
        if not unknown_segments:
            return segments
        
        logger.info(f"ğŸ” å‘ç° {len(unknown_segments)} ä¸ªæœªçŸ¥è§’è‰²segmentï¼Œå¯åŠ¨AIäºŒæ¬¡åˆ†æ")
        
        # æ„å»ºAIäºŒæ¬¡åˆ†æprompt
        prompt = self._build_unknown_segment_analysis_prompt(unknown_segments, known_character_names)
        
        try:
            # å»¶è¿Ÿåˆå§‹åŒ–OllamaCharacterDetector
            if self.ollama_detector is None:
                from ..detectors.ollama_character_detector import OllamaCharacterDetector
                self.ollama_detector = OllamaCharacterDetector()
            
            # è°ƒç”¨AIè¿›è¡ŒäºŒæ¬¡åˆ†æ
            response = self.ollama_detector._call_ollama(prompt)
            if response:
                analysis_result = self._parse_unknown_segment_response(response)
                
                # åº”ç”¨AIåˆ†æç»“æœ
                updated_segments = segments.copy()
                for result in analysis_result:
                    segment_index = result.get('segment_index')
                    new_speaker = result.get('speaker', '').strip()
                    reasoning = result.get('reasoning', '')
                    
                    if segment_index is not None and 0 <= segment_index < len(updated_segments):
                        if new_speaker and new_speaker != 'æœªçŸ¥è§’è‰²':
                            updated_segments[segment_index]['speaker'] = new_speaker
                            logger.info(f"âœ… AIäºŒæ¬¡åˆ†æä¿®æ­£ segment_{segment_index}: '{new_speaker}' (ç†ç”±: {reasoning})")
                        else:
                            # AIä¹Ÿæ— æ³•ç¡®å®šï¼Œä¿æŒä¸ºæœªçŸ¥è§’è‰²
                            logger.warning(f"âš ï¸ AIäºŒæ¬¡åˆ†æä»æ— æ³•ç¡®å®š segment_{segment_index}ï¼Œä¿æŒä¸ºæœªçŸ¥è§’è‰²")
                
                return updated_segments
            else:
                logger.warning("AIäºŒæ¬¡åˆ†æè°ƒç”¨å¤±è´¥ï¼Œä¿æŒåŸå§‹ç»“æœ")
                return segments
                
        except Exception as e:
            logger.error(f"AIäºŒæ¬¡åˆ†æå¼‚å¸¸: {str(e)}ï¼Œä¿æŒåŸå§‹ç»“æœ")
            return segments

    def _build_unknown_segment_analysis_prompt(self, unknown_segments: List[Dict], known_characters: List[str]) -> str:
        """æ„å»ºæœªçŸ¥è§’è‰²segmentçš„AIåˆ†æprompt"""
        
        segments_text = ""
        for i, item in enumerate(unknown_segments):
            segment = item['segment']
            context_before = item['context_before']
            context_after = item['context_after']
            
            segments_text += f"\n=== æœªçŸ¥Segment {item['index']} ===\n"
            
            # ä¸Šä¸‹æ–‡
            if context_before:
                segments_text += "ã€ä¸Šæ–‡ã€‘:\n"
                for ctx in context_before:
                    segments_text += f"  {ctx.get('speaker', 'æ—ç™½')}: {ctx.get('text', '')}\n"
            
            # å½“å‰æœªçŸ¥segment
            segments_text += f"ã€å¾…åˆ†æã€‘: {segment.get('text', '')}\n"
            segments_text += f"ã€æ–‡æœ¬ç±»å‹ã€‘: {segment.get('text_type', 'unknown')}\n"
            
            # ä¸‹æ–‡
            if context_after:
                segments_text += "ã€ä¸‹æ–‡ã€‘:\n"
                for ctx in context_after:
                    segments_text += f"  {ctx.get('speaker', 'æ—ç™½')}: {ctx.get('text', '')}\n"
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ä¸­æ–‡å°è¯´è§’è‰²è¯†åˆ«ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ— æ³•ç¡®å®šè¯´è¯è€…çš„æ–‡æœ¬æ®µè½ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡é‡æ–°åˆ†æã€‚

å·²çŸ¥è§’è‰²åˆ—è¡¨ï¼š{', '.join(known_characters) if known_characters else 'æ— '}

å¾…åˆ†æçš„æœªçŸ¥è§’è‰²æ®µè½ï¼š
{segments_text}

åˆ†æè¦æ±‚ï¼š
1. **ä»”ç»†é˜…è¯»ä¸Šä¸‹æ–‡**ï¼šç†è§£å‰åæ–‡çš„é€»è¾‘å…³ç³»å’Œè¯­å¢ƒ
2. **ç†è§£æ–‡æœ¬ç±»å‹**ï¼š
   - dialogue: ç›´æ¥å¯¹è¯ï¼Œéœ€è¦ç¡®å®šå…·ä½“è¯´è¯è€…
   - inner_monologue: å¿ƒç†æ´»åŠ¨ï¼Œé€šå¸¸æ˜¯ä¸»è§’çš„å†…å¿ƒæƒ³æ³•
   - narration: å™è¿°æ–‡å­—ï¼Œé€šå¸¸æ˜¯æ—ç™½
3. **è§’è‰²ä¸€è‡´æ€§**ï¼šç¡®ä¿è§’è‰²åç§°ä¸å·²çŸ¥è§’è‰²åˆ—è¡¨ä¸€è‡´
4. **é€»è¾‘æ¨ç†**ï¼šåŸºäºå¸¸è¯†å’Œå°è¯´æƒ¯ä¾‹è¿›è¡Œåˆç†æ¨æ–­

ç‰¹æ®Šè¯†åˆ«è§„åˆ™ï¼š
- **é—´æ¥å¼•è¿°å†…å®¹**ï¼šå¦‚æœä¸Šæ–‡æåˆ°"æŸæŸå‘æ¥æ¶ˆæ¯"ã€"æŸæŸè¯´"ï¼Œé‚£ä¹ˆå¼•å·å†…å®¹é€šå¸¸æ˜¯è¯¥è§’è‰²çš„è¯
- **å¿ƒç†æ´»åŠ¨**ï¼šé€šå¸¸å±äºå½“å‰åœºæ™¯çš„ä¸»è¦è§’è‰²ï¼ˆé€šå¸¸æ˜¯ä¸»è§’ï¼‰
- **å¯¹è¯å†…å®¹**ï¼šéœ€è¦æ ¹æ®åœºæ™¯å’Œä¸Šä¸‹æ–‡ç¡®å®šè¯´è¯è€…
- **å™è¿°æ–‡å­—**ï¼šæè¿°ç¯å¢ƒã€åŠ¨ä½œã€å£°éŸ³ç­‰çš„æ–‡å­—é€šå¸¸æ˜¯æ—ç™½
- **æ— æ³•ç¡®å®šæ—¶**ï¼šä¼˜å…ˆé€‰æ‹©"æ—ç™½"è€Œéä¿æŒ"æœªçŸ¥è§’è‰²"

é‡è¦æç¤ºï¼š
- å¦‚æœæ–‡æœ¬æ˜¯æ¶ˆæ¯å†…å®¹ã€ç”µè¯å†…å®¹ã€ä¿¡ä»¶å†…å®¹ç­‰ï¼Œè¯´è¯è€…åº”è¯¥æ˜¯æ¶ˆæ¯çš„å‘é€è€…
- å¦‚æœæ–‡æœ¬æ˜¯å¿ƒç†æ´»åŠ¨ï¼Œè¯´è¯è€…é€šå¸¸æ˜¯å½“å‰åœºæ™¯çš„ä¸»è§’
- å¦‚æœæ–‡æœ¬æ˜¯çº¯å™è¿°ï¼Œè¯´è¯è€…åº”è¯¥æ˜¯"æ—ç™½"

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
  "analysis_results": [
    {{
      "segment_index": æ®µè½åœ¨åŸåˆ—è¡¨ä¸­çš„ç´¢å¼•,
      "speaker": "ç¡®å®šçš„è¯´è¯è€…åç§°",
      "reasoning": "è¯¦ç»†çš„åˆ†æç†ç”±å’Œä¾æ®",
      "confidence": 0.8
    }}
  ]
}}

åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š"""
        
        return prompt

    def _parse_unknown_segment_response(self, response: str) -> List[Dict]:
        """è§£æAIäºŒæ¬¡åˆ†æçš„å“åº”"""
        try:
            import json
            
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                data = json.loads(json_str)
                
                return data.get('analysis_results', [])
            else:
                logger.error("AIäºŒæ¬¡åˆ†æå“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆJSON")
                return []
                
        except Exception as e:
            logger.error(f"è§£æAIäºŒæ¬¡åˆ†æå“åº”å¤±è´¥: {str(e)}")
            return []

    async def _adapt_to_synthesis_format(
        self, 
        analysis_result: Dict, 
        voice_mapping: Dict[str, int],
        available_voices: List[Dict] = None,
        tts_optimization_mode: str = "balanced"
    ) -> Dict:
        """è½¬æ¢åˆ†æç»“æœä¸ºè¯­éŸ³åˆæˆæ ¼å¼"""
        segments = analysis_result.get('segments', [])
        detected_characters = analysis_result.get('detected_characters', [])
        
        logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ä¸ºåˆæˆæ ¼å¼ï¼Œå…± {len(segments)} ä¸ªsegment")
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå…³è”è§’è‰²é…éŸ³åº“
        # è·å–å½“å‰ç« èŠ‚æ‰€å±çš„ä¹¦ç±ï¼Œå¹¶æŸ¥æ‰¾è§’è‰²é…éŸ³åº“ä¸­çš„è§’è‰²
        chapter_id = getattr(self, 'current_chapter_id', None)
        book_id = None
        character_library = {}
        
        logger.info(f"ğŸ”¥ğŸ”¥ğŸ”¥ [DEBUG] å¼€å§‹è§’è‰²é…éŸ³åº“å…³è”æ£€æŸ¥ï¼Œç« èŠ‚ID: {chapter_id}")
        
        if chapter_id:
            try:
                from ..models import BookChapter, Character
                chapter = self.db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
                if chapter:
                    book_id = chapter.book_id
                    logger.info(f"ğŸ”¥ğŸ”¥ğŸ”¥ [DEBUG] æ‰¾åˆ°ç« èŠ‚{chapter_id}ï¼Œä¹¦ç±ID: {book_id}")
                    
                    # è·å–è¯¥ä¹¦ç±çš„æ‰€æœ‰è§’è‰²é…éŸ³åº“è§’è‰²
                    library_characters = self.db.query(Character).filter(Character.book_id == book_id).all()
                    character_library = {char.name: char for char in library_characters}
                    
                    logger.info(f"ğŸ”¥ğŸ”¥ğŸ”¥ [DEBUG] ğŸ“š [è§’è‰²é…éŸ³åº“å…³è”] ä¹¦ç±{book_id}å…±æœ‰{len(character_library)}ä¸ªè§’è‰²é…éŸ³åº“è§’è‰²: {list(character_library.keys())}")
                    for name, char in character_library.items():
                        logger.info(f"ğŸ”¥ğŸ”¥ğŸ”¥ [DEBUG] è§’è‰²é…éŸ³åº“è§’è‰²: {name} -> ID={char.id}, é…ç½®çŠ¶æ€={char.is_voice_configured}")
                else:
                    logger.warning(f"ğŸ”¥ğŸ”¥ğŸ”¥ [DEBUG] ç« èŠ‚{chapter_id}ä¸å­˜åœ¨")
            except Exception as e:
                logger.error(f"ğŸ”¥ğŸ”¥ğŸ”¥ [DEBUG] è·å–è§’è‰²é…éŸ³åº“å¤±è´¥: {str(e)}")
                import traceback
                logger.error(f"ğŸ”¥ğŸ”¥ğŸ”¥ [DEBUG] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        else:
            logger.warning(f"ğŸ”¥ğŸ”¥ğŸ”¥ [DEBUG] current_chapter_id ä¸ºç©ºï¼Œæ— æ³•å…³è”è§’è‰²é…éŸ³åº“")
        
        # ğŸ¤– æ–°å¢ï¼šAIäºŒæ¬¡åˆ†æå¤„ç†æœªçŸ¥è§’è‰²
        segments = await self._ai_reanalyze_unknown_segments(segments, detected_characters)
        
        synthesis_plan = []
        
        for i, segment in enumerate(segments):
            text_content = segment.get('text', '').strip()
            if not text_content:
                continue
            
            speaker = segment.get('speaker', '').strip()
            
            # ğŸ”§ ç°åœ¨åªå¤„ç†çœŸæ­£æ— æ³•ç¡®å®šçš„æƒ…å†µ
            if not speaker:
                speaker = 'æ—ç™½'
                logger.info(f"ğŸ”§ ç©ºspeakerè‡ªåŠ¨è®¾ä¸ºæ—ç™½: {text_content[:30]}...")
            
            # ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥ä»è§’è‰²é…éŸ³åº“è·å–IDï¼Œç®€åŒ–é€»è¾‘
            voice_id = None
            character_id = None  # ğŸš€ æ–°æ¶æ„ï¼šä½¿ç”¨character_id
            voice_name = "æœªåˆ†é…"
            
            # 1. ä¼˜å…ˆä»è§’è‰²é…éŸ³åº“è·å–IDï¼ˆæ— è®ºæ˜¯å¦é…ç½®è¯­éŸ³ï¼‰
            if speaker in character_library:
                library_char = character_library[speaker]
                character_id = library_char.id  # ğŸš€ æ–°æ¶æ„ï¼šä½¿ç”¨character_id
                # voice_id = library_char.id     # ğŸ”¥ ç§»é™¤ï¼šé¿å…IDç©ºé—´å†²çª
                voice_name = library_char.name
                logger.info(f"âœ… [è§’è‰²é…éŸ³åº“] è§’è‰²'{speaker}'ç›´æ¥ä½¿ç”¨é…éŸ³åº“ID: {character_id}")
            else:
                # 2. å¦‚æœè§’è‰²é…éŸ³åº“æ²¡æœ‰ï¼Œå†æ£€æŸ¥ä¼ ç»Ÿæ˜ å°„ï¼ˆåº”è¯¥å¾ˆå°‘è§ï¼‰
                if voice_mapping.get(speaker):
                    voice_id = voice_mapping.get(speaker)
                    # ğŸš€ æ–°æ¶æ„ï¼šä¼ ç»Ÿæ˜ å°„çš„voice_idå¯èƒ½æŒ‡å‘VoiceProfileï¼Œè®¾ä¸ºNone
                    character_id = None
                    try:
                        from ..models import VoiceProfile
                        voice_profile = self.db.query(VoiceProfile).filter(VoiceProfile.id == voice_id).first()
                        if voice_profile:
                            voice_name = voice_profile.name
                        else:
                            voice_name = f"Voice_{voice_id}"
                        logger.info(f"ğŸ“¢ [ä¼ ç»Ÿæ˜ å°„] è§’è‰²'{speaker}'ä½¿ç”¨ä¼ ç»Ÿæ˜ å°„: voice_id={voice_id}")
                    except Exception as e:
                        logger.warning(f"è·å–ä¼ ç»Ÿvoice_nameå¤±è´¥: {str(e)}")
                        voice_name = f"Voice_{voice_id}"
                else:
                    logger.warning(f"âš ï¸ è§’è‰²'{speaker}'æ—¢ä¸åœ¨è§’è‰²é…éŸ³åº“ä¸­ï¼Œä¹Ÿæ²¡æœ‰ä¼ ç»Ÿæ˜ å°„ï¼Œéœ€è¦ç”¨æˆ·æ‰‹åŠ¨åˆ†é…")
            
            # ğŸ”¥ TTSä¼˜åŒ–ï¼šæ ¹æ®æ¨¡å¼è°ƒæ•´å‚æ•°
            tts_params = self._get_optimized_tts_params(speaker, tts_optimization_mode, segment)
            
            # ğŸ”¥ æ¶æ„ä¿®å¤ï¼šè·å–ç« èŠ‚ä¿¡æ¯å¹¶å¼ºåˆ¶æ·»åŠ åˆ°segment_data
            chapter_number = None
            if chapter_id:
                try:
                    from ..models import BookChapter
                    chapter = self.db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
                    if chapter:
                        chapter_number = chapter.chapter_number
                        logger.debug(f"è·å–ç« èŠ‚ä¿¡æ¯: chapter_id={chapter_id}, chapter_number={chapter_number}")
                    else:
                        logger.error(f"ç« èŠ‚ID {chapter_id} ä¸å­˜åœ¨äºæ•°æ®åº“ä¸­")
                        raise ValueError(f"ç« èŠ‚ID {chapter_id} ä¸å­˜åœ¨")
                except Exception as e:
                    logger.error(f"è·å–ç« èŠ‚ä¿¡æ¯å¤±è´¥: {str(e)}")
                    raise ValueError(f"è·å–ç« èŠ‚ä¿¡æ¯å¤±è´¥: {str(e)}")
            else:
                logger.error("chapter_idä¸ºç©ºï¼Œæ— æ³•æ„å»ºå®Œæ•´çš„segmentæ•°æ®")
                raise ValueError("chapter_idä¸ºç©ºï¼Œæ— æ³•æ„å»ºå®Œæ•´çš„segmentæ•°æ®")

            # ğŸš€ æ–°æ¶æ„ï¼šå¼ºåˆ¶åŒ…å«ç« èŠ‚ä¿¡æ¯çš„æ ‡å‡†åŒ–segment_data
            segment_data = {
                "segment_id": i + 1,
                "chapter_id": chapter_id,           # ğŸ”¥ å¼ºåˆ¶æ·»åŠ ç« èŠ‚ID
                "chapter_number": chapter_number,   # ğŸ”¥ å¼ºåˆ¶æ·»åŠ ç« èŠ‚ç¼–å·
                "text": text_content,
                "speaker": speaker,
                "voice_name": voice_name,
                "text_type": segment.get('text_type', 'dialogue'),
                "confidence": segment.get('confidence', 0.8),
                "detection_rule": segment.get('detection_rule', 'ai_analysis'),
                **tts_params
            }
            
            # ğŸš€ æ–°æ¶æ„ï¼šä¸¥æ ¼åˆ†ç¦»IDç©ºé—´ï¼Œç¡®ä¿ä¸€è‡´æ€§
            if character_id:
                segment_data["character_id"] = character_id
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè§’è‰²é…éŸ³åº“ä¸è®¾ç½®voice_idï¼Œé¿å…IDå†²çª
                # segment_data["voice_id"] = voice_id  # ç§»é™¤è¿™è¡Œï¼Œé¿å…ä¸VoiceProfile IDå†²çª
            else:
                segment_data["voice_id"] = voice_id  # ä»…ä¼ ç»Ÿæ˜ å°„æ–¹å¼ä½¿ç”¨
            
            # ğŸ”¥ æ•°æ®å®Œæ•´æ€§éªŒè¯ï¼šä½¿ç”¨æ–°çš„SchemaéªŒè¯segmentæ•°æ®
            try:
                from ..schemas.segment_data import DataIntegrityValidator
                validated_segment = DataIntegrityValidator.validate_segment_data(segment_data)
                logger.debug(f"Segment {i+1} æ•°æ®éªŒè¯é€šè¿‡")
            except Exception as e:
                logger.error(f"Segment {i+1} æ•°æ®éªŒè¯å¤±è´¥: {str(e)}")
                logger.error(f"åŸå§‹æ•°æ®: {segment_data}")
                raise ValueError(f"Segmentæ•°æ®ä¸å®Œæ•´: {str(e)}")
            
            synthesis_plan.append(segment_data)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ„å»ºè§’è‰²ä¿¡æ¯æ—¶ä¼˜å…ˆä½¿ç”¨è§’è‰²é…éŸ³åº“æ•°æ®
        characters = []
        character_library_mappings = {}  # ç”¨äºæ”¶é›†è§’è‰²é…éŸ³åº“çš„æ˜ å°„
        
        # ğŸ”¥ æ–°å¢ï¼šä»synthesis_planä¸­æå–æ‰€æœ‰å®é™…å‡ºç°çš„è§’è‰²
        actual_speakers = set()
        for segment in synthesis_plan:
            speaker = segment.get('speaker')
            if speaker and speaker.strip():
                actual_speakers.add(speaker.strip())
        
        # ğŸ”¥ ä¼˜åŒ–ï¼šç¡®ä¿detected_charactersåŒ…å«æ‰€æœ‰å®é™…å‡ºç°çš„è§’è‰²
        detected_character_names = {char.get('name', '') for char in detected_characters}
        missing_characters = actual_speakers - detected_character_names
        
        if missing_characters:
            logger.warning(f"âš ï¸ [è§’è‰²æ±‡æ€»ä¿®å¤] å‘ç°synthesis_planä¸­å­˜åœ¨ä½†detected_charactersä¸­ç¼ºå¤±çš„è§’è‰²: {missing_characters}")
            # ä¸ºç¼ºå¤±çš„è§’è‰²åˆ›å»ºé»˜è®¤é…ç½®
            for missing_char in missing_characters:
                detected_characters.append({
                    'name': missing_char,
                    'voice_type': 'neutral',
                    'confidence': 0.7,
                    'source': 'synthesis_plan_è¡¥å……'
                })
                logger.info(f"ğŸ”§ [è§’è‰²æ±‡æ€»ä¿®å¤] è‡ªåŠ¨è¡¥å……è§’è‰²: {missing_char}")
        
        for character in detected_characters:
            char_name = character.get('name', '')
            if not char_name:
                continue
            
            voice_id = None
            character_id = None  # ğŸš€ æ–°æ¶æ„ï¼šä½¿ç”¨character_id
            voice_name = "æœªåˆ†é…"
            voice_type = "neutral"
            
            # ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥ä»è§’è‰²é…éŸ³åº“è·å–å®Œæ•´ä¿¡æ¯å¹¶å†™å…¥JSON
            if char_name in character_library:
                library_char = character_library[char_name]
                # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šæ— è®ºæ˜¯å¦é…ç½®è¯­éŸ³ï¼Œéƒ½ä½¿ç”¨è§’è‰²é…éŸ³åº“çš„ID
                character_id = library_char.id  # ğŸš€ æ–°æ¶æ„ï¼šä½¿ç”¨character_id
                # voice_id = library_char.id      # ğŸ”¥ ç§»é™¤ï¼šé¿å…IDç©ºé—´å†²çª
                voice_name = library_char.name
                voice_type = library_char.voice_type or "neutral"
                character_library_mappings[char_name] = str(library_char.id)
                logger.info(f"âœ… [è§’è‰²é…éŸ³åº“] è§’è‰²'{char_name}'ç›´æ¥ä½¿ç”¨é…éŸ³åº“ID: {character_id}")
            else:
                # å¦‚æœè§’è‰²é…éŸ³åº“æ²¡æœ‰ï¼Œä½¿ç”¨ä¼ ç»Ÿæ˜ å°„ï¼ˆä½†è¿™ç§æƒ…å†µåº”è¯¥å¾ˆå°‘ï¼‰
                if voice_mapping.get(char_name):
                    voice_id = voice_mapping.get(char_name)
                    character_id = None  # ğŸš€ æ–°æ¶æ„ï¼šä¼ ç»Ÿæ˜ å°„ä¸è®¾ç½®character_id
                    try:
                        from ..models import VoiceProfile
                        voice_profile = self.db.query(VoiceProfile).filter(VoiceProfile.id == voice_id).first()
                        if voice_profile:
                            voice_name = voice_profile.name
                        else:
                            voice_name = f"Voice_{voice_id}"
                    except Exception as e:
                        voice_name = f"Voice_{voice_id}"
                logger.warning(f"âš ï¸ [ä¼ ç»Ÿæ˜ å°„] è§’è‰²'{char_name}'ä¸åœ¨è§’è‰²é…éŸ³åº“ä¸­ï¼Œä½¿ç”¨ä¼ ç»Ÿæ˜ å°„: voice_id={voice_id}")
            
            # è®¡ç®—è§’è‰²åœ¨åˆæˆè®¡åˆ’ä¸­çš„å‡ºç°æ¬¡æ•°
            char_count = len([s for s in synthesis_plan if s.get('speaker') == char_name])
            
            # ğŸ”¥ æ–°å¢ï¼šä»è§’è‰²é…éŸ³åº“è·å–å®Œæ•´ä¿¡æ¯ï¼ˆåŒ…æ‹¬å¤´åƒï¼‰
            avatar_url = None
            if char_name in character_library:
                library_char = character_library[char_name]
                if library_char.avatar_path:
                    filename = os.path.basename(library_char.avatar_path)
                    avatar_url = f"/api/v1/avatars/{filename}"
            
            # ğŸš€ æ–°æ¶æ„ï¼šæ„å»ºè§’è‰²ä¿¡æ¯
            char_data = {
                "name": char_name,
                "voice_name": voice_name,
                "voice_type": voice_type,
                "count": char_count,
                "in_character_library": char_name in character_library,  # æ ‡è®°æ˜¯å¦åœ¨è§’è‰²é…éŸ³åº“ä¸­
                "is_voice_configured": char_name in character_library and character_library[char_name].is_voice_configured,  # ä»è§’è‰²é…éŸ³åº“åˆ¤æ–­
                "avatarUrl": avatar_url  # ğŸ”¥ æ–°å¢ï¼šå¤´åƒURL
            }
            
            # ğŸš€ æ–°æ¶æ„ï¼šä¸¥æ ¼åˆ†ç¦»IDç©ºé—´ï¼Œç¡®ä¿ä¸€è‡´æ€§
            if character_id:
                char_data["character_id"] = character_id
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè§’è‰²é…éŸ³åº“ä¸è®¾ç½®voice_idï¼Œé¿å…IDå†²çª
                # char_data["voice_id"] = voice_id  # ç§»é™¤è¿™è¡Œï¼Œé¿å…ä¸VoiceProfile IDå†²çª
            else:
                char_data["voice_id"] = voice_id if voice_id else ""  # ä»…ä¼ ç»Ÿæ˜ å°„æ–¹å¼ä½¿ç”¨
            
            characters.append(char_data)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœä½¿ç”¨äº†è§’è‰²é…éŸ³åº“ï¼ŒåŒæ­¥æ›´æ–°ä¹¦ç±çš„voice_mappings
        if character_library_mappings and chapter_id:
            try:
                from ..models import BookChapter, Book
                chapter = self.db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
                if chapter:
                    book = self.db.query(Book).filter(Book.id == chapter.book_id).first()
                    if book:
                        logger.info(f"ğŸ”„ [è§’è‰²é…éŸ³åº“åŒæ­¥] æ›´æ–°ä¹¦ç±{book.id}çš„voice_mappings: {character_library_mappings}")
                        # æ›´æ–°ä¹¦ç±çš„voice_mappings
                        for char_name, voice_id in character_library_mappings.items():
                            book.set_character_voice_mapping(char_name, voice_id)
                        self.db.commit()
                        logger.info(f"âœ… [è§’è‰²é…éŸ³åº“åŒæ­¥] æˆåŠŸæ›´æ–°ä¹¦ç±voice_mappings")
            except Exception as e:
                logger.error(f"âŒ [è§’è‰²é…éŸ³åº“åŒæ­¥] æ›´æ–°ä¹¦ç±voice_mappingså¤±è´¥: {str(e)}")
                # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ
        
        # ğŸ”¥ æœ€ç»ˆæ•°æ®å®Œæ•´æ€§éªŒè¯ï¼šéªŒè¯æ•´ä¸ªsynthesis_plan
        final_synthesis_data = {
            "project_info": {
                "novel_type": "æ™ºèƒ½æ£€æµ‹",
                "analysis_time": datetime.now().isoformat(),
                "total_segments": len(synthesis_plan),
                "ai_model": "optimized-smart-analysis",
                "detected_characters": len(characters),
                "character_library_linked": len(character_library) > 0  # æ ‡è®°æ˜¯å¦å…³è”äº†è§’è‰²é…éŸ³åº“
            },
            "synthesis_plan": synthesis_plan,
            "characters": characters
        }
        
        # ğŸ”¥ æ¶æ„çº§éªŒè¯ï¼šç¡®ä¿æ•´ä¸ªæ•°æ®ç»“æ„çš„ä¸€è‡´æ€§
        try:
            from ..schemas.segment_data import DataIntegrityValidator
            validated_plan = DataIntegrityValidator.validate_synthesis_plan(final_synthesis_data)
            logger.info(f"âœ… åˆæˆè®¡åˆ’æ•´ä½“éªŒè¯é€šè¿‡ï¼Œå…± {len(synthesis_plan)} ä¸ªsegments")
            
            # è®°å½•éªŒè¯æˆåŠŸçš„ç»Ÿè®¡ä¿¡æ¯
            chapter_ids = set(seg.chapter_id for seg in validated_plan.synthesis_plan)
            logger.info(f"âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡ï¼šæ¶‰åŠç« èŠ‚ {sorted(chapter_ids)}")
            
        except Exception as e:
            logger.error(f"âŒ åˆæˆè®¡åˆ’æ•´ä½“éªŒè¯å¤±è´¥: {str(e)}")
            # è®°å½•è¯¦ç»†çš„éªŒè¯å¤±è´¥ä¿¡æ¯
            logger.error(f"éªŒè¯å¤±è´¥çš„æ•°æ®ç»“æ„: {final_synthesis_data}")
            raise ValueError(f"æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥ï¼Œæ— æ³•ä¿å­˜åˆ°æ•°æ®åº“: {str(e)}")
        
        # å®Œå…¨åŒ¹é…ç°æœ‰ç³»ç»Ÿæ ¼å¼
        return final_synthesis_data

    def _get_optimized_tts_params(self, speaker: str, optimization_mode: str, segment: Dict) -> Dict:
        """è·å–ä¼˜åŒ–çš„TTSå‚æ•°"""
        try:
            # ğŸ¯ æ™ºèƒ½TTSå‚æ•°é…ç½® - åŸºäºè§’è‰²å’Œæ–‡æœ¬å†…å®¹
            if not self.tts_optimizer:
                from ..services.ai_tts_optimizer import AITTSOptimizer
                self.tts_optimizer = AITTSOptimizer(self.ollama_detector)
                # æ ¹æ®ä¼˜åŒ–æ¨¡å¼é…ç½®TTSåˆ†æ
                if optimization_mode == "fast":
                    self.tts_optimizer.set_enable_ai_analysis(False)
                    logger.info("ğŸš€ TTSä¼˜åŒ–å™¨è®¾ç½®ä¸ºå¿«é€Ÿæ¨¡å¼ï¼ˆç¦ç”¨AIåˆ†æï¼‰")
                elif optimization_mode == "quality":
                    self.tts_optimizer.set_enable_ai_analysis(True)
                    logger.info("ğŸ¯ TTSä¼˜åŒ–å™¨è®¾ç½®ä¸ºè´¨é‡æ¨¡å¼ï¼ˆå¯ç”¨AIåˆ†æï¼‰")
                else:  # balanced
                    self.tts_optimizer.set_enable_ai_analysis(True)
                    logger.info("âš–ï¸ TTSä¼˜åŒ–å™¨è®¾ç½®ä¸ºå¹³è¡¡æ¨¡å¼")
            
            # è·å–æ™ºèƒ½TTSå‚æ•°
            tts_params = self.tts_optimizer.get_smart_tts_params(segment, [])
            return tts_params
            
        except Exception as e:
            logger.warning(f"è·å–TTSå‚æ•°å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            # è¿”å›é»˜è®¤å‚æ•°
            return {
                "speed": 1.0,
                "pitch": 1.0,
                "volume": 1.0,
                "emotion": "neutral"
            }
    

    
    def _clean_and_normalize(self, text: str) -> str:
        """æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬"""
        # åŸºæœ¬æ¸…ç†ï¼Œä¿æŒåŸæ–‡å®Œæ•´æ€§
        text = text.strip()
        # ç»Ÿä¸€æ¢è¡Œç¬¦
        text = re.sub(r'\r\n', '\n', text)
        # ç§»é™¤å¤šä½™ç©ºè¡Œï¼ˆä¿ç•™æ®µè½ç»“æ„ï¼‰
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬tokenæ•°é‡"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        return int(chinese_chars * 1.5 + english_words)
    


    async def get_content_stats(self, chapter_id: int, db: Session) -> Dict:
        """
        è·å–ç« èŠ‚å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            from app.models import BookChapter
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ValueError("ç« èŠ‚ä¸å­˜åœ¨")
            
            # åŸºæœ¬ç»Ÿè®¡
            content = chapter.content or ""
            word_count = len(content.replace(' ', '').replace('\n', ''))
            
            # ä½¿ç”¨ChapterChunkerè¿›è¡Œåˆ†å—åˆ†æ
            chunker = ChapterChunker()
            chunks = chunker.chunk_chapter(content)
            
            # ä¼°ç®—å¤„ç†æ—¶é—´å’Œå»ºè®®
            estimated_time = len(chunks) * 2  # æ¯ä¸ªchunkå¤§çº¦2ç§’
            processing_recommendation = "fast" if len(chunks) <= 5 else "detailed" if len(chunks) <= 15 else "distributed"
            
            return {
                "chapter_id": chapter_id,
                "chapter_title": chapter.chapter_title,
                "word_count": word_count,
                "chunk_count": len(chunks),
                "estimated_processing_time": estimated_time,
                "processing_recommendation": processing_recommendation,
                "content_preview": content[:200] + "..." if len(content) > 200 else content
            }
            
        except Exception as e:
            logger.error(f"è·å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {str(e)}")
            raise

    async def get_synthesis_preview(self, chapter_id: int, db: Session) -> Dict:
        """
        è·å–ç« èŠ‚è¯­éŸ³åˆæˆé¢„è§ˆ
        """
        try:
            from app.models import BookChapter
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ValueError("ç« èŠ‚ä¸å­˜åœ¨")
            
            content = chapter.content or ""
            
            # å¿«é€Ÿè§’è‰²æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            import re
            dialogue_patterns = [
                r'"([^"]*)"',  # åŒå¼•å·å¯¹è¯
                r'"([^"]*)"',  # ä¸­æ–‡åŒå¼•å·
                r'ã€Œ([^ã€]*)ã€',  # æ—¥å¼å¼•å·
            ]
            
            dialogues = []
            for pattern in dialogue_patterns:
                matches = re.findall(pattern, content)
                dialogues.extend(matches)
            
            # ä¼°ç®—è§’è‰²æ•°é‡ï¼ˆç®€åŒ–ï¼‰
            estimated_characters = min(len(set(dialogues[:10])), 8) if dialogues else 1
            
            # ä½¿ç”¨ChapterChunkerè¿›è¡Œåˆ†å—
            chunker = ChapterChunker()
            chunks = chunker.chunk_chapter(content)
            
            return {
                "chapter_id": chapter_id,
                "chapter_title": chapter.chapter_title,
                "estimated_characters": estimated_characters,
                "dialogue_count": len(dialogues),
                "chunk_count": len(chunks),
                "sample_dialogues": dialogues[:5],  # å‰5ä¸ªå¯¹è¯ç¤ºä¾‹
                "processing_complexity": "simple" if len(chunks) <= 3 else "moderate" if len(chunks) <= 10 else "complex"
            }
            
        except Exception as e:
            logger.error(f"è·å–åˆæˆé¢„è§ˆå¤±è´¥: {str(e)}")
            raise

    async def get_preparation_status(self, chapter_id: int, db: Session) -> Dict:
        """
        è·å–ç« èŠ‚æ™ºèƒ½å‡†å¤‡çŠ¶æ€
        æ£€æŸ¥ç« èŠ‚æ˜¯å¦å·²ç»å®Œæˆæ™ºèƒ½å‡†å¤‡ï¼Œä»¥åŠå‡†å¤‡çš„è´¨é‡
        """
        try:
            from app.models import BookChapter, AnalysisResult
            
            # è·å–ç« èŠ‚åŸºæœ¬ä¿¡æ¯
            chapter = db.query(BookChapter).filter(BookChapter.id == chapter_id).first()
            if not chapter:
                raise ValueError("ç« èŠ‚ä¸å­˜åœ¨")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æç»“æœ
            analysis_result = db.query(AnalysisResult).filter(
                AnalysisResult.chapter_id == chapter_id
            ).order_by(AnalysisResult.created_at.desc()).first()
            
            if analysis_result and analysis_result.synthesis_plan:
                # æœ‰åˆ†æç»“æœï¼Œæ£€æŸ¥å®Œæ•´æ€§
                synthesis_plan = analysis_result.synthesis_plan
                if isinstance(synthesis_plan, dict) and 'synthesis_plan' in synthesis_plan:
                    segments_count = len(synthesis_plan['synthesis_plan'])
                    characters_count = len(synthesis_plan.get('characters', []))
                    
                    return {
                        "chapter_id": chapter_id,
                        "preparation_complete": True,
                        "analysis_status": chapter.analysis_status,
                        "synthesis_status": chapter.synthesis_status,
                        "segments_count": segments_count,
                        "characters_count": characters_count,
                        "last_prepared": analysis_result.created_at.isoformat() if analysis_result.created_at else None,
                        "preparation_quality": "good" if segments_count > 0 and characters_count > 0 else "poor"
                    }
            
            # æ²¡æœ‰åˆ†æç»“æœæˆ–ç»“æœä¸å®Œæ•´
            return {
                "chapter_id": chapter_id,
                "preparation_complete": False,
                "analysis_status": chapter.analysis_status,
                "synthesis_status": chapter.synthesis_status,
                "segments_count": 0,
                "characters_count": 0,
                "last_prepared": None,
                "preparation_quality": "none"
            }
            
        except Exception as e:
            logger.error(f"è·å–ç« èŠ‚ {chapter_id} å‡†å¤‡çŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                "chapter_id": chapter_id,
                "preparation_complete": False,
                "analysis_status": "unknown",
                "synthesis_status": "unknown",
                "error": str(e)
            }
    
    def _validate_synthesis_completeness(self, original_text: str, synthesis_json: Dict) -> bool:
        """ğŸ”¥ æ–°å¢ï¼šæ ¡éªŒæœ€ç»ˆåˆæˆè®¡åˆ’çš„å®Œæ•´æ€§"""
        try:
            synthesis_plan = synthesis_json.get('synthesis_plan', [])
            if not synthesis_plan:
                logger.warning("åˆæˆè®¡åˆ’ä¸ºç©º")
                return False
            
            # ç»Ÿè®¡åŸæ–‡å­—æ•°ï¼ˆå»é™¤ç©ºæ ¼å’Œæ¢è¡Œï¼‰
            original_chars = len(original_text.replace(' ', '').replace('\n', '').replace('\r', ''))
            
            # ç»Ÿè®¡synthesis_planä¸­æ‰€æœ‰textçš„å­—æ•°
            synthesis_chars = sum(
                len(segment.get('text', '').replace(' ', '').replace('\n', '').replace('\r', ''))
                for segment in synthesis_plan
            )
            
            # è®¡ç®—å®Œæ•´åº¦æ¯”ä¾‹
            completeness_ratio = synthesis_chars / original_chars if original_chars > 0 else 0
            
            logger.info(f"æœ€ç»ˆåˆæˆè®¡åˆ’å®Œæ•´æ€§æ ¡éªŒ: åŸæ–‡{original_chars}å­—ç¬¦ï¼Œåˆæˆè®¡åˆ’{synthesis_chars}å­—ç¬¦ï¼Œå®Œæ•´åº¦{completeness_ratio:.2%}")
            
            # å¦‚æœå·®å¼‚è¶…è¿‡10%ï¼Œè®¤ä¸ºä¸å®Œæ•´
            if completeness_ratio < 0.90:
                logger.warning(f"æœ€ç»ˆåˆæˆè®¡åˆ’å®Œæ•´æ€§æ ¡éªŒå¤±è´¥: å®Œæ•´åº¦ä»…{completeness_ratio:.2%}")
                return False
            
            logger.info("æœ€ç»ˆåˆæˆè®¡åˆ’å®Œæ•´æ€§æ ¡éªŒé€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆå®Œæ•´æ€§æ ¡éªŒå¼‚å¸¸: {str(e)}")
            return False
    
    def _log_completeness_details(self, original_text: str, synthesis_json: Dict):
        """ğŸ”¥ æ–°å¢ï¼šè®°å½•å®Œæ•´æ€§æ ¡éªŒçš„è¯¦ç»†ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•"""
        try:
            synthesis_plan = synthesis_json.get('synthesis_plan', [])
            
            logger.info("=== å®Œæ•´æ€§æ ¡éªŒè¯¦ç»†ä¿¡æ¯ ===")
            logger.info(f"åŸæ–‡é•¿åº¦: {len(original_text)} å­—ç¬¦")
            logger.info(f"åˆæˆè®¡åˆ’æ®µè½æ•°: {len(synthesis_plan)}")
            
            # è®°å½•åŸæ–‡çš„å‰100å­—ç¬¦å’Œå100å­—ç¬¦
            original_start = original_text[:100] if len(original_text) > 100 else original_text
            original_end = original_text[-100:] if len(original_text) > 100 else ""
            
            logger.info(f"åŸæ–‡å¼€å¤´: {original_start}")
            if original_end:
                logger.info(f"åŸæ–‡ç»“å°¾: {original_end}")
            
            # è®°å½•åˆæˆè®¡åˆ’çš„ç¬¬ä¸€æ®µå’Œæœ€åä¸€æ®µ
            if synthesis_plan:
                first_segment = synthesis_plan[0].get('text', '')
                last_segment = synthesis_plan[-1].get('text', '')
                
                logger.info(f"åˆæˆè®¡åˆ’ç¬¬ä¸€æ®µ: {first_segment}")
                logger.info(f"åˆæˆè®¡åˆ’æœ€åä¸€æ®µ: {last_segment}")
                
                # æ£€æŸ¥åŸæ–‡ç»“å°¾æ˜¯å¦åœ¨åˆæˆè®¡åˆ’ä¸­
                if original_end and original_end not in ' '.join([seg.get('text', '') for seg in synthesis_plan]):
                    logger.warning(f"åŸæ–‡ç»“å°¾å†…å®¹åœ¨åˆæˆè®¡åˆ’ä¸­æœªæ‰¾åˆ°: {original_end}")
            
            logger.info("=== å®Œæ•´æ€§æ ¡éªŒè¯¦ç»†ä¿¡æ¯ç»“æŸ ===")
            
        except Exception as e:
            logger.error(f"è®°å½•å®Œæ•´æ€§è¯¦ç»†ä¿¡æ¯å¼‚å¸¸: {str(e)}")
    
    def _extract_missing_content(self, original_text: str, synthesis_json: Dict) -> str:
        """ğŸ”¥ æ–°å¢ï¼šæå–ä¸¢å¤±çš„å†…å®¹ï¼Œç”¨äºè°ƒè¯•å’Œä¿®å¤"""
        try:
            synthesis_plan = synthesis_json.get('synthesis_plan', [])
            synthesis_text = ' '.join([seg.get('text', '') for seg in synthesis_plan])
            
            # ç®€å•çš„å·®å¼‚æ£€æµ‹ï¼šæ‰¾å‡ºåŸæ–‡ä¸­ä½†ä¸åœ¨åˆæˆè®¡åˆ’ä¸­çš„å†…å®¹
            missing_parts = []
            
            # æŒ‰å¥å­åˆ†å‰²åŸæ–‡
            import re
            original_sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', original_text)
            
            for sentence in original_sentences:
                sentence = sentence.strip()
                if sentence and sentence not in synthesis_text:
                    missing_parts.append(sentence)
            
            if missing_parts:
                missing_content = 'ï¼›'.join(missing_parts)
                logger.warning(f"æ£€æµ‹åˆ°ä¸¢å¤±çš„å†…å®¹ç‰‡æ®µ: {missing_content}")
                return missing_content
            
            return ""
            
        except Exception as e:
            logger.error(f"æå–ä¸¢å¤±å†…å®¹å¼‚å¸¸: {str(e)}")
            return ""