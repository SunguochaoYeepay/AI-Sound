"""
Ollama AIè§’è‰²æ£€æµ‹å™¨
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½è§’è‰²è¯†åˆ«å’Œåˆ†æ
"""

import json
import logging
import os
import requests
import time
from typing import Dict, List, Optional

logger = logging.getLogger(__name__)


class OllamaCharacterDetector:
    """åŸºäºOllamaçš„è§’è‰²æ£€æµ‹å™¨"""
    
    def __init__(self, model_name: str = "qwen2.5:7b", ollama_url: str = None):
        self.model_name = model_name
        self.api_url = ollama_url or "http://localhost:11434/api/generate"
        self.logger = logging.getLogger(__name__)

    def _smart_chunk_text(self, text: str, max_chunk_size: int = 3000) -> List[Dict]:
        """ğŸš€ æ™ºèƒ½åˆ†å—ï¼šæŒ‰æ®µè½å’Œå¥å­è¾¹ç•Œåˆ†å—ï¼Œé¿å…æˆªæ–­"""
        import re
        
        # å¦‚æœæ–‡æœ¬è¾ƒçŸ­ï¼Œä¸éœ€è¦åˆ†å—
        if len(text) <= max_chunk_size:
            return [{"chunk_id": 0, "text": text, "start_pos": 0, "end_pos": len(text)}]
        
        logger.info(f"æ–‡æœ¬è¿‡é•¿({len(text)}å­—ç¬¦)ï¼Œå¼€å§‹æ™ºèƒ½åˆ†å—(æœ€å¤§{max_chunk_size}å­—ç¬¦/å—)")
        
        chunks = []
        chunk_id = 0
        
        # é¦–å…ˆæŒ‰åŒæ¢è¡Œç¬¦åˆ†æ®µ
        paragraphs = text.split('\n\n')
        current_chunk = ""
        current_start = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            # å¦‚æœåŠ å…¥å½“å‰æ®µè½åè¶…è¿‡é™åˆ¶ï¼Œå…ˆä¿å­˜å½“å‰å—
            if len(current_chunk) + len(para) > max_chunk_size and current_chunk:
                # ä¿å­˜å½“å‰å—
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": current_chunk.strip(),
                    "start_pos": current_start,
                    "end_pos": current_start + len(current_chunk)
                })
                chunk_id += 1
                current_start += len(current_chunk)
                current_chunk = para + "\n\n"
            else:
                current_chunk += para + "\n\n"
        
        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append({
                "chunk_id": chunk_id,
                "text": current_chunk.strip(),
                "start_pos": current_start,
                "end_pos": current_start + len(current_chunk)
            })
        
        # å¦‚æœæŸä¸ªå—ä»ç„¶è¿‡å¤§ï¼ŒæŒ‰å¥å­è¿›ä¸€æ­¥åˆ†å—
        final_chunks = []
        for chunk in chunks:
            if len(chunk["text"]) > max_chunk_size:
                sub_chunks = self._split_by_sentences(chunk["text"], max_chunk_size, chunk["start_pos"])
                final_chunks.extend(sub_chunks)
            else:
                final_chunks.append(chunk)
        
        logger.info(f"æ™ºèƒ½åˆ†å—å®Œæˆï¼š{len(text)}å­—ç¬¦ â†’ {len(final_chunks)}å—ï¼Œå¹³å‡{len(text)//len(final_chunks)}å­—ç¬¦/å—")
        return final_chunks
    
    def _split_by_sentences(self, text: str, max_size: int, start_offset: int = 0) -> List[Dict]:
        """æŒ‰å¥å­è¾¹ç•Œè¿›ä¸€æ­¥åˆ†å—"""
        import re
        
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å¥
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ][""]?)', text)
        chunks = []
        chunk_id = len(chunks)
        current_chunk = ""
        current_start = start_offset
        
        i = 0
        while i < len(sentences):
            sentence = sentences[i]
            # å¦‚æœæ˜¯æ ‡ç‚¹ç¬¦å·ï¼Œä¸å‰ä¸€å¥åˆå¹¶
            if i + 1 < len(sentences) and sentences[i + 1] in ['ã€‚', 'ï¼', 'ï¼Ÿ', '"', '"']:
                sentence += sentences[i + 1]
                i += 2
            else:
                i += 1
            
            if len(current_chunk) + len(sentence) > max_size and current_chunk:
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": current_chunk.strip(),
                    "start_pos": current_start,
                    "end_pos": current_start + len(current_chunk)
                })
                chunk_id += 1
                current_start += len(current_chunk)
                current_chunk = sentence
            else:
                current_chunk += sentence
        
        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append({
                "chunk_id": chunk_id,
                "text": current_chunk.strip(),
                "start_pos": current_start,
                "end_pos": current_start + len(current_chunk)
            })
        
        return chunks
    
    def _merge_chunk_results(self, chunk_results: List[Dict]) -> Dict:
        """ğŸ”— åˆå¹¶åˆ†å—åˆ†æç»“æœ"""
        merged_segments = []
        merged_characters = {}
        total_order = 0
        
        for chunk_result in chunk_results:
            chunk_id = chunk_result.get("chunk_id", 0)
            
            # åˆå¹¶segmentsï¼Œè°ƒæ•´order
            for segment in chunk_result.get("segments", []):
                segment["order"] = total_order
                segment["chunk_id"] = chunk_id  # æ ‡è®°æ¥æºå—
                merged_segments.append(segment)
                total_order += 1
            
            # åˆå¹¶charactersï¼ŒæŒ‰åå­—å»é‡
            for char in chunk_result.get("characters", []):
                char_name = char["name"]
                if char_name in merged_characters:
                    # åˆå¹¶é¢‘æ¬¡å’Œç½®ä¿¡åº¦
                    existing = merged_characters[char_name]
                    existing["frequency"] += char.get("frequency", 1)
                    existing["confidence"] = max(existing["confidence"], char.get("confidence", 0.5))
                    # ä¿ç•™æ›´è¯¦ç»†çš„æè¿°
                    if len(char.get("personality_description", "")) > len(existing.get("personality_description", "")):
                        existing["personality_description"] = char["personality_description"]
                else:
                    merged_characters[char_name] = char
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰é¢‘æ¬¡æ’åº
        characters_list = list(merged_characters.values())
        characters_list.sort(key=lambda x: x.get("frequency", 0), reverse=True)
        
        logger.info(f"åˆ†å—ç»“æœåˆå¹¶å®Œæˆï¼š{len(merged_segments)}ä¸ªæ®µè½ï¼Œ{len(characters_list)}ä¸ªè§’è‰²")
        
        return {
            "segments": merged_segments,
            "characters": characters_list
        }
    
    async def analyze_text(self, text: str, chapter_info: dict) -> dict:
        """ä½¿ç”¨Ollamaåˆ†ææ–‡æœ¬ä¸­çš„è§’è‰² - æ”¯æŒæ™ºèƒ½åˆ†å—å¤„ç†"""
        # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çš„WebSocketç®¡ç†å™¨
        try:
            from app.websocket.manager import websocket_manager
            
            async def send_analysis_progress(session_id, progress, message):
                await websocket_manager.publish_to_topic(
                    f"analysis_session_{session_id}",
                    {
                        "type": "progress_update",
                        "data": {
                            "progress": progress,
                            "message": message,
                            "session_id": session_id
                        }
                    }
                )
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰ä¸€ä¸ªmockå‡½æ•°
            async def send_analysis_progress(session_id, progress, message):
                logger.info(f"[è¿›åº¦ {progress}%] {message}")
            logger.warning("æ— æ³•å¯¼å…¥websocket_managerï¼Œä½¿ç”¨æ—¥å¿—è®°å½•è¿›åº¦")
        
        start_time = time.time()
        session_id = chapter_info.get('session_id', chapter_info['chapter_id'])
        
        try:
            # å‘é€å¼€å§‹åˆ†æè¿›åº¦
            await send_analysis_progress(session_id, 10, f"å¼€å§‹åˆ†æç« èŠ‚: {chapter_info['chapter_title']}")
            
            # ğŸš€ æ™ºèƒ½åˆ†å—å¤„ç†ï¼šé•¿æ–‡æœ¬è‡ªåŠ¨åˆ†å—
            text_length = len(text)
            chunk_threshold = 4000  # è¶…è¿‡4000å­—ç¬¦å¯ç”¨åˆ†å—
            
            if text_length > chunk_threshold:
                logger.info(f"æ–‡æœ¬é•¿åº¦{text_length}å­—ç¬¦ï¼Œå¯ç”¨æ™ºèƒ½åˆ†å—å¤„ç†")
                await send_analysis_progress(session_id, 20, f"æ–‡æœ¬è¾ƒé•¿({text_length}å­—ç¬¦)ï¼Œå¯ç”¨æ™ºèƒ½åˆ†å—å¤„ç†...")
                
                # æ™ºèƒ½åˆ†å—
                chunks = self._smart_chunk_text(text, max_chunk_size=3000)
                await send_analysis_progress(session_id, 30, f"å·²åˆ†ä¸º{len(chunks)}ä¸ªå—ï¼Œå¼€å§‹é€å—åˆ†æ...")
                
                # é€å—åˆ†æ
                chunk_results = []
                progress_step = 50 / len(chunks)  # 50%çš„è¿›åº¦ç”¨äºåˆ†å—åˆ†æ
                
                for i, chunk in enumerate(chunks):
                    chunk_progress = 30 + int(i * progress_step)
                    await send_analysis_progress(session_id, chunk_progress, f"åˆ†æç¬¬{i+1}/{len(chunks)}å—...")
                    
                    chunk_result = await self._analyze_single_chunk(chunk["text"], chunk["chunk_id"])
                    chunk_result["chunk_id"] = chunk["chunk_id"]
                    chunk_results.append(chunk_result)
                
                await send_analysis_progress(session_id, 80, "æ­£åœ¨åˆå¹¶åˆ†å—åˆ†æç»“æœ...")
                
                # åˆå¹¶åˆ†å—ç»“æœ
                result = self._merge_chunk_results(chunk_results)
                
                # å®Œæ•´æ€§æ ¡éªŒï¼ˆåŸºäºåŸæ–‡ï¼‰
                completeness_valid = self._validate_completeness(text, result['segments'])
                
                analysis_method = f"ollama_ai_chunked_{len(chunks)}_blocks"
                
            else:
                logger.info(f"æ–‡æœ¬é•¿åº¦{text_length}å­—ç¬¦ï¼Œä½¿ç”¨å•æ¬¡åˆ†æ")
                await send_analysis_progress(session_id, 30, "æ­£åœ¨è°ƒç”¨AIæ¨¡å‹è¿›è¡Œè§’è‰²è¯†åˆ«...")
                
                # ç›´æ¥å•æ¬¡åˆ†æ
                result = await self._analyze_single_text(text)
                completeness_valid = self._validate_completeness(text, result['segments'])
                analysis_method = "ollama_ai_single"
                chunks = []  # å•æ¬¡åˆ†ææ—¶ä¸ºç©ºåˆ—è¡¨
            
            processing_time = time.time() - start_time
            
            await send_analysis_progress(session_id, 100, f"åˆ†æå®Œæˆï¼Œè¯†åˆ«åˆ°{len(result['characters'])}ä¸ªè§’è‰²")
            
            # æ™ºèƒ½åˆ†æé˜¶æ®µï¼šè¿”å›æ‰€æœ‰è¯†åˆ«åˆ°çš„è§’è‰²ï¼ˆä¸è¿‡æ»¤å·²å­˜åœ¨çš„ï¼‰
            all_characters = result['characters']
            
            return {
                "chapter_id": chapter_info['chapter_id'],
                "chapter_title": chapter_info['chapter_title'],
                "chapter_number": chapter_info['chapter_number'],
                "detected_characters": all_characters,  # è¿”å›æ‰€æœ‰è§’è‰²
                "segments": result['segments'],
                "processing_stats": {
                    "total_segments": len(result['segments']),
                    "dialogue_segments": len([s for s in result['segments'] if s['text_type'] == 'dialogue']),
                    "narration_segments": len([s for s in result['segments'] if s['text_type'] == 'narration']),
                    "characters_found": len(result['characters']),
                    "new_characters_found": len(result['characters']),
                    "analysis_method": analysis_method,
                    "processing_time": round(processing_time, 2),
                    "text_length": len(text),
                    "ai_model": self.model_name,
                    "completeness_validated": completeness_valid,  # ğŸ”¥ æ–°å¢ï¼šå®Œæ•´æ€§æ ¡éªŒç»“æœ
                    "chunk_count": len(chunks) if text_length > chunk_threshold else 1  # ğŸ”¥ æ–°å¢ï¼šåˆ†å—æ•°é‡
                }
            }
                
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ Ollamaè§’è‰²åˆ†æå¼‚å¸¸å¤±è´¥: {str(e)}")
            await send_analysis_progress(session_id, 0, f"AIåˆ†æå¤±è´¥: {str(e)}")
            raise Exception(f"Ollamaè§’è‰²åˆ†æå¤±è´¥: {str(e)}")
    
    async def _analyze_single_text(self, text: str) -> Dict:
        """å•æ¬¡åˆ†ææ–‡æœ¬ï¼ˆä¸åˆ†å—ï¼‰"""
        max_retries = 3
        response = None
        
        for attempt in range(max_retries):
            try:
                prompt = self._build_comprehensive_analysis_prompt(text)
                response = self._call_ollama(prompt)
                
                if response:
                    break
                else:
                    logger.warning(f"ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ï¼ŒOllamaè¿”å›ç©ºå“åº”")
                    if attempt < max_retries - 1:
                        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    
            except Exception as e:
                logger.error(f"ç¬¬{attempt + 1}æ¬¡å°è¯•å¼‚å¸¸: {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                else:
                    raise e
        
        if response:
            # è§£æOllamaè¿”å›çš„å®Œæ•´ç»“æœ
            result = self._parse_comprehensive_response(response)
            
            # ğŸ”¥ ä¿®å¤ï¼šå¢åŠ å†…å®¹å®Œæ•´æ€§æ ¡éªŒå’Œé‡è¯•
            completeness_valid = self._validate_completeness(text, result['segments'])
            if not completeness_valid:
                logger.warning("å†…å®¹å®Œæ•´æ€§æ ¡éªŒå¤±è´¥ï¼Œå°è¯•é‡æ–°åˆ†æ")
                
                # å¦‚æœå®Œæ•´æ€§æ ¡éªŒå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ›´è¯¦ç»†çš„æç¤ºè¯é‡æ–°åˆ†æ
                detailed_prompt = self._build_detailed_analysis_prompt(text)
                retry_response = self._call_ollama(detailed_prompt)
                
                if retry_response:
                    retry_result = self._parse_comprehensive_response(retry_response)
                    retry_completeness = self._validate_completeness(text, retry_result['segments'])
                    
                    if retry_completeness:
                        result = retry_result
                        logger.info("é‡æ–°åˆ†ææˆåŠŸï¼Œå†…å®¹å®Œæ•´æ€§æ ¡éªŒé€šè¿‡")
                    else:
                        logger.warning("é‡æ–°åˆ†æä»æœªé€šè¿‡å®Œæ•´æ€§æ ¡éªŒï¼Œä½¿ç”¨åŸç»“æœå¹¶è®°å½•è­¦å‘Š")
            
            return result
        else:
            # Ollamaè°ƒç”¨å¤±è´¥ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
            logger.error("âŒ Ollama APIè°ƒç”¨å¤±è´¥ï¼Œæ²¡æœ‰è¿”å›æœ‰æ•ˆå“åº”")
            raise Exception("Ollama APIè°ƒç”¨å¤±è´¥ï¼Œæ²¡æœ‰è¿”å›æœ‰æ•ˆå“åº”")
    
    async def _analyze_single_chunk(self, chunk_text: str, chunk_id: int) -> Dict:
        """åˆ†æå•ä¸ªåˆ†å—"""
        logger.info(f"å¼€å§‹åˆ†æç¬¬{chunk_id}å—ï¼Œé•¿åº¦{len(chunk_text)}å­—ç¬¦")
        
        try:
            prompt = self._build_comprehensive_analysis_prompt(chunk_text)
            response = self._call_ollama(prompt)
            
            if response:
                result = self._parse_comprehensive_response(response)
                logger.info(f"ç¬¬{chunk_id}å—åˆ†æå®Œæˆï¼š{len(result.get('segments', []))}æ®µè½ï¼Œ{len(result.get('characters', []))}ä¸ªè§’è‰²")
                return result
            else:
                logger.warning(f"ç¬¬{chunk_id}å—åˆ†æå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                return {"segments": [], "characters": []}
                
        except Exception as e:
            logger.error(f"ç¬¬{chunk_id}å—åˆ†æå¼‚å¸¸: {str(e)}")
            return {"segments": [], "characters": []}
    
    def _validate_completeness(self, original_text: str, segments: List[Dict]) -> bool:
        """ğŸ”¥ æ–°å¢ï¼šæ ¡éªŒåˆ†æç»“æœçš„å®Œæ•´æ€§"""
        try:
            # ç»Ÿè®¡åŸæ–‡å­—æ•°ï¼ˆå»é™¤ç©ºæ ¼å’Œæ¢è¡Œï¼‰
            original_chars = len(original_text.replace(' ', '').replace('\n', '').replace('\r', ''))
            
            # ç»Ÿè®¡segmentså­—æ•°ï¼ˆå»é™¤ç©ºæ ¼å’Œæ¢è¡Œï¼‰
            segment_chars = sum(len(seg.get('text', '').replace(' ', '').replace('\n', '').replace('\r', '')) for seg in segments)
            
            # è®¡ç®—å®Œæ•´åº¦æ¯”ä¾‹
            completeness_ratio = segment_chars / original_chars if original_chars > 0 else 0
            
            logger.info(f"å†…å®¹å®Œæ•´æ€§æ ¡éªŒ: åŸæ–‡{original_chars}å­—ç¬¦ï¼Œåˆ†æç»“æœ{segment_chars}å­—ç¬¦ï¼Œå®Œæ•´åº¦{completeness_ratio:.2%}")
            
            # å¦‚æœå·®å¼‚è¶…è¿‡25%ï¼Œè®¤ä¸ºä¸å®Œæ•´ (é’ˆå¯¹7Bæ¨¡å‹ä¼˜åŒ–)
            if completeness_ratio < 0.75:
                logger.warning(f"å†…å®¹å®Œæ•´æ€§æ ¡éªŒå¤±è´¥: å®Œæ•´åº¦ä»…{completeness_ratio:.2%}ï¼Œå¯èƒ½æœ‰å†…å®¹ä¸¢å¤±")
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„æ–‡æœ¬é—æ¼ï¼ˆé€šè¿‡å…³é”®è¯æ£€æŸ¥ï¼‰
            original_keywords = self._extract_keywords(original_text)
            segment_text = ' '.join([seg.get('text', '') for seg in segments])
            segment_keywords = self._extract_keywords(segment_text)
            
            missing_keywords = original_keywords - segment_keywords
            if len(missing_keywords) > len(original_keywords) * 0.2:  # å¦‚æœè¶…è¿‡20%çš„å…³é”®è¯ä¸¢å¤±
                logger.warning(f"å…³é”®è¯å®Œæ•´æ€§æ ¡éªŒå¤±è´¥: ä¸¢å¤±å…³é”®è¯{missing_keywords}")
                return False
            
            logger.info("å†…å®¹å®Œæ•´æ€§æ ¡éªŒé€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"å®Œæ•´æ€§æ ¡éªŒå¼‚å¸¸: {str(e)}")
            return False  # æ ¡éªŒå¼‚å¸¸æ—¶è®¤ä¸ºä¸å®Œæ•´ï¼Œè§¦å‘é‡è¯•
    
    def _extract_keywords(self, text: str) -> set:
        """æå–æ–‡æœ¬ä¸­çš„å…³é”®è¯ç”¨äºå®Œæ•´æ€§æ ¡éªŒ"""
        import re
        
        # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2-4ä¸ªå­—ç¬¦çš„è¯ï¼‰
        chinese_words = set(re.findall(r'[\u4e00-\u9fff]{2,4}', text))
        
        # æå–äººåã€åœ°åç­‰ä¸“æœ‰åè¯ï¼ˆé€šå¸¸åŒ…å«ç‰¹å®šå­—ç¬¦ï¼‰
        proper_nouns = set(re.findall(r'[\u4e00-\u9fff]*[ç‹æå¼ åˆ˜é™ˆæ¨é»„èµµå´å‘¨][\u4e00-\u9fff]*', text))
        
        # æå–å¼•å·å†…çš„å¯¹è¯å…³é”®è¯
        dialogue_keywords = set(re.findall(r'["""]([^"""]{2,10})["""]', text))
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯ï¼Œå–å‰50ä¸ªæœ€é‡è¦çš„
        all_keywords = chinese_words | proper_nouns | dialogue_keywords
        return set(list(all_keywords)[:50])  # é™åˆ¶å…³é”®è¯æ•°é‡ï¼Œæé«˜æ•ˆç‡
    
    def _build_detailed_analysis_prompt(self, text: str) -> str:
        """ğŸ”¥ æ–°å¢ï¼šæ„å»ºæ›´è¯¦ç»†çš„åˆ†ææç¤ºè¯ï¼Œç”¨äºé‡è¯•æ—¶ç¡®ä¿å®Œæ•´æ€§"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡å°è¯´æ–‡æœ¬åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹å°è¯´æ–‡æœ¬ï¼Œç¡®ä¿ä¸é—æ¼ä»»ä½•å†…å®¹ã€‚

**é‡è¦æé†’ï¼šå¿…é¡»åˆ†æå®Œæ•´çš„æ–‡æœ¬å†…å®¹ï¼Œæ¯ä¸ªå¥å­éƒ½è¦åŒ…å«åœ¨ç»“æœä¸­ï¼**

æ–‡æœ¬ï¼š
{text}

è¯¦ç»†åˆ†æè¦æ±‚ï¼š
1. **å®Œæ•´æ€§ç¬¬ä¸€**ï¼šç¡®ä¿æ¯ä¸ªå¥å­ã€æ¯ä¸ªæ®µè½éƒ½è¢«åˆ†æåˆ°
2. **é€å¥åˆ†æ®µ**ï¼šæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰æ ‡ç‚¹ç¬¦å·åˆ†æ®µ
3. **è§’è‰²è¯†åˆ«**ï¼šå‡†ç¡®è¯†åˆ«æ‰€æœ‰è¯´è¯çš„è§’è‰²
4. **å¯¹è¯åˆ†ç¦»**ï¼šå°†"æŸæŸè¯´ï¼š'å†…å®¹'"åˆ†ä¸ºä¸¤æ®µ

åˆ†æ®µç­–ç•¥ï¼š
- æ¯ä¸ªå®Œæ•´çš„å¥å­ä½œä¸ºä¸€ä¸ªsegment
- å¯¹è¯å‰çš„åŠ¨ä½œæè¿°ï¼ˆå¦‚"æ—æ¸Šè¯´ï¼š"ï¼‰å•ç‹¬æˆæ®µï¼Œæ ‡è®°ä¸ºæ—ç™½
- å¼•å·å†…çš„å¯¹è¯å†…å®¹å•ç‹¬æˆæ®µï¼Œæ ‡è®°ä¸ºç›¸åº”è§’è‰²
- å¿ƒç†æ´»åŠ¨æŒ‰åŒæ ·è§„åˆ™å¤„ç†

**ğŸµ å£°éŸ³æè¿°è§„åˆ™**ï¼š
- "å¨‡å–å£°å¸¦ç€æ€’æ„" â†’ æ—ç™½ï¼ˆæè¿°å£°éŸ³ç‰¹å¾ï¼‰
- "ç¬‘å£°ä¼ æ¥" â†’ æ—ç™½ï¼ˆæè¿°å£°éŸ³ç°è±¡ï¼‰
- "è¯éŸ³åˆšè½" â†’ æ—ç™½ï¼ˆæè¿°è¯´è¯çŠ¶æ€ï¼‰
- åªæœ‰å¼•å·å†…çš„å®é™…è¯è¯­æ‰æ˜¯è§’è‰²å¯¹è¯ï¼

**ğŸ“ é—´æ¥å¼•è¿°å¯¹è¯é€šç”¨è§„åˆ™**ï¼š
å½“é‡åˆ°"æŸæŸ[åŠ¨ä½œ]ï¼š'å†…å®¹'"æ ¼å¼æ—¶ï¼Œå¿…é¡»åˆ†ç¦»ï¼š
- åŠ¨ä½œæè¿°éƒ¨åˆ† â†’ æ—ç™½
- å¼•è¿°å†…å®¹éƒ¨åˆ† â†’ ç›¸åº”è§’è‰²
- é€‚ç”¨äºï¼šè¯´é“ã€å†™é“ã€ä¸‹æ—¨ã€ä¼ è¯ã€å‘ŠçŸ¥ã€å‘½ä»¤ã€è¯¢é—®ç­‰æ‰€æœ‰å¼•è¿°å½¢å¼

è¾“å‡ºè¦æ±‚ï¼š
- å¿…é¡»åŒ…å«åŸæ–‡çš„æ¯ä¸ªå­—ç¬¦ï¼ˆé™¤äº†æ ‡ç‚¹ç¬¦å·çš„è°ƒæ•´ï¼‰
- segmentæ•°é‡åº”è¯¥ä¸åŸæ–‡å¥å­æ•°é‡åŸºæœ¬å¯¹åº”
- ä¸èƒ½è·³è¿‡ä»»ä½•å†…å®¹æ®µè½

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
  "segments": [
    {{"order": 1, "text": "å®Œæ•´çš„å¥å­å†…å®¹", "speaker": "è¯´è¯è€…", "text_type": "dialogue/narration/inner_monologue", "confidence": 0.9}}
  ],
  "characters": [
    {{"name": "è§’è‰²å", "frequency": å‡ºç°æ¬¡æ•°, "gender": "male/female/neutral", "personality": "calm/brave/gentle", "personality_description": "æ€§æ ¼æè¿°", "is_main_character": true/false, "confidence": 0.8}}
  ]
}}

**å†æ¬¡å¼ºè°ƒï¼šä¸èƒ½é—æ¼ä»»ä½•æ–‡æœ¬å†…å®¹ï¼æ¯ä¸ªå¥å­éƒ½å¿…é¡»åœ¨segmentsä¸­ä½“ç°ï¼**"""
        
        return prompt

    def _detect_novel_type(self, text: str) -> str:
        """ğŸ†• æ£€æµ‹å°è¯´ç±»å‹ï¼Œä¸ºåç»­åˆ†ææä¾›ä¸Šä¸‹æ–‡"""
        # å–æ–‡æœ¬å‰1000å­—ç¬¦è¿›è¡Œç±»å‹åˆ†æ
        sample_text = text[:1000] if len(text) > 1000 else text
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡å°è¯´ç±»å‹è¯†åˆ«ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å°è¯´æ–‡æœ¬ç‰‡æ®µï¼Œåˆ¤æ–­å…¶æ‰€å±ç±»å‹ã€‚

æ–‡æœ¬ç‰‡æ®µï¼š
{sample_text}

è¯·æ ¹æ®ä»¥ä¸‹ç‰¹å¾åˆ¤æ–­å°è¯´ç±»å‹ï¼š

**å¤ä»£/å¤è£…å°è¯´**ï¼š
- æ—¶ä»£èƒŒæ™¯ï¼šå¤ä»£ä¸­å›½ã€æœä»£ã€çš‡å¸ã€å®˜å‘˜
- è¯­è¨€ç‰¹è‰²ï¼šæ–‡è¨€æ–‡è‰²å½©ã€å¤ä»£ç§°è°“
- å…³é”®è¯ï¼šé™›ä¸‹ã€çš‡ä¸Šã€å…¬ä¸»ã€ç‹çˆ·ã€å¤§äººã€è‡£ã€å¥´å©¢ã€åºœé‚¸ã€æœå»·

**ç°ä»£/éƒ½å¸‚å°è¯´**ï¼š
- æ—¶ä»£èƒŒæ™¯ï¼šç°ä»£ç¤¾ä¼šã€åŸå¸‚ç”Ÿæ´»
- ç§‘æŠ€å…ƒç´ ï¼šæ‰‹æœºã€ç”µè„‘ã€ç½‘ç»œã€æ±½è½¦ã€å…¬å¸ã€åšç‰©é¦†
- å…³é”®è¯ï¼šè€æ¿ã€ç»ç†ã€åŒäº‹ã€æ‰‹æœºã€çŸ­ä¿¡ã€ç”µè¯ã€ç½‘ç»œã€å…¬å¸ã€åšç‰©é¦†ã€å¯¼å¸ˆ

**æ­¦ä¾ /ä»™ä¾ å°è¯´**ï¼š
- æ­¦åŠŸå…ƒç´ ï¼šå†…åŠ›ã€å‰‘æ³•ã€è½»åŠŸã€ä¿®ç‚¼
- æ±Ÿæ¹–èƒŒæ™¯ï¼šé—¨æ´¾ã€æŒé—¨ã€å¼Ÿå­ã€æ±Ÿæ¹–
- å…³é”®è¯ï¼šå¸ˆçˆ¶ã€æŒé—¨ã€å¼Ÿå­ã€å†…åŠ›ã€çœŸæ°”ã€å‰‘æ°”ã€é—¨æ´¾

**ç„å¹»/å¥‡å¹»å°è¯´**ï¼š
- é­”æ³•å…ƒç´ ï¼šæ³•æœ¯ã€é­”æ³•ã€å¼‚èƒ½ã€é­”å…½
- å¼‚ä¸–ç•Œï¼šå¤§é™†ã€ç‹å›½ã€é­”æ³•å¸ˆã€æˆ˜å£«
- å…³é”®è¯ï¼šé­”æ³•ã€æ³•æœ¯ã€é­”å…½ã€å¤§é™†ã€ç‹å›½ã€å¼‚èƒ½
- æ³¨æ„ï¼šå¦‚æœåŒæ—¶åŒ…å«ç°ä»£å…ƒç´ ï¼ˆæ‰‹æœºã€åšç‰©é¦†ã€å¯¼å¸ˆï¼‰ä¼˜å…ˆåˆ¤æ–­ä¸ºç°ä»£å°è¯´

**ç§‘å¹»å°è¯´**ï¼š
- ç§‘æŠ€å…ƒç´ ï¼šæœªæ¥ç§‘æŠ€ã€æœºå™¨äººã€å¤ªç©ºã€æ—¶é—´æ—…è¡Œ
- å…³é”®è¯ï¼šæœºå™¨äººã€å¤–æ˜Ÿäººã€å¤ªç©ºã€æœªæ¥ã€ç§‘æŠ€ã€å®éªŒå®¤

**å†›äº‹/å†å²å°è¯´**ï¼š
- å†›äº‹å…ƒç´ ï¼šæˆ˜äº‰ã€å†›é˜Ÿã€å°†å†›ã€å£«å…µ
- å…³é”®è¯ï¼šå°†å†›ã€å£«å…µã€æˆ˜äº‰ã€å†›é˜Ÿã€æˆ˜åœºã€ä½œæˆ˜

åªéœ€è¦è¾“å‡ºç±»å‹åç§°ï¼Œä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š
- ancientï¼ˆå¤ä»£/å¤è£…ï¼‰
- modernï¼ˆç°ä»£/éƒ½å¸‚ï¼‰
- wuxiaï¼ˆæ­¦ä¾ /ä»™ä¾ ï¼‰
- fantasyï¼ˆç„å¹»/å¥‡å¹»ï¼‰
- scifiï¼ˆç§‘å¹»ï¼‰
- militaryï¼ˆå†›äº‹/å†å²ï¼‰
- unknownï¼ˆæ— æ³•ç¡®å®šï¼‰

åªè¾“å‡ºç±»å‹åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š"""

        try:
            response = self._call_ollama(prompt)
            if response:
                # æå–ç±»å‹åç§°
                novel_type = response.strip().lower()
                if novel_type in ['ancient', 'modern', 'wuxia', 'fantasy', 'scifi', 'military']:
                    self.logger.info(f"æ£€æµ‹åˆ°å°è¯´ç±»å‹: {novel_type}")
                    return novel_type
                else:
                    self.logger.warning(f"æœªè¯†åˆ«çš„å°è¯´ç±»å‹: {novel_type}ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹")
                    return 'unknown'
            else:
                self.logger.error("å°è¯´ç±»å‹æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹")
                return 'unknown'
        except Exception as e:
            self.logger.error(f"å°è¯´ç±»å‹æ£€æµ‹å¼‚å¸¸: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹")
            return 'unknown'

    def _build_type_aware_analysis_prompt(self, text: str, novel_type: str) -> str:
        """ğŸ†• åŸºäºå°è¯´ç±»å‹æ„å»ºä¸“é—¨çš„åˆ†ææç¤ºè¯"""
        
        # åŸºç¡€åˆ†æè¦æ±‚
        base_requirements = """
åˆ†æè¦æ±‚ï¼š
1. è¯†åˆ«æ‰€æœ‰è¯´è¯çš„è§’è‰²ï¼ˆåŒ…æ‹¬æ—ç™½ï¼‰
2. å°†æ–‡æœ¬æŒ‰å¥å­åˆ†æ®µï¼Œæ¯æ®µæ ‡è®°æ­£ç¡®çš„è¯´è¯è€…
3. **ğŸš¨ ç»å¯¹å¼ºåˆ¶è¦æ±‚**ï¼šå¿…é¡»ä¸¥æ ¼åˆ†ç¦»æ‰€æœ‰æ··åˆæ–‡æœ¬

ğŸš¨ **å¼ºåˆ¶åˆ†ç¦»è§„åˆ™ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰**ï¼š
â— ä»»ä½•åŒ…å«"è§’è‰²å + åŠ¨ä½œ + å†’å· + å¼•å·å†…å®¹"çš„æ–‡æœ¬éƒ½å¿…é¡»åˆ†ä¸ºä¸¤æ®µï¼š
- "é¡¹ç¾½å†·ç¬‘ä¸€å£°ï¼š"ä½ åˆæ˜¯ä½•äººï¼Ÿ"" â†’ å¿…é¡»åˆ†ä¸ºï¼š
  ç¬¬ä¸€æ®µï¼š"é¡¹ç¾½å†·ç¬‘ä¸€å£°ï¼š" â†’ è¯´è¯è€…ï¼šæ—ç™½
  ç¬¬äºŒæ®µï¼š"ä½ åˆæ˜¯ä½•äººï¼Ÿ" â†’ è¯´è¯è€…ï¼šé¡¹ç¾½

â— ä»»ä½•åŒ…å«"å¼•å·å†…å®¹ + è§’è‰²åŠ¨ä½œæè¿°"çš„æ–‡æœ¬éƒ½å¿…é¡»åˆ†ä¸ºä¸¤æ®µï¼š
- ""ä½•äººåœ¨æ­¤ï¼Ÿ" å°†é¢†å‹’é©¬ï¼Œé•¿æªç›´æŒ‡ä»–å’½å–‰ã€‚" â†’ å¿…é¡»åˆ†ä¸ºï¼š
  ç¬¬ä¸€æ®µï¼š"ä½•äººåœ¨æ­¤ï¼Ÿ" â†’ è¯´è¯è€…ï¼šå°†é¢†
  ç¬¬äºŒæ®µï¼š"å°†é¢†å‹’é©¬ï¼Œé•¿æªç›´æŒ‡ä»–å’½å–‰ã€‚" â†’ è¯´è¯è€…ï¼šæ—ç™½

â— ä»»ä½•é€šè®¯ã€æ¶ˆæ¯ã€ä¼ è¯ç±»æ–‡æœ¬éƒ½å¿…é¡»åˆ†ç¦»ï¼š
- "æ˜¯å¯¼å¸ˆå‘æ¥çš„æ¶ˆæ¯ï¼š"æ–°å‡ºåœŸçš„æœªå¤®å®«æ®‹ç®€ï¼Œé€Ÿæ¥ã€‚"" â†’ å¿…é¡»åˆ†ä¸ºï¼š
  ç¬¬ä¸€æ®µï¼š"æ˜¯å¯¼å¸ˆå‘æ¥çš„æ¶ˆæ¯ï¼š" â†’ è¯´è¯è€…ï¼šæ—ç™½
  ç¬¬äºŒæ®µï¼š"æ–°å‡ºåœŸçš„æœªå¤®å®«æ®‹ç®€ï¼Œé€Ÿæ¥ã€‚" â†’ è¯´è¯è€…ï¼šå¯¼å¸ˆ

ğŸ¯ **åˆ†ç¦»åˆ¤æ–­æ ‡å‡†**ï¼š
- æè¿°æ€§æ–‡å­—ï¼ˆåŠ¨ä½œã€åœºæ™¯ã€æ¥æºè¯´æ˜ï¼‰= æ—ç™½
- å¼•å·å†…çš„å®é™…è¯è¯­å†…å®¹ = å¯¹åº”è§’è‰²å‘è¨€
- ç»ä¸å…è®¸å°†æ··åˆå†…å®¹å½’ä¸ºå•ä¸€è¯´è¯è€…ï¼"""

        # æ ¹æ®å°è¯´ç±»å‹æ·»åŠ ä¸“é—¨çš„è§„åˆ™
        type_specific_rules = ""
        
        if novel_type == 'ancient':
            type_specific_rules = """
**ğŸ›ï¸ å¤ä»£å°è¯´ä¸“é—¨è§„åˆ™**ï¼š
- é—´æ¥å¼•è¿°ï¼šçš‡å¸ä¸‹æ—¨ã€ä¼ æ—¨ã€åœ£æ—¨ã€è¯ä¹¦ã€å¯†ä¿¡ç­‰
  ç¤ºä¾‹ï¼š"çš‡å¸ä¸‹æ—¨ï¼š'å³åˆ»ç­å¸ˆå›æœã€‚'" â†’ åˆ†ç¦»ä¸ºæ—ç™½æè¿° + çš‡å¸è¯è¯­
- å¤ä»£ç§°è°“ï¼šé™›ä¸‹ã€çš‡ä¸Šã€å…¬ä¸»ã€ç‹çˆ·ã€å¤§äººã€è‡£ç­‰è¦å‡†ç¡®è¯†åˆ«
- æ–‡è¨€æ–‡å¯¹è¯ï¼šæ³¨æ„"æ›°"ã€"äº‘"ã€"é“"ç­‰å¤ä»£å¯¹è¯åŠ¨è¯
- ä¹¦ä¿¡ä¼ è¯ï¼šä¿¡ä¸­å†™é“ã€å¯†æŠ¥ã€ä¼ ä»¤ç­‰è¦åˆ†ç¦»åŠ¨ä½œå’Œå†…å®¹"""
            
        elif novel_type == 'modern':
            type_specific_rules = """
**ğŸ™ï¸ ç°ä»£å°è¯´ä¸“é—¨è§„åˆ™**ï¼š
- ç°ä»£é€šè®¯åˆ†ç¦»ï¼ˆé‡ç‚¹ï¼‰ï¼š
  * "å¯¼å¸ˆå‘æ¥çš„æ¶ˆæ¯ï¼š'å†…å®¹'" â†’ ä¸¤æ®µï¼š
    ç¬¬ä¸€æ®µï¼š"å¯¼å¸ˆå‘æ¥çš„æ¶ˆæ¯ï¼š" â†’ æ—ç™½ï¼ˆæè¿°æ”¶åˆ°æ¶ˆæ¯ï¼‰
    ç¬¬äºŒæ®µï¼š"å†…å®¹" â†’ å¯¼å¸ˆï¼ˆæ¶ˆæ¯å†…å®¹ï¼‰
  * "ä»–çœ‹äº†ä¸€çœ¼å±å¹•ï¼Œæ˜¯å¯¼å¸ˆå‘æ¥çš„æ¶ˆæ¯ï¼š'å†…å®¹'" â†’ ä¸‰æ®µï¼š
    ç¬¬ä¸€æ®µï¼š"ä»–çœ‹äº†ä¸€çœ¼å±å¹•ï¼Œ" â†’ æ—ç™½ï¼ˆåŠ¨ä½œæè¿°ï¼‰
    ç¬¬äºŒæ®µï¼š"æ˜¯å¯¼å¸ˆå‘æ¥çš„æ¶ˆæ¯ï¼š" â†’ æ—ç™½ï¼ˆæ¶ˆæ¯æ¥æºæè¿°ï¼‰
    ç¬¬ä¸‰æ®µï¼š"å†…å®¹" â†’ å¯¼å¸ˆï¼ˆæ¶ˆæ¯å†…å®¹ï¼‰
  * "è€æ¿åœ¨ç”µè¯é‡Œè¯´ï¼š'å¼€ä¼šäº†ã€‚'" â†’ ä¸¤æ®µï¼š
    ç¬¬ä¸€æ®µï¼š"è€æ¿åœ¨ç”µè¯é‡Œè¯´ï¼š" â†’ æ—ç™½ï¼ˆé€šè¯æè¿°ï¼‰
    ç¬¬äºŒæ®µï¼š"å¼€ä¼šäº†ã€‚" â†’ è€æ¿ï¼ˆç”µè¯å†…å®¹ï¼‰

- ç°ä»£é€šè®¯å…³é”®è¯ï¼šæ‰‹æœºã€çŸ­ä¿¡ã€ç”µè¯ã€å¾®ä¿¡ã€QQã€é‚®ä»¶ã€æ¶ˆæ¯ã€è¯­éŸ³
- èŒåœºç§°è°“ï¼šè€æ¿ã€ç»ç†ã€åŒäº‹ã€å¯¼å¸ˆã€è€å¸ˆã€é¢†å¯¼
- å¿…é¡»ä¸¥æ ¼åˆ†ç¦»æè¿°æ€§æ–‡å­—å’Œå®é™…é€šè®¯å†…å®¹ï¼"""
            
        elif novel_type == 'wuxia':
            type_specific_rules = """
**âš”ï¸ æ­¦ä¾ å°è¯´ä¸“é—¨è§„åˆ™**ï¼š
- æ±Ÿæ¹–ç§°è°“ï¼šæŒé—¨ã€å¸ˆçˆ¶ã€å¸ˆå…„ã€å¼Ÿå­ã€ä¾ å®¢ç­‰
- ä¼ éŸ³å…¥å¯†ï¼šå†…åŠ›ä¼ éŸ³ã€ç¥è¯†ä¼ è¯ç­‰ç‰¹æ®Šå¯¹è¯æ–¹å¼
- é—¨æ´¾è§„åˆ™ï¼šå¸ˆé—¨è§„çŸ©ã€æ±Ÿæ¹–è§„çŸ©ç›¸å…³çš„å¯¹è¯
- æ­¦åŠŸæ‹›å¼ï¼šæ³¨æ„åŒºåˆ†æ‹›å¼åç§°ï¼ˆæ—ç™½ï¼‰å’Œå®é™…å¯¹è¯"""
            
        elif novel_type == 'fantasy':
            type_specific_rules = """
**ğŸ”® ç„å¹»å°è¯´ä¸“é—¨è§„åˆ™**ï¼š
- é­”æ³•é€šè®¯ï¼šæ³•æœ¯ä¼ éŸ³ã€é­”æ³•é€šè¯ã€ç²¾ç¥é“¾æ¥ç­‰
- å¼‚ä¸–ç•Œç§°è°“ï¼šæ³•å¸ˆã€æˆ˜å£«ã€é­”å¯¼å¸ˆã€å›½ç‹ã€è´µæ—ç­‰
- é­”æ³•å…ƒç´ ï¼šå’’è¯­åŸå”±ã€æ³•æœ¯é‡Šæ”¾è¦åŒºåˆ†äºå¯¹è¯
- ç§æ—å¯¹è¯ï¼šç²¾çµã€çŸ®äººã€é¾™æ—ç­‰ä¸åŒç§æ—çš„å¯¹è¯ç‰¹ç‚¹

**ğŸŒŸ ç°ä»£ç©¿è¶Šæ–‡ç‰¹æ®Šå¤„ç†**ï¼š
å¦‚æœæ–‡æœ¬åŒ…å«ç°ä»£å…ƒç´ ï¼ˆæ‰‹æœºã€åšç‰©é¦†ã€å¯¼å¸ˆç­‰ï¼‰ï¼ŒåŒæ—¶åº”ç”¨ç°ä»£é€šè®¯åˆ†ç¦»è§„åˆ™ï¼š
- "å¯¼å¸ˆå‘æ¥çš„æ¶ˆæ¯ï¼š'å†…å®¹'" â†’ åˆ†ç¦»ä¸ºæ—ç™½æè¿° + å¯¼å¸ˆè¯è¯­
- "æ‰‹æœºéœ‡åŠ¨"ã€"æ”¶åˆ°çŸ­ä¿¡"ç­‰ç°ä»£é€šè®¯åœºæ™¯è¦æ­£ç¡®åˆ†ç¦»
- ç°ä»£ç§°è°“ï¼ˆå¯¼å¸ˆã€è€å¸ˆã€åŒäº‹ï¼‰è¦æ­£ç¡®è¯†åˆ«"""
            
        elif novel_type == 'scifi':
            type_specific_rules = """
**ğŸš€ ç§‘å¹»å°è¯´ä¸“é—¨è§„åˆ™**ï¼š
- ç§‘æŠ€é€šè®¯ï¼šå…¨æ¯é€šè¯ã€é‡å­é€šè®¯ã€è„‘æ³¢ä¼ è¾“ç­‰
- æœªæ¥ç§°è°“ï¼šæŒ‡æŒ¥å®˜ã€èˆ°é•¿ã€åšå£«ã€å®éªŒå‘˜ç­‰
- AIå¯¹è¯ï¼šäººå·¥æ™ºèƒ½ã€æœºå™¨äººçš„å¯¹è¯è¦æ­£ç¡®è¯†åˆ«
- ç§‘æŠ€æœ¯è¯­ï¼šåŒºåˆ†ç§‘æŠ€æè¿°ï¼ˆæ—ç™½ï¼‰å’Œå®é™…å¯¹è¯"""
            
        elif novel_type == 'military':
            type_specific_rules = """
**ğŸ–ï¸ å†›äº‹å°è¯´ä¸“é—¨è§„åˆ™**ï¼š
- å†›äº‹é€šè®¯ï¼šæ— çº¿ç”µã€ä½œæˆ˜æŒ‡ä»¤ã€å†›ä»¤ä¼ è¾¾ç­‰
- å†›äº‹ç§°è°“ï¼šå°†å†›ã€æŒ‡æŒ¥å®˜ã€å£«å…µã€å‚è°‹ç­‰
- ä½œæˆ˜æŒ‡ä»¤ï¼šå‘½ä»¤ä¸‹è¾¾ã€æˆ˜æŠ¥æ±‡æŠ¥è¦åˆ†ç¦»åŠ¨ä½œå’Œå†…å®¹
- å†›äº‹æœ¯è¯­ï¼šåŒºåˆ†æˆ˜æœ¯æè¿°ï¼ˆæ—ç™½ï¼‰å’Œå®é™…å¯¹è¯"""
        else:
            type_specific_rules = """
**ğŸ” é€šç”¨è§„åˆ™**ï¼š
- é—´æ¥å¼•è¿°ï¼šä»»ä½•"æŸæŸ[åŠ¨ä½œ]ï¼š'å†…å®¹'"æ ¼å¼éƒ½è¦åˆ†ç¦»
- ç°ä»£é€šè®¯ï¼šç”µè¯ã€çŸ­ä¿¡ã€é‚®ä»¶ç­‰è¦åˆ†ç¦»æè¿°å’Œå†…å®¹
- ä¼ ç»Ÿå¯¹è¯ï¼šæ³¨æ„å„ç§å¯¹è¯å¼•å¯¼è¯çš„æ­£ç¡®åˆ†ç¦»"""

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡å°è¯´æ–‡æœ¬åˆ†æä¸“å®¶ã€‚å½“å‰åˆ†æçš„æ˜¯**{novel_type}ç±»å‹å°è¯´**ï¼Œè¯·é’ˆå¯¹æ€§åœ°åˆ†æä»¥ä¸‹æ–‡æœ¬ã€‚

æ–‡æœ¬ï¼š
{text[:4000] if len(text) > 4000 else text}

{base_requirements}

{type_specific_rules}

**ğŸµ å£°éŸ³æè¿°ç‰¹æ®Šè§„åˆ™ï¼ˆé€šç”¨ï¼‰**ï¼š
- æ‰€æœ‰æè¿°å£°éŸ³ã€è¯­è°ƒã€éŸ³è‰²çš„æ–‡å­—éƒ½æ˜¯æ—ç™½ï¼Œä¸æ˜¯å¯¹è¯
- å…³é”®è¯è¯†åˆ«ï¼šå‡¡æ˜¯åŒ…å«"å£°"ã€"éŸ³"ã€"å“"ã€"ä¼ æ¥"ã€"å“èµ·"ç­‰æè¿°å£°éŸ³çš„è¯æ±‡ï¼Œéƒ½æ˜¯æ—ç™½å™è¿°

**ğŸ§  å¿ƒç†æå†™ç‰¹æ®Šè§„åˆ™ï¼ˆé€šç”¨ï¼‰**ï¼š
- "ä»–å¿ƒé‡Œæƒ³"ã€"å¥¹æš—è‡ªç¢ç£¨"åçš„å¼•å·å†…å®¹æ˜¯è¯¥è§’è‰²çš„å¿ƒç†æ´»åŠ¨
- å¿ƒç†æå†™å…³é”®è¯ï¼šå¿ƒé‡Œæƒ³ã€å¿ƒæƒ³ã€æš—æƒ³ã€æš—é“ã€å¿ƒé“ã€ç¢ç£¨ã€æ€è€ƒã€æƒ³åˆ°ç­‰

**ğŸ¯ è§’è‰²åç§°ä¸€è‡´æ€§è¦æ±‚ï¼ˆæ ¸å¿ƒï¼‰**ï¼š
- åŒä¸€è§’è‰²å¿…é¡»ä½¿ç”¨ç»Ÿä¸€çš„åç§°ï¼Œé¿å…å¤šç§ç§°å‘¼
- ä¼˜å…ˆä½¿ç”¨å…·ä½“äººåï¼Œé¿å…æ³›æŒ‡ç§°å‘¼

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
  "novel_type": "{novel_type}",
  "segments": [
    {{"order": 1, "text": "æ–‡æœ¬å†…å®¹", "speaker": "è¯´è¯è€…", "text_type": "dialogue/narration/inner_monologue", "confidence": 0.9}}
  ],
  "characters": [
    {{"name": "è§’è‰²å", "frequency": å‡ºç°æ¬¡æ•°, "gender": "male/female/neutral", "personality": "calm/brave/gentle", "personality_description": "æ€§æ ¼æè¿°", "is_main_character": true/false, "confidence": 0.8}}
  ]
}}

åªè¾“å‡ºJSONï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—"""
        
        return prompt

    def _build_comprehensive_analysis_prompt(self, text: str) -> str:
        """æ„å»ºç»¼åˆåˆ†ææç¤ºè¯ - ç°åœ¨æ”¯æŒç±»å‹æ„ŸçŸ¥"""
        # ğŸ†• ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹å°è¯´ç±»å‹
        novel_type = self._detect_novel_type(text)
        
        # ğŸ†• ç¬¬äºŒæ­¥ï¼šåŸºäºç±»å‹æ„å»ºä¸“é—¨æç¤ºè¯
        return self._build_type_aware_analysis_prompt(text, novel_type)

    def _call_ollama(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Ollama API"""
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.2,
                    "top_p": 0.8,
                    "max_tokens": 8000,    # ğŸ”¥ ä¿®å¤ï¼šå¢åŠ åˆ°8000ï¼Œé¿å…è¾“å‡ºæˆªæ–­
                    "num_ctx": 8192        # ğŸ”¥ ä¿®å¤ï¼šå¢åŠ åˆ°8192ï¼Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡
                }
            }
            
            response = requests.post(
                self.api_url,
                json=payload,
                timeout=300  # ğŸ”¥ ä¿®å¤ï¼šå¢åŠ è¶…æ—¶æ—¶é—´åˆ°5åˆ†é’Ÿï¼Œé¿å…é•¿æ–‡æœ¬åˆ†æè¶…æ—¶
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                logger.error(f"Ollama APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return None
                
        except requests.exceptions.Timeout:
            logger.error("Ollama APIè°ƒç”¨è¶…æ—¶")
            return None
        except Exception as e:
            logger.error(f"Ollama APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return None
    
    def _parse_comprehensive_response(self, response: str) -> Dict:
        """è§£æOllamaè¿”å›çš„ç»¼åˆåˆ†æç»“æœ"""
        try:
            # è®°å½•åŸå§‹å“åº”ç”¨äºè°ƒè¯•
            logger.info(f"OllamaåŸå§‹å“åº”: {response[:500]}...")
            
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                data = json.loads(json_str)
                
                logger.info(f"è§£æçš„JSONæ•°æ®: {json.dumps(data, ensure_ascii=False, indent=2)}")
                
                # å¤„ç†segments
                segments = []
                for i, seg_data in enumerate(data.get('segments', [])):
                    # æ”¯æŒæ–°çš„text_type: inner_monologue
                    text_type = seg_data.get('text_type', 'narration')
                    if text_type not in ['dialogue', 'narration', 'inner_monologue']:
                        text_type = 'narration'  # é»˜è®¤ä¸ºæ—ç™½
                        
                    # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†ç©ºçš„speakerå­—æ®µ
                    speaker = seg_data.get('speaker', '').strip()
                    if not speaker:  # å¤„ç†ç©ºå­—ç¬¦ä¸²ã€Noneã€æˆ–åªæœ‰ç©ºæ ¼çš„æƒ…å†µ
                        if text_type in ['narration', 'inner_monologue']:
                            speaker = 'æ—ç™½'
                        else:
                            speaker = 'æœªçŸ¥è§’è‰²'
                    
                    segments.append({
                        'order': seg_data.get('order', i + 1),
                        'text': seg_data.get('text', ''),
                        'speaker': speaker,
                        'confidence': seg_data.get('confidence', 0.8),
                        'detection_rule': 'ollama_ai',
                        'text_type': text_type
                    })
                
                # å¤„ç†characters
                characters = []
                for char_data in data.get('characters', []):
                    if isinstance(char_data, dict) and 'name' in char_data:
                        name = char_data.get('name', '')
                        if name and len(name) >= 2:
                            characters.append({
                                'name': name,
                                'frequency': char_data.get('frequency', 1),
                                'character_trait': {
                                    'trait': char_data.get('personality', 'calm'),
                                    'confidence': char_data.get('confidence', 0.8),
                                    'description': char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ')
                                },
                                'first_appearance': 1,
                                'is_main_character': char_data.get('is_main_character', False),
                                'recommended_config': {
                                    'gender': self._infer_gender_smart(name, char_data.get('gender', 'unknown')),
                                    'personality': char_data.get('personality', 'calm'),
                                    'personality_description': char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ'),
                                    'personality_confidence': char_data.get('confidence', 0.8),
                                    'description': f"{name}ï¼Œ{self._infer_gender_smart(name, char_data.get('gender', 'unknown'))}è§’è‰²ï¼Œ{char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ')}ï¼Œåœ¨æ–‡æœ¬ä¸­å‡ºç°{char_data.get('frequency', 1)}æ¬¡ã€‚",
                                    'recommended_tts_params': self._get_tts_params(char_data.get('personality', 'calm')),
                                    'voice_type': f"{self._infer_gender_smart(name, char_data.get('gender', 'unknown'))}_{char_data.get('personality', 'calm')}",
                                    'color': self._get_character_color(char_data.get('personality', 'calm'))
                                }
                            })
                
                return {
                    'segments': segments,
                    'characters': characters
                }
            
            else:
                logger.error("æ— æ³•ä»Ollamaå“åº”ä¸­æå–JSONæ•°æ®")
                return {'segments': [], 'characters': []}
                
        except json.JSONDecodeError as e:
            logger.error(f"è§£æOllama JSONå“åº”å¤±è´¥: {str(e)}")
            logger.error(f"åŸå§‹å“åº”: {response}")
            return {'segments': [], 'characters': []}
        except Exception as e:
            logger.error(f"å¤„ç†Ollamaå“åº”å¼‚å¸¸: {str(e)}")
            return {'segments': [], 'characters': []}

    def _get_tts_params(self, personality: str) -> Dict:
        """æ ¹æ®æ€§æ ¼è·å–TTSå‚æ•°"""
        params_map = {
            'gentle': {'time_step': 35, 'p_w': 1.2, 't_w': 2.8},
            'fierce': {'time_step': 28, 'p_w': 1.6, 't_w': 3.2},
            'calm': {'time_step': 32, 'p_w': 1.4, 't_w': 3.0},
            'lively': {'time_step': 30, 'p_w': 1.3, 't_w': 2.9},
            'wise': {'time_step': 34, 'p_w': 1.3, 't_w': 3.1},
            'brave': {'time_step': 29, 'p_w': 1.5, 't_w': 3.1}
        }
        return params_map.get(personality, {'time_step': 32, 'p_w': 1.4, 't_w': 3.0})
    
    def _get_character_color(self, personality: str) -> str:
        """æ ¹æ®æ€§æ ¼è·å–è§’è‰²é¢œè‰²"""
        color_map = {
            'gentle': '#FFB6C1',  # æµ…ç²‰è‰²
            'fierce': '#FF6347',  # ç•ªèŒ„çº¢
            'calm': '#06b6d4',   # é’è‰²
            'lively': '#32CD32', # ç»¿è‰²
            'wise': '#9370DB',   # ç´«è‰²
            'brave': '#FF8C00'   # æ©™è‰²
        }
        return color_map.get(personality, '#06b6d4')
    
    def _infer_gender_smart(self, name: str, ai_gender: str) -> str:
        """æ™ºèƒ½æ¨æ–­è§’è‰²æ€§åˆ« - å®Œå…¨ä¾èµ–AIåˆ¤æ–­ï¼Œç§»é™¤ç¡¬ç¼–ç """
        # å¦‚æœAIå·²ç»æ­£ç¡®è¯†åˆ«äº†æ€§åˆ«ï¼Œç›´æ¥ä½¿ç”¨
        if ai_gender and ai_gender in ['male', 'female', 'neutral']:
            return ai_gender
        
        # å¦‚æœAIæ²¡æœ‰è¿”å›æ€§åˆ«ä¿¡æ¯ï¼Œè°ƒç”¨ä¸“é—¨çš„æ€§åˆ«è¯†åˆ«AI
        try:
            gender = self._ai_infer_gender(name)
            if gender in ['male', 'female', 'neutral']:
                logger.info(f"AIæ¨æ–­è§’è‰² '{name}' æ€§åˆ«: {gender}")
                return gender
        except Exception as e:
            logger.warning(f"AIæ€§åˆ«æ¨æ–­å¤±è´¥: {str(e)}")
        
        # é»˜è®¤è¿”å›unknownï¼Œè®©ç”¨æˆ·æ‰‹åŠ¨é€‰æ‹©
        logger.warning(f"æ— æ³•æ¨æ–­è§’è‰² '{name}' çš„æ€§åˆ«")
        return 'unknown'
    
    def _ai_infer_gender(self, character_name: str) -> str:
        """ä½¿ç”¨AIæ¨æ–­è§’è‰²æ€§åˆ«"""
        try:
            prompt = f"""è¯·åˆ¤æ–­è§’è‰² "{character_name}" çš„æ€§åˆ«ã€‚

åˆ¤æ–­è§„åˆ™ï¼š
1. åŸºäºä¸­æ–‡å§“åçš„å¸¸è§ç‰¹å¾
2. åŸºäºæ–‡å­¦ä½œå“ä¸­çš„è§’è‰²è®¾å®š
3. åŸºäºç§°è°“ã€å¤´è¡”çš„è¯­ä¹‰å«ä¹‰

è¿”å›æ ¼å¼ï¼ˆåªè¿”å›ä¸€ä¸ªè¯ï¼‰ï¼š
- maleï¼ˆç”·æ€§ï¼‰
- femaleï¼ˆå¥³æ€§ï¼‰  
- neutralï¼ˆä¸­æ€§ï¼Œå¦‚æ—ç™½ã€å™è¿°è€…ï¼‰

è§’è‰²åï¼š{character_name}
æ€§åˆ«ï¼š"""

            response = self._call_ollama(prompt)
            if response:
                # æå–æ€§åˆ«åˆ¤æ–­
                gender = response.strip().lower()
                if 'male' in gender and 'female' not in gender:
                    return 'male'
                elif 'female' in gender:
                    return 'female'
                elif 'neutral' in gender:
                    return 'neutral'
            
            return 'unknown'
            
        except Exception as e:
            logger.error(f"AIæ€§åˆ«æ¨æ–­å¼‚å¸¸: {str(e)}")
            return 'unknown' 