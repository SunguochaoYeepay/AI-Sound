"""
Ollama AIè§’è‰²æ£€æµ‹å™¨
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½è§’è‰²è¯†åˆ«å’Œåˆ†æ
"""

import json
import logging
import os
import requests
import time
from typing import Dict, List, Optional

logger = logging.getLogger(__name__)


class OllamaCharacterDetector:
    """åŸºäºOllamaçš„è§’è‰²æ£€æµ‹å™¨ - ä¼˜åŒ–ç‰ˆä½¿ç”¨14Bæ¨¡å‹"""
    
    def __init__(self, model_name: str = "auto", ollama_url: str = None):
        self.base_model_name = model_name
        self.model_name = model_name  # åˆå§‹åŒ–æ—¶è®¾ç½®é»˜è®¤å€¼
        self.api_url = ollama_url or "http://localhost:11434/api/generate"
        self.logger = logging.getLogger(__name__)
        
        # è¯»å–ç³»ç»Ÿè®¾ç½®
        self.settings = self._load_system_settings()
        
        # æ™ºèƒ½æ¨¡å‹é€‰æ‹©ç­–ç•¥
        self.model_selection_strategy = {
            "short_text_threshold": 2000,  # çŸ­æ–‡æœ¬é˜ˆå€¼
            "long_text_threshold": 6000,   # é•¿æ–‡æœ¬é˜ˆå€¼
            "short_model": "qwen2.5:14b",  # çŸ­æ–‡æœ¬ä½¿ç”¨14Bé«˜ç²¾åº¦æ¨¡å‹
            "long_model": "qwen2.5:7b"     # é•¿æ–‡æœ¬ä½¿ç”¨7Bé«˜é€Ÿæ¨¡å‹
        }
        
        mode = "å¿«é€Ÿæ¨¡å¼" if self.settings.get("fastModeEnabled", True) else "æ ‡å‡†æ¨¡å¼"
        self.logger.info(f"ğŸš€ OllamaCharacterDetector åˆå§‹åŒ–å®Œæˆï¼Œåˆ†ææ¨¡å¼: {mode}")
    
    def _load_system_settings(self) -> dict:
        """åŠ è½½ç³»ç»Ÿè®¾ç½®"""
        try:
            import os
            config_file = os.path.join(
                os.path.dirname(os.path.dirname(__file__)),
                "config", "data", "system_settings.json"
            )
            if os.path.exists(config_file):
                import json
                with open(config_file, 'r', encoding='utf-8') as f:
                    settings = json.load(f)
                    return settings.get("ai", {})
        except Exception as e:
            self.logger.warning(f"æ— æ³•åŠ è½½ç³»ç»Ÿè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        
        # é»˜è®¤å¿«é€Ÿæ¨¡å¼è®¾ç½®
        return {
            "fastModeEnabled": True,
            "analysisTimeout": 60,
            "enableSecondaryCheck": False
        }

    def _select_optimal_model(self, text: str) -> str:
        """ğŸ¯ æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼šæ ¹æ®æ–‡æœ¬é•¿åº¦é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
        if self.base_model_name != "auto":
            # å¦‚æœç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨
            selected_model = self.base_model_name
            self.logger.info(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šæ¨¡å‹: {selected_model}")
        else:
            text_length = len(text)
            strategy = self.model_selection_strategy
            
            if text_length <= strategy["short_text_threshold"]:
                # çŸ­æ–‡æœ¬ï¼šä½¿ç”¨14Bé«˜ç²¾åº¦æ¨¡å‹
                selected_model = strategy["short_model"]
                self.logger.info(f"ğŸ“ çŸ­æ–‡æœ¬({text_length}å­—ç¬¦) â†’ ä½¿ç”¨é«˜ç²¾åº¦æ¨¡å‹: {selected_model}")
            elif text_length >= strategy["long_text_threshold"]:
                # é•¿æ–‡æœ¬ï¼šä½¿ç”¨7Bé«˜é€Ÿæ¨¡å‹
                selected_model = strategy["long_model"]
                self.logger.info(f"ğŸ“„ é•¿æ–‡æœ¬({text_length}å­—ç¬¦) â†’ ä½¿ç”¨é«˜é€Ÿæ¨¡å‹: {selected_model}")
            else:
                # ä¸­ç­‰æ–‡æœ¬ï¼šä½¿ç”¨14Bæ¨¡å‹ä½†è°ƒæ•´å‚æ•°
                selected_model = strategy["short_model"]
                self.logger.info(f"ğŸ“ ä¸­ç­‰æ–‡æœ¬({text_length}å­—ç¬¦) â†’ ä½¿ç”¨å¹³è¡¡æ¨¡å‹: {selected_model}")
        
        # ä¿å­˜é€‰æ‹©çš„æ¨¡å‹åç§°åˆ°å®ä¾‹å˜é‡
        self.model_name = selected_model
        return selected_model

    def _get_model_options(self) -> Dict:
        """ğŸ¯ è·å–å¹³è¡¡çš„åˆ†æå‚æ•° - ä¿®å¤å†…å®¹ä¸¢å¤±é—®é¢˜"""
        if self.model_name == "qwen2.5:14b":
            # 14Bæ¨¡å‹ï¼šç¡®ä¿è¾“å‡ºå®Œæ•´æ€§
            return {
                "temperature": 0.2,    # ä½æ¸©åº¦ç¡®ä¿ç¨³å®šæ€§
                "top_p": 0.9,          # æ ‡å‡†é‡‡æ ·
                "max_tokens": 8000,    # è¶³å¤Ÿçš„è¾“å‡ºé•¿åº¦ç¡®ä¿å®Œæ•´æ€§
                "num_ctx": 8192        # è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡é•¿åº¦
            }
        else:
            # 7Bæ¨¡å‹ï¼šå¹³è¡¡é€Ÿåº¦å’Œå®Œæ•´æ€§
            return {
                "temperature": 0.3,    # é€‚ä¸­æ¸©åº¦
                "top_p": 0.9,          # å¿«é€Ÿé‡‡æ ·
                "max_tokens": 6000,    # å……è¶³çš„è¾“å‡ºé•¿åº¦
                "num_ctx": 6144        # å……è¶³çš„ä¸Šä¸‹æ–‡
            }

    def _smart_chunk_text(self, text: str, max_chunk_size: int = 3000) -> List[Dict]:
        """ğŸš€ æ™ºèƒ½åˆ†å—ï¼šæŒ‰æ®µè½å’Œå¥å­è¾¹ç•Œåˆ†å—ï¼Œé¿å…æˆªæ–­"""
        import re
        
        # å¦‚æœæ–‡æœ¬è¾ƒçŸ­ï¼Œä¸éœ€è¦åˆ†å—
        if len(text) <= max_chunk_size:
            return [{"chunk_id": 0, "text": text, "start_pos": 0, "end_pos": len(text)}]
        
        logger.info(f"æ–‡æœ¬è¿‡é•¿({len(text)}å­—ç¬¦)ï¼Œå¼€å§‹æ™ºèƒ½åˆ†å—(æœ€å¤§{max_chunk_size}å­—ç¬¦/å—)")
        
        chunks = []
        chunk_id = 0
        
        # é¦–å…ˆæŒ‰åŒæ¢è¡Œç¬¦åˆ†æ®µ
        paragraphs = text.split('\n\n')
        current_chunk = ""
        current_start = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            # å¦‚æœåŠ å…¥å½“å‰æ®µè½åè¶…è¿‡é™åˆ¶ï¼Œå…ˆä¿å­˜å½“å‰å—
            if len(current_chunk) + len(para) > max_chunk_size and current_chunk:
                # ä¿å­˜å½“å‰å—
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": current_chunk.strip(),
                    "start_pos": current_start,
                    "end_pos": current_start + len(current_chunk)
                })
                chunk_id += 1
                current_start += len(current_chunk)
                current_chunk = para + "\n\n"
            else:
                current_chunk += para + "\n\n"
        
        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append({
                "chunk_id": chunk_id,
                "text": current_chunk.strip(),
                "start_pos": current_start,
                "end_pos": current_start + len(current_chunk)
            })
        
        # å¦‚æœæŸä¸ªå—ä»ç„¶è¿‡å¤§ï¼ŒæŒ‰å¥å­è¿›ä¸€æ­¥åˆ†å—
        final_chunks = []
        for chunk in chunks:
            if len(chunk["text"]) > max_chunk_size:
                sub_chunks = self._split_by_sentences(chunk["text"], max_chunk_size, chunk["start_pos"])
                final_chunks.extend(sub_chunks)
            else:
                final_chunks.append(chunk)
        
        logger.info(f"æ™ºèƒ½åˆ†å—å®Œæˆï¼š{len(text)}å­—ç¬¦ â†’ {len(final_chunks)}å—ï¼Œå¹³å‡{len(text)//len(final_chunks)}å­—ç¬¦/å—")
        return final_chunks
    
    def _split_by_sentences(self, text: str, max_size: int, start_offset: int = 0) -> List[Dict]:
        """æŒ‰å¥å­è¾¹ç•Œè¿›ä¸€æ­¥åˆ†å—"""
        import re
        
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å¥
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ][""]?)', text)
        chunks = []
        chunk_id = len(chunks)
        current_chunk = ""
        current_start = start_offset
        
        i = 0
        while i < len(sentences):
            sentence = sentences[i]
            # å¦‚æœæ˜¯æ ‡ç‚¹ç¬¦å·ï¼Œä¸å‰ä¸€å¥åˆå¹¶
            if i + 1 < len(sentences) and sentences[i + 1] in ['ã€‚', 'ï¼', 'ï¼Ÿ', '"', '"']:
                sentence += sentences[i + 1]
                i += 2
            else:
                i += 1
            
            if len(current_chunk) + len(sentence) > max_size and current_chunk:
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": current_chunk.strip(),
                    "start_pos": current_start,
                    "end_pos": current_start + len(current_chunk)
                })
                chunk_id += 1
                current_start += len(current_chunk)
                current_chunk = sentence
            else:
                current_chunk += sentence
        
        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append({
                "chunk_id": chunk_id,
                "text": current_chunk.strip(),
                "start_pos": current_start,
                "end_pos": current_start + len(current_chunk)
            })
        
        return chunks
    
    def _merge_chunk_results(self, chunk_results: List[Dict]) -> Dict:
        """ğŸ”— åˆå¹¶åˆ†å—åˆ†æç»“æœ"""
        merged_segments = []
        merged_characters = {}
        total_order = 0
        
        for chunk_result in chunk_results:
            chunk_id = chunk_result.get("chunk_id", 0)
            
            # åˆå¹¶segmentsï¼Œè°ƒæ•´order
            for segment in chunk_result.get("segments", []):
                segment["order"] = total_order
                segment["chunk_id"] = chunk_id  # æ ‡è®°æ¥æºå—
                merged_segments.append(segment)
                total_order += 1
            
            # åˆå¹¶charactersï¼ŒæŒ‰åå­—å»é‡
            for char in chunk_result.get("characters", []):
                char_name = char["name"]
                if char_name in merged_characters:
                    # åˆå¹¶é¢‘æ¬¡å’Œç½®ä¿¡åº¦
                    existing = merged_characters[char_name]
                    existing["frequency"] += char.get("frequency", 1)
                    
                    # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è·å–confidenceå­—æ®µ
                    char_confidence = char.get("confidence", 0.5)
                    if char_confidence is None:
                        # å°è¯•ä»character_traitä¸­è·å–
                        char_trait = char.get("character_trait", {})
                        char_confidence = char_trait.get("confidence", 0.5)
                    
                    existing_confidence = existing.get("confidence", 0.5)
                    if existing_confidence is None:
                        existing_trait = existing.get("character_trait", {})
                        existing_confidence = existing_trait.get("confidence", 0.5)
                    
                    # è®¾ç½®æ›´é«˜çš„ç½®ä¿¡åº¦
                    if "character_trait" in existing:
                        existing["character_trait"]["confidence"] = max(existing_confidence, char_confidence)
                    else:
                        existing["confidence"] = max(existing_confidence, char_confidence)
                    
                    # ä¿ç•™æ›´è¯¦ç»†çš„æè¿°
                    char_desc = char.get("personality_description", "")
                    existing_desc = existing.get("personality_description", "")
                    if len(char_desc) > len(existing_desc):
                        existing["personality_description"] = char_desc
                        # åŒæ—¶æ›´æ–°recommended_configä¸­çš„æè¿°
                        if "recommended_config" in existing:
                            existing["recommended_config"]["personality_description"] = char_desc
                else:
                    merged_characters[char_name] = char
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰é¢‘æ¬¡æ’åº
        characters_list = list(merged_characters.values())
        characters_list.sort(key=lambda x: x.get("frequency", 0), reverse=True)
        
        logger.info(f"åˆ†å—ç»“æœåˆå¹¶å®Œæˆï¼š{len(merged_segments)}ä¸ªæ®µè½ï¼Œ{len(characters_list)}ä¸ªè§’è‰²")
        
        return {
            "segments": merged_segments,
            "characters": characters_list
        }
    
    async def analyze_text(self, text: str, chapter_info: dict) -> dict:
        """ä½¿ç”¨Ollamaåˆ†ææ–‡æœ¬ä¸­çš„è§’è‰² - æ”¯æŒæ™ºèƒ½åˆ†å—å¤„ç†"""
        # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çš„WebSocketç®¡ç†å™¨
        try:
            from app.websocket.manager import websocket_manager
            
            async def send_analysis_progress(session_id, progress, message):
                await websocket_manager.publish_to_topic(
                    f"analysis_session_{session_id}",
                    {
                        "type": "progress_update",
                        "data": {
                            "progress": progress,
                            "message": message,
                            "session_id": session_id
                        }
                    }
                )
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰ä¸€ä¸ªmockå‡½æ•°
            async def send_analysis_progress(session_id, progress, message):
                logger.info(f"[è¿›åº¦ {progress}%] {message}")
            logger.warning("æ— æ³•å¯¼å…¥websocket_managerï¼Œä½¿ç”¨æ—¥å¿—è®°å½•è¿›åº¦")
        
        start_time = time.time()
        session_id = chapter_info.get('session_id', chapter_info['chapter_id'])
        
        try:
            # å‘é€å¼€å§‹åˆ†æè¿›åº¦
            await send_analysis_progress(session_id, 10, f"å¼€å§‹åˆ†æç« èŠ‚: {chapter_info['chapter_title']}")
            
            # ğŸš€ æ™ºèƒ½åˆ†å—å¤„ç†ï¼šé•¿æ–‡æœ¬è‡ªåŠ¨åˆ†å—
            text_length = len(text)
            
            # ğŸ¯ æ™ºèƒ½æ¨¡å‹é€‰æ‹©
            self.model_name = self._select_optimal_model(text)
            
            # ğŸ¯ ç®€åŒ–åˆ†å—ç­–ç•¥ï¼šæé«˜é˜ˆå€¼å‡å°‘åˆ†å—
            if self.model_name == "qwen2.5:14b":
                chunk_threshold = 6000  # 14Bæ¨¡å‹ï¼š6000å­—ç¬¦å¯ç”¨åˆ†å—
                max_chunk_size = 4000   # 14Bæ¨¡å‹ï¼šæ¯å—4000å­—ç¬¦
            else:
                chunk_threshold = 8000  # 7Bæ¨¡å‹ï¼š8000å­—ç¬¦å¯ç”¨åˆ†å—  
                max_chunk_size = 5000   # 7Bæ¨¡å‹ï¼šæ¯å—5000å­—ç¬¦
            
            if text_length > chunk_threshold:
                logger.info(f"æ–‡æœ¬é•¿åº¦{text_length}å­—ç¬¦ï¼Œå¯ç”¨æ™ºèƒ½åˆ†å—å¤„ç†({self.model_name})")
                await send_analysis_progress(session_id, 20, f"æ–‡æœ¬è¾ƒé•¿({text_length}å­—ç¬¦)ï¼Œå¯ç”¨æ™ºèƒ½åˆ†å—å¤„ç†...")
                
                # æ™ºèƒ½åˆ†å—
                chunks = self._smart_chunk_text(text, max_chunk_size=max_chunk_size)
                await send_analysis_progress(session_id, 30, f"å·²åˆ†ä¸º{len(chunks)}ä¸ªå—ï¼Œå¼€å§‹é€å—åˆ†æ...")
                
                # é€å—åˆ†æ
                chunk_results = []
                progress_step = 50 / len(chunks)  # 50%çš„è¿›åº¦ç”¨äºåˆ†å—åˆ†æ
                
                for i, chunk in enumerate(chunks):
                    chunk_progress = 30 + int(i * progress_step)
                    await send_analysis_progress(session_id, chunk_progress, f"åˆ†æç¬¬{i+1}/{len(chunks)}å—...")
                    
                    chunk_result = await self._analyze_single_chunk(chunk["text"], chunk["chunk_id"])
                    chunk_result["chunk_id"] = chunk["chunk_id"]
                    chunk_results.append(chunk_result)
                
                await send_analysis_progress(session_id, 80, "æ­£åœ¨åˆå¹¶åˆ†å—åˆ†æç»“æœ...")
                
                # åˆå¹¶åˆ†å—ç»“æœ
                result = self._merge_chunk_results(chunk_results)
                
                # å®Œæ•´æ€§æ ¡éªŒï¼ˆåŸºäºåŸæ–‡ï¼‰
                completeness_valid = self._validate_completeness(text, result['segments'])
                
                analysis_method = f"ollama_ai_chunked_{len(chunks)}_blocks"
                
            else:
                logger.info(f"æ–‡æœ¬é•¿åº¦{text_length}å­—ç¬¦ï¼Œä½¿ç”¨å•æ¬¡åˆ†æ")
                await send_analysis_progress(session_id, 30, "æ­£åœ¨è°ƒç”¨AIæ¨¡å‹è¿›è¡Œè§’è‰²è¯†åˆ«...")
                
                # ç›´æ¥å•æ¬¡åˆ†æ
                result = await self._analyze_single_text(text)
                completeness_valid = self._validate_completeness(text, result['segments'])
                analysis_method = "ollama_ai_single"
                chunks = []  # å•æ¬¡åˆ†ææ—¶ä¸ºç©ºåˆ—è¡¨
            
            processing_time = time.time() - start_time
            
            await send_analysis_progress(session_id, 100, f"åˆ†æå®Œæˆï¼Œè¯†åˆ«åˆ°{len(result['characters'])}ä¸ªè§’è‰²")
            
            # æ™ºèƒ½åˆ†æé˜¶æ®µï¼šè¿”å›æ‰€æœ‰è¯†åˆ«åˆ°çš„è§’è‰²ï¼ˆä¸è¿‡æ»¤å·²å­˜åœ¨çš„ï¼‰
            all_characters = result['characters']
            
            # ğŸ”¥ ä¿®å¤ï¼šä¿ç•™å·²æœ‰çš„processing_statså­—æ®µï¼Œç‰¹åˆ«æ˜¯secondary_check_applied
            existing_stats = result.get('processing_stats', {})
            
            return {
                "chapter_id": chapter_info['chapter_id'],
                "chapter_title": chapter_info['chapter_title'],
                "chapter_number": chapter_info['chapter_number'],
                "detected_characters": all_characters,  # è¿”å›æ‰€æœ‰è§’è‰²
                "segments": result['segments'],
                "processing_stats": {
                    # ä¿ç•™å·²æœ‰çš„å­—æ®µ
                    **existing_stats,
                    # æ›´æ–°æˆ–æ·»åŠ æ–°çš„å­—æ®µ
                    "total_segments": len(result['segments']),
                    "dialogue_segments": len([s for s in result['segments'] if s['text_type'] == 'dialogue']),
                    "narration_segments": len([s for s in result['segments'] if s['text_type'] == 'narration']),
                    "characters_found": len(result['characters']),
                    "new_characters_found": len(result['characters']),
                    "analysis_method": analysis_method,
                    "processing_time": round(processing_time, 2),
                    "text_length": len(text),
                    "ai_model": self.model_name,
                    "completeness_validated": completeness_valid,  # ğŸ”¥ æ–°å¢ï¼šå®Œæ•´æ€§æ ¡éªŒç»“æœ
                    "chunk_count": len(chunks) if text_length > chunk_threshold else 1  # ğŸ”¥ æ–°å¢ï¼šåˆ†å—æ•°é‡
                }
            }
                
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ Ollamaè§’è‰²åˆ†æå¼‚å¸¸å¤±è´¥: {str(e)}")
            await send_analysis_progress(session_id, 0, f"AIåˆ†æå¤±è´¥: {str(e)}")
            raise Exception(f"Ollamaè§’è‰²åˆ†æå¤±è´¥: {str(e)}")
    
    async def _analyze_single_text(self, text: str) -> Dict:
        """å•æ¬¡åˆ†ææ–‡æœ¬ï¼ˆä¸åˆ†å—ï¼‰"""
        
        # ğŸ”¥ ä¿®å¤ï¼šåœ¨åˆ†æå‰é€‰æ‹©æœ€ä¼˜æ¨¡å‹
        self._select_optimal_model(text)
        
        max_retries = 3
        response = None
        
        for attempt in range(max_retries):
            try:
                prompt = self._build_comprehensive_analysis_prompt(text)
                response = self._call_ollama(prompt)
                
                if response:
                    break
                else:
                    logger.warning(f"ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ï¼ŒOllamaè¿”å›ç©ºå“åº”")
                    if attempt < max_retries - 1:
                        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    
            except Exception as e:
                logger.error(f"ç¬¬{attempt + 1}æ¬¡å°è¯•å¼‚å¸¸: {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                else:
                    raise e
        
        if response:
            # è§£æOllamaè¿”å›çš„å®Œæ•´ç»“æœ
            result = self._parse_comprehensive_response(response)
            
            # ğŸ”¥ ä¿®å¤ï¼šå¢åŠ å†…å®¹å®Œæ•´æ€§æ ¡éªŒå’Œé‡è¯•
            logger.info(f"ğŸ” è°ƒç”¨å®Œæ•´æ€§æ ¡éªŒå‰ - result keys: {list(result.keys())}")
            segments_list = result.get('segments', [])
            logger.info(f"ğŸ” è°ƒç”¨å®Œæ•´æ€§æ ¡éªŒå‰ - segmentsæ•°é‡: {len(segments_list)}")
            if len(segments_list) > 0:
                first_seg = segments_list[0]
                logger.info(f"ğŸ” ç¬¬ä¸€ä¸ªsegment: order={first_seg.get('order')}, text_len={len(first_seg.get('text', ''))}, speaker='{first_seg.get('speaker')}'")
            
            completeness_valid = self._validate_completeness(text, result['segments'])
            if not completeness_valid:
                logger.warning("å†…å®¹å®Œæ•´æ€§æ ¡éªŒå¤±è´¥ï¼Œå°è¯•é‡æ–°åˆ†æ")
                
                # å¦‚æœå®Œæ•´æ€§æ ¡éªŒå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ›´è¯¦ç»†çš„æç¤ºè¯é‡æ–°åˆ†æ
                detailed_prompt = self._build_detailed_analysis_prompt(text)
                retry_response = self._call_ollama(detailed_prompt)
                
                if retry_response:
                    retry_result = self._parse_comprehensive_response(retry_response)
                    retry_completeness = self._validate_completeness(text, retry_result['segments'])
                    
                    if retry_completeness:
                        result = retry_result
                        logger.info("é‡æ–°åˆ†ææˆåŠŸï¼Œå†…å®¹å®Œæ•´æ€§æ ¡éªŒé€šè¿‡")
                    else:
                        logger.warning("é‡æ–°åˆ†æä»æœªé€šè¿‡å®Œæ•´æ€§æ ¡éªŒï¼Œä½¿ç”¨åŸç»“æœå¹¶è®°å½•è­¦å‘Š")
            
            # ğŸš€ æ ¹æ®ç³»ç»Ÿè®¾ç½®å†³å®šæ˜¯å¦å¯ç”¨äºŒæ¬¡æ£€æŸ¥æœºåˆ¶
            if self.settings.get("enableSecondaryCheck", False):
                result = await self._secondary_check_analysis(text, result)
            else:
                self.logger.info("å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡äºŒæ¬¡æ£€æŸ¥æœºåˆ¶")
            
            return result
        else:
            # Ollamaè°ƒç”¨å¤±è´¥ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
            logger.error("âŒ Ollama APIè°ƒç”¨å¤±è´¥ï¼Œæ²¡æœ‰è¿”å›æœ‰æ•ˆå“åº”")
            raise Exception("Ollama APIè°ƒç”¨å¤±è´¥ï¼Œæ²¡æœ‰è¿”å›æœ‰æ•ˆå“åº”")
    
    async def _secondary_check_analysis(self, original_text: str, primary_result: Dict) -> Dict:
        """ğŸš€ äºŒæ¬¡æ£€æŸ¥æœºåˆ¶ï¼šé€šç”¨éªŒè¯å’Œä¿®å¤"""
        logger.info("ğŸ” æ‰§è¡Œé€šç”¨äºŒæ¬¡æ£€æŸ¥...")
        
        segments = primary_result['segments']
        
        # 1. åŸºç¡€å®Œæ•´æ€§éªŒè¯
        refined_segments = self._validate_and_fix_completeness(original_text, segments)
        
        # 2. é€šç”¨æ ¼å¼æ ‡å‡†åŒ–ï¼ˆç§»é™¤è¿‡åº¦å…·ä½“çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼‰
        refined_segments = self._universal_format_normalization(refined_segments)
        
        # 3. æ›´æ–°ç»“æœ
        primary_result['segments'] = refined_segments
        
        # ç¡®ä¿processing_statså­—æ®µå­˜åœ¨
        if 'processing_stats' not in primary_result:
            primary_result['processing_stats'] = {}
        
        primary_result['processing_stats']['total_segments'] = len(refined_segments)
        primary_result['processing_stats']['dialogue_segments'] = len([s for s in refined_segments if s['text_type'] == 'dialogue'])
        primary_result['processing_stats']['narration_segments'] = len([s for s in refined_segments if s['text_type'] == 'narration'])
        primary_result['processing_stats']['secondary_check_applied'] = True
        primary_result['processing_stats']['model_version'] = "qwen2.5:14b"
        
        logger.info(f"âœ… é€šç”¨äºŒæ¬¡æ£€æŸ¥å®Œæˆï¼Œæ®µè½æ•°: {len(refined_segments)}")
        return primary_result

    def _universal_format_normalization(self, segments: List[Dict]) -> List[Dict]:
        """é€šç”¨æ ¼å¼æ ‡å‡†åŒ–ï¼šåŸºäºè¯­ä¹‰è€Œéç‰¹å®šæ¨¡å¼"""
        normalized_segments = []
        
        for segment in segments:
            text = segment['text'].strip()
            speaker = segment['speaker'].strip()
            text_type = segment['text_type']
            
            # 1. æ ‡å‡†åŒ–è§’è‰²åç§°
            if speaker and speaker != 'æ—ç™½':
                # ç§»é™¤å¸¸è§çš„è¯´è¯æ ‡è®°
                speaker = speaker.replace('è¯´', '').replace('é“', '').replace('ï¼š', '').replace(':', '').strip()
            
            # 2. æ ‡å‡†åŒ–æ–‡æœ¬å†…å®¹
            if text:
                # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
                text = ' '.join(text.split())
                
                # ç¡®ä¿å¼•å·å†…å®¹è¢«æ­£ç¡®è¯†åˆ«ä¸ºå¯¹è¯
                if text.startswith('"') and text.endswith('"') and text_type == 'narration':
                    text = text[1:-1]  # ç§»é™¤å¼•å·
                    text_type = 'dialogue'
                elif text.startswith('"') and text.endswith('"') and text_type == 'dialogue':
                    text = text[1:-1]  # ç§»é™¤å¼•å·
            
            # 3. éªŒè¯speakerå’Œtext_typeçš„ä¸€è‡´æ€§
            if text_type == 'dialogue' and speaker == 'æ—ç™½':
                # å¯¹è¯ä¸åº”è¯¥æ˜¯æ—ç™½ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–è§’è‰²
                speaker = self._extract_speaker_from_context(text) or 'æœªçŸ¥è§’è‰²'
            elif text_type == 'narration' and speaker != 'æ—ç™½':
                # å™è¿°åº”è¯¥æ˜¯æ—ç™½
                speaker = 'æ—ç™½'
            
            # 4. è¿‡æ»¤ç©ºå†…å®¹
            if text:
                normalized_segments.append({
                    'order': segment['order'],
                    'text': text,
                    'speaker': speaker,
                    'confidence': segment.get('confidence', 0.9),
                    'detection_rule': segment.get('detection_rule', 'ai_analysis'),
                    'text_type': text_type
                })
        
        return normalized_segments

    def _extract_speaker_from_context(self, text: str) -> Optional[str]:
        """ä»ä¸Šä¸‹æ–‡ä¸­æå–è¯´è¯è€…ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç®€å•çš„è§’è‰²åæå–ï¼Œä¸ä½¿ç”¨å¤æ‚çš„æ­£åˆ™è¡¨è¾¾å¼
        import re
        
        # æŸ¥æ‰¾å¯èƒ½çš„è§’è‰²åï¼ˆ2-4ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
        potential_names = re.findall(r'[ä¸€-é¾¯]{2,4}', text)
        
        # è¿‡æ»¤æ‰å¸¸è§çš„éè§’è‰²è¯æ±‡
        excluded_words = ['è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'å“ªé‡Œ', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¯æ˜¯', 'ä½†æ˜¯', 'æ‰€ä»¥', 'å› ä¸º', 'å¦‚æœ', 'è™½ç„¶', 'ç„¶å', 'æ¥ç€', 'æœ€å', 'æ­¤æ—¶', 'æ­¤å', 'çªç„¶', 'å¿½ç„¶', 'æ˜¾ç„¶', 'æ˜æ˜¾', 'ä¼¼ä¹', 'å¥½åƒ', 'ä»¿ä½›', 'åŸæ¥', 'åªè§', 'åªå¬', 'åªè§‰', 'ä½†è§', 'ä½†å¬', 'å´è§', 'å´å¬', 'ä¾¿è§']
        
        for name in potential_names:
            if name not in excluded_words and len(name) >= 2:
                return name
        
        return None

    def _detect_and_refine_mixed_sentences(self, segments: List[Dict]) -> List[Dict]:
        """æ£€æµ‹å¹¶ç²¾ç»†åŒ–åˆ†å‰²æ··åˆå¥å­ - é€šç”¨ç‰ˆæœ¬"""
        import re
        
        # æ–°çš„é€šç”¨æ–¹æ³•ï¼šä¸ä½¿ç”¨ç‰¹å®šçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        # è€Œæ˜¯åŸºäºè¯­ä¹‰ç‰¹å¾æ¥åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†ç¦»
        
        logger.info("ğŸ” ä½¿ç”¨é€šç”¨æ··åˆå¥å­æ£€æµ‹...")
        
        refined_segments = []
        
        for segment in segments:
            text = segment['text'].strip()
            
            # é€šç”¨æ£€æµ‹åŸåˆ™ï¼šå¦‚æœä¸€ä¸ªsegmentåŒ…å«å¼•å·å’Œéå¼•å·å†…å®¹ï¼Œå¯èƒ½éœ€è¦åˆ†ç¦»
            # ä½†ä¸ä½¿ç”¨å¤æ‚çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œè€Œæ˜¯åŸºäºç®€å•çš„è¯­ä¹‰åˆ¤æ–­
            
            # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«å¼•å·
            has_quotes = '"' in text or '"' in text or "'" in text or "'" in text
            
            # 2. æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„åŠ¨ä½œæè¿°è¯æ±‡
            action_words = ['è¯´', 'é“', 'å«', 'å–Š', 'é—®', 'ç­”', 'å›å¤', 'è¡¨ç¤º', 'å‹’é©¬', 'è½¬èº«', 'èµ·èº«', 'åä¸‹', 'èµ°å‘', 'å¥”æ¥', 'ç¦»å¼€', 'åœä¸‹', 'ä¸¾æ‰‹', 'æ”¾ä¸‹', 'æŠ¬å¤´', 'ä½å¤´', 'ç‚¹å¤´', 'æ‘‡å¤´', 'çš±çœ‰', 'å¾®ç¬‘', 'å†·ç¬‘', 'å¹æ°”', 'å’³å—½', 'æ¸…å—“', 'å‘æ¥', 'æ¶ˆæ¯', 'éœ‡åŠ¨', 'æ‰“æ–­']
            
            has_action_words = any(word in text for word in action_words)
            
            # 3. å¦‚æœæ—¢åŒ…å«å¼•å·åˆåŒ…å«åŠ¨ä½œè¯æ±‡ï¼Œå¯èƒ½æ˜¯æ··åˆæ–‡æœ¬
            if has_quotes and has_action_words and len(text) > 10:
                logger.info(f"æ£€æµ‹åˆ°å¯èƒ½çš„æ··åˆæ–‡æœ¬: {text[:50]}...")
                
                # ä¸è¿›è¡Œå¤æ‚çš„åˆ†ç¦»å¤„ç†ï¼Œè€Œæ˜¯è®°å½•æ—¥å¿—å¹¶ä¿æŒåŸæ ·
                # è®©æ”¹è¿›çš„æç¤ºè¯åœ¨ç¬¬ä¸€æ¬¡åˆ†ææ—¶å¤„ç†è¿™ç§æƒ…å†µ
                logger.info("ä¾èµ–æ”¹è¿›çš„æç¤ºè¯å¤„ç†æ··åˆæ–‡æœ¬ï¼Œä¸è¿›è¡Œåå¤„ç†åˆ†ç¦»")
            
            # ä¿æŒåŸsegmentä¸å˜
            refined_segments.append(segment)
        
        logger.info(f"é€šç”¨æ··åˆå¥å­æ£€æµ‹å®Œæˆï¼Œä¿æŒ{len(refined_segments)}ä¸ªsegmentä¸å˜")
        return refined_segments

    def _basic_format_normalization(self, segments: List[Dict]) -> List[Dict]:
        """åŸºç¡€æ ¼å¼æ ‡å‡†åŒ– - ç®€åŒ–ç‰ˆï¼Œé€‚é…14Bæ¨¡å‹"""
        normalized_segments = []
        
        for segment in segments:
            text = segment['text'].strip()
            speaker = segment['speaker']
            
            # åŸºç¡€æ¸…ç†ï¼šå»é™¤å¤šä½™å¼•å·
            if text.startswith('"') and text.endswith('"'):
                text = text[1:-1]
            
            # ç¡®ä¿æ—ç™½æ ‡è®°æ­£ç¡®
            if text.startswith('æ—ç™½ï¼š'):
                text = text[3:].strip()
                speaker = 'æ—ç™½'
            
            # åŸºç¡€è§’è‰²åè§„èŒƒåŒ–
            if speaker and speaker != 'æ—ç™½':
                speaker = speaker.replace('ï¼š', '').replace(':', '').strip()
            
            segment['text'] = text
            segment['speaker'] = speaker
            normalized_segments.append(segment)
        
        return normalized_segments

    def _refine_grammar_structure(self, segments: List[Dict]) -> List[Dict]:
        """ç²¾ç»†åŒ–è¯­æ³•ç»“æ„åˆ†æ"""
        import re
        
        refined_segments = []
        
        for segment in segments:
            text = segment['text'].strip()
            
            # å¤„ç†å¼•å·å†…å®¹
            if '"' in text or '"' in text or "'" in text or "'" in text:
                # æ ‡å‡†åŒ–å¼•å·
                text = text.replace('"', '"').replace('"', '"').replace("'", '"').replace("'", '"')
                
                # å¦‚æœæ•´ä¸ªæ–‡æœ¬è¢«å¼•å·åŒ…å›´ï¼Œè¿™æ˜¯çº¯å¯¹è¯
                if text.startswith('"') and text.endswith('"') and text.count('"') == 2:
                    segment['text'] = text[1:-1]  # å»æ‰å¼•å·
                    segment['text_type'] = 'dialogue'
                elif text.startswith('"') and text.endswith('"') and text.count('"') > 2:
                    # å¤æ‚åµŒå¥—å¼•å·ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                    pass  # æš‚æ—¶ä¿æŒåŸæ ·
            
            # å¤„ç†æ—ç™½æ ‡è®°
            if text.startswith('æ—ç™½ï¼š') or text.startswith('æ—ç™½:'):
                segment['text'] = text[3:].strip()
                segment['speaker'] = 'æ—ç™½'
                segment['text_type'] = 'narration'
            
            # ä¿®æ­£é”™è¯¯åˆ†ç±»ï¼šæ£€æµ‹æ˜æ˜¾åº”è¯¥æ˜¯æ—ç™½çš„å†…å®¹
            if segment['text_type'] == 'dialogue' and segment['speaker'] != 'æ—ç™½':
                # æ£€æµ‹é€šç”¨æè¿°æ€§åŠ¨è¯å’Œè¯æ±‡æ¨¡å¼
                narration_indicators = [
                    # åŸºç¡€åŠ¨ä½œåŠ¨è¯
                    'èµ°', 'è·‘', 'ç«™', 'å', 'èºº', 'èµ·èº«', 'æŠ¬å¤´', 'ä½å¤´', 'è½¬èº«', 'å›å¤´',
                    'çœ‹', 'æœ›', 'ç›¯', 'ç§', 'ç„', 'ç…', 'è§‚å¯Ÿ', 'æ³¨è§†', 'å‡è§†',
                    'è¯´', 'è®²', 'é“', 'è¨€', 'è¯­', 'è¯', 'å£°', 'éŸ³', 'å“', 'å¬',
                    'æƒ³', 'æ€', 'å¿µ', 'å¿†', 'è®°', 'å¿˜', 'çŸ¥', 'è§‰', 'æ„Ÿ', 'å¯Ÿ',
                    'æ‹¿', 'å–', 'æŠ“', 'æ¡', 'æ”¾', 'æ‰”', 'é€’', 'äº¤', 'ç»™', 'é€',
                    # é€šç”¨è¿æ¥è¯å’Œå‰¯è¯
                    'ç„¶å', 'æ¥ç€', 'éšå³', 'ç«‹å³', 'é©¬ä¸Š', 'é¡¿æ—¶', 'çªç„¶', 'å¿½ç„¶',
                    'åªè§', 'åªå¬', 'åªè§‰', 'ä½†è§', 'ä½†å¬', 'å´è§', 'å´å¬', 'ä¾¿è§',
                    'åŸæ¥', 'æ˜¾ç„¶', 'æ˜æ˜¾', 'ä¼¼ä¹', 'å¥½åƒ', 'ä»¿ä½›', 'å¦‚åŒ', 'çŠ¹å¦‚',
                    # æè¿°æ€§è¯æ±‡
                    'å‘ç°', 'æ„è¯†åˆ°', 'å¯Ÿè§‰', 'æ„Ÿåˆ°', 'è§‰å¾—', 'è®¤ä¸º', 'ä»¥ä¸º', 'æ–™æƒ³'
                ]
                
                # å¦‚æœåŒ…å«å¤ªå¤šæè¿°æ€§è¯æ±‡ï¼Œå¯èƒ½æ˜¯æ—ç™½
                narration_count = sum(1 for indicator in narration_indicators if indicator in text)
                if narration_count >= 2:  # åŒ…å«2ä¸ªæˆ–æ›´å¤šæè¿°æ€§è¯æ±‡
                    segment['text_type'] = 'narration'
                    segment['speaker'] = 'æ—ç™½'
                    segment['confidence'] = 0.9
                    segment['detection_rule'] = 'secondary_check_narration_fix'
            
            refined_segments.append(segment)
        
        return refined_segments

    def _validate_and_fix_completeness(self, original_text: str, segments: List[Dict]) -> List[Dict]:
        """éªŒè¯å¹¶ä¿®å¤æ–‡æœ¬å®Œæ•´æ€§"""
        # åˆå¹¶æ‰€æœ‰åˆ†æ®µæ–‡æœ¬
        combined_text = ''.join([seg['text'] for seg in segments])
        
        # è®¡ç®—å®Œæ•´åº¦
        original_clean = ''.join(original_text.split())
        combined_clean = ''.join(combined_text.split())
        
        completeness_ratio = len(combined_clean) / len(original_clean) if original_clean else 0
        
        if completeness_ratio < 0.9:
            logger.warning(f"å®Œæ•´æ€§ä¸è¶³({completeness_ratio:.2%})ï¼Œå°è¯•ä¿®å¤...")
            
            # æŸ¥æ‰¾é—æ¼çš„æ–‡æœ¬éƒ¨åˆ†
            missing_parts = []
            original_lines = original_text.strip().split('\n')
            processed_lines = set()
            
            for segment in segments:
                # æ‰¾åˆ°åŸæ–‡ä¸­å¯¹åº”çš„è¡Œ
                for line in original_lines:
                    if line.strip() and line.strip() in segment['text']:
                        processed_lines.add(line.strip())
            
            # æ·»åŠ é—æ¼çš„è¡Œ
            for line in original_lines:
                if line.strip() and line.strip() not in processed_lines:
                    missing_parts.append(line.strip())
            
            # å°†é—æ¼éƒ¨åˆ†æ·»åŠ ä¸ºæ—ç™½
            for i, missing in enumerate(missing_parts):
                segments.append({
                    'order': len(segments) + i + 1,
                    'text': missing,
                    'speaker': 'æ—ç™½',
                    'confidence': 0.8,
                    'detection_rule': 'completeness_fix',
                    'text_type': 'narration'
                })
            
            logger.info(f"ä¿®å¤å®Œæˆï¼Œæ·»åŠ äº†{len(missing_parts)}ä¸ªé—æ¼æ®µè½")
        
        # é‡æ–°æ’åº
        segments.sort(key=lambda x: x['order'])
        for i, segment in enumerate(segments):
            segment['order'] = i + 1
        
        return segments

    async def _analyze_single_chunk(self, chunk_text: str, chunk_id: int) -> Dict:
        """åˆ†æå•ä¸ªåˆ†å—"""
        logger.info(f"å¼€å§‹åˆ†æç¬¬{chunk_id}å—ï¼Œé•¿åº¦{len(chunk_text)}å­—ç¬¦")
        
        try:
            prompt = self._build_comprehensive_analysis_prompt(chunk_text)
            response = self._call_ollama(prompt)
            
            if response:
                result = self._parse_comprehensive_response(response)
                
                # ğŸ”¥ æ ¹æ®ç³»ç»Ÿè®¾ç½®å†³å®šæ˜¯å¦å¯ç”¨åˆ†å—çš„äºŒæ¬¡æ£€æŸ¥
                if self.settings.get("enableSecondaryCheck", False):
                    result = await self._secondary_check_analysis(chunk_text, result)
                else:
                    self.logger.debug("å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡åˆ†å—äºŒæ¬¡æ£€æŸ¥")
                
                logger.info(f"ç¬¬{chunk_id}å—åˆ†æå®Œæˆï¼š{len(result.get('segments', []))}æ®µè½ï¼Œ{len(result.get('characters', []))}ä¸ªè§’è‰²")
                return result
            else:
                logger.warning(f"ç¬¬{chunk_id}å—åˆ†æå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                return {"segments": [], "characters": []}
                
        except Exception as e:
            logger.error(f"ç¬¬{chunk_id}å—åˆ†æå¼‚å¸¸: {str(e)}")
            return {"segments": [], "characters": []}
    
    def _validate_completeness(self, original_text: str, segments: List[Dict]) -> bool:
        """ğŸ”¥ æ–°å¢ï¼šæ ¡éªŒåˆ†æç»“æœçš„å®Œæ•´æ€§"""
        try:
            # ç»Ÿè®¡åŸæ–‡å­—æ•°ï¼ˆå»é™¤ç©ºæ ¼å’Œæ¢è¡Œï¼‰
            original_chars = len(original_text.replace(' ', '').replace('\n', '').replace('\r', ''))
            
            # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥segmentså‚æ•°
            logger.info(f"ğŸ” å®Œæ•´æ€§æ ¡éªŒ - segmentsæ•°é‡: {len(segments)}")
            if len(segments) > 0:
                logger.info(f"ğŸ” ç¬¬ä¸€ä¸ªsegmentç¤ºä¾‹: {segments[0]}")
                logger.info(f"ğŸ” æœ€åä¸€ä¸ªsegmentç¤ºä¾‹: {segments[-1]}")
            else:
                logger.error(f"âŒ segmentsä¸ºç©ºåˆ—è¡¨ï¼è¿™æ˜¯é—®é¢˜æ‰€åœ¨")
            
            # ç»Ÿè®¡segmentså­—æ•°ï¼ˆå»é™¤ç©ºæ ¼å’Œæ¢è¡Œï¼‰
            segment_chars = sum(len(seg.get('text', '').replace(' ', '').replace('\n', '').replace('\r', '')) for seg in segments)
            
            # ğŸ” è°ƒè¯•ï¼šè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            if segment_chars == 0 and len(segments) > 0:
                logger.warning(f"å¼‚å¸¸ï¼šæœ‰{len(segments)}ä¸ªsegmentsä½†æ€»å­—æ•°ä¸º0ï¼Œæ£€æŸ¥segmentså†…å®¹")
                for i, seg in enumerate(segments[:3]):  # åªæ£€æŸ¥å‰3ä¸ª
                    text = seg.get('text', '')
                    logger.warning(f"Segment {i}: text='{text}', length={len(text)}")
            
            # è®¡ç®—å®Œæ•´åº¦æ¯”ä¾‹
            completeness_ratio = segment_chars / original_chars if original_chars > 0 else 0
            
            logger.info(f"å†…å®¹å®Œæ•´æ€§æ ¡éªŒ: åŸæ–‡{original_chars}å­—ç¬¦ï¼Œåˆ†æç»“æœ{segment_chars}å­—ç¬¦ï¼Œå®Œæ•´åº¦{completeness_ratio:.2%}")
            
            # æ ¹æ®å¿«é€Ÿæ¨¡å¼è°ƒæ•´å®Œæ•´æ€§é˜ˆå€¼
            if self.settings.get("fastModeEnabled", True):
                min_ratio = 0.75  # å¿«é€Ÿæ¨¡å¼ï¼š75%ä»¥ä¸Šè®¤ä¸ºå¯æ¥å—
            else:
                min_ratio = 0.85  # æ ‡å‡†æ¨¡å¼ï¼š85%ä»¥ä¸Šè®¤ä¸ºå®Œæ•´
            
            if completeness_ratio < min_ratio:
                logger.warning(f"å†…å®¹å®Œæ•´æ€§æ ¡éªŒå¤±è´¥: å®Œæ•´åº¦ä»…{completeness_ratio:.2%}ï¼Œé˜ˆå€¼{min_ratio:.0%}")
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„æ–‡æœ¬é—æ¼ï¼ˆé€šè¿‡å…³é”®è¯æ£€æŸ¥ï¼‰
            original_keywords = self._extract_keywords(original_text)
            segment_text = ' '.join([seg.get('text', '') for seg in segments])
            segment_keywords = self._extract_keywords(segment_text)
            
            missing_keywords = original_keywords - segment_keywords
            missing_ratio = len(missing_keywords) / len(original_keywords) if original_keywords else 0
            
            # æ ¹æ®å¿«é€Ÿæ¨¡å¼è°ƒæ•´å…³é”®è¯ä¸¢å¤±å®¹å¿åº¦
            if self.settings.get("fastModeEnabled", True):
                max_missing = 0.3  # å¿«é€Ÿæ¨¡å¼ï¼šå…è®¸30%å…³é”®è¯ä¸¢å¤±
            else:
                max_missing = 0.2  # æ ‡å‡†æ¨¡å¼ï¼šå…è®¸20%å…³é”®è¯ä¸¢å¤±
            
            if missing_ratio > max_missing:
                logger.warning(f"å…³é”®è¯å®Œæ•´æ€§æ ¡éªŒå¤±è´¥: ä¸¢å¤±{missing_ratio:.1%}å…³é”®è¯ï¼Œé˜ˆå€¼{max_missing:.0%}")
                return False
            
            logger.info("å†…å®¹å®Œæ•´æ€§æ ¡éªŒé€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"å®Œæ•´æ€§æ ¡éªŒå¼‚å¸¸: {str(e)}")
            return False  # æ ¡éªŒå¼‚å¸¸æ—¶è®¤ä¸ºä¸å®Œæ•´ï¼Œè§¦å‘é‡è¯•
    
    def _extract_keywords(self, text: str) -> set:
        """æå–æ–‡æœ¬ä¸­çš„å…³é”®è¯ç”¨äºå®Œæ•´æ€§æ ¡éªŒ"""
        import re
        
        # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2-4ä¸ªå­—ç¬¦çš„è¯ï¼‰
        chinese_words = set(re.findall(r'[\u4e00-\u9fff]{2,4}', text))
        
        # æå–äººåã€åœ°åç­‰ä¸“æœ‰åè¯ï¼ˆé€šå¸¸åŒ…å«ç‰¹å®šå­—ç¬¦ï¼‰
        proper_nouns = set(re.findall(r'[\u4e00-\u9fff]*[ç‹æå¼ åˆ˜é™ˆæ¨é»„èµµå´å‘¨][\u4e00-\u9fff]*', text))
        
        # æå–å¼•å·å†…çš„å¯¹è¯å…³é”®è¯
        dialogue_keywords = set(re.findall(r'["""]([^"""]{2,10})["""]', text))
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯ï¼Œå–å‰50ä¸ªæœ€é‡è¦çš„
        all_keywords = chinese_words | proper_nouns | dialogue_keywords
        return set(list(all_keywords)[:50])  # é™åˆ¶å…³é”®è¯æ•°é‡ï¼Œæé«˜æ•ˆç‡
    
    def _build_detailed_analysis_prompt(self, text: str) -> str:
        """ğŸ”¥ æ–°å¢ï¼šæ„å»ºæ›´è¯¦ç»†çš„åˆ†ææç¤ºè¯ï¼Œç”¨äºé‡è¯•æ—¶ç¡®ä¿å®Œæ•´æ€§"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡å°è¯´æ–‡æœ¬åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹å°è¯´æ–‡æœ¬ï¼Œç¡®ä¿ä¸é—æ¼ä»»ä½•å†…å®¹ã€‚

**é‡è¦æé†’ï¼šå¿…é¡»åˆ†æå®Œæ•´çš„æ–‡æœ¬å†…å®¹ï¼Œæ¯ä¸ªå¥å­éƒ½è¦åŒ…å«åœ¨ç»“æœä¸­ï¼**

æ–‡æœ¬ï¼š
{text}

è¯¦ç»†åˆ†æè¦æ±‚ï¼š
1. **å®Œæ•´æ€§ç¬¬ä¸€**ï¼šç¡®ä¿æ¯ä¸ªå¥å­ã€æ¯ä¸ªæ®µè½éƒ½è¢«åˆ†æåˆ°
2. **é€å¥åˆ†æ®µ**ï¼šæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰æ ‡ç‚¹ç¬¦å·åˆ†æ®µ
3. **è§’è‰²è¯†åˆ«**ï¼šå‡†ç¡®è¯†åˆ«æ‰€æœ‰è¯´è¯çš„è§’è‰²
4. **å¯¹è¯åˆ†ç¦»**ï¼šå°†"æŸæŸè¯´ï¼š'å†…å®¹'"åˆ†ä¸ºä¸¤æ®µ

åˆ†æ®µç­–ç•¥ï¼š
- æ¯ä¸ªå®Œæ•´çš„å¥å­ä½œä¸ºä¸€ä¸ªsegment
- å¯¹è¯å‰çš„åŠ¨ä½œæè¿°ï¼ˆå¦‚"æ—æ¸Šè¯´ï¼š"ï¼‰å•ç‹¬æˆæ®µï¼Œæ ‡è®°ä¸ºæ—ç™½
- å¼•å·å†…çš„å¯¹è¯å†…å®¹å•ç‹¬æˆæ®µï¼Œæ ‡è®°ä¸ºç›¸åº”è§’è‰²
- å¿ƒç†æ´»åŠ¨æŒ‰åŒæ ·è§„åˆ™å¤„ç†

**ğŸµ å£°éŸ³æè¿°è§„åˆ™**ï¼š
- "å¨‡å–å£°å¸¦ç€æ€’æ„" â†’ æ—ç™½ï¼ˆæè¿°å£°éŸ³ç‰¹å¾ï¼‰
- "ç¬‘å£°ä¼ æ¥" â†’ æ—ç™½ï¼ˆæè¿°å£°éŸ³ç°è±¡ï¼‰
- "è¯éŸ³åˆšè½" â†’ æ—ç™½ï¼ˆæè¿°è¯´è¯çŠ¶æ€ï¼‰
- åªæœ‰å¼•å·å†…çš„å®é™…è¯è¯­æ‰æ˜¯è§’è‰²å¯¹è¯ï¼

**ğŸ“ é—´æ¥å¼•è¿°å¯¹è¯é€šç”¨è§„åˆ™**ï¼š
å½“é‡åˆ°"æŸæŸ[åŠ¨ä½œ]ï¼š'å†…å®¹'"æ ¼å¼æ—¶ï¼Œå¿…é¡»åˆ†ç¦»ï¼š
- åŠ¨ä½œæè¿°éƒ¨åˆ† â†’ æ—ç™½
- å¼•è¿°å†…å®¹éƒ¨åˆ† â†’ ç›¸åº”è§’è‰²
- é€‚ç”¨äºï¼šè¯´é“ã€å†™é“ã€ä¸‹æ—¨ã€ä¼ è¯ã€å‘ŠçŸ¥ã€å‘½ä»¤ã€è¯¢é—®ç­‰æ‰€æœ‰å¼•è¿°å½¢å¼

**ğŸ—¨ï¸ è‡ªè¨€è‡ªè¯­å’Œå¿ƒç†æ´»åŠ¨ç‰¹æ®Šå¤„ç†**ï¼š
- "è‡ªè¨€è‡ªè¯­é“ï¼š"ã€"æš—è‡ªæƒ³é“ï¼š"ã€"å¿ƒä¸­å¿µå¨ï¼š"ç­‰æè¿°æ˜¯æ—ç™½
- å¼•å·å†…çš„å®é™…å†…å®¹æ‰æ˜¯è§’è‰²çš„è¯è¯­æˆ–å¿ƒç†æ´»åŠ¨
- ç¤ºä¾‹ï¼š"ç™½éª¨ç²¾è‡ªè¨€è‡ªè¯­é“ï¼š'é€ åŒ–ï¼'" â†’ åˆ†ä¸ºä¸¤æ®µï¼š
  * "ç™½éª¨ç²¾è‡ªè¨€è‡ªè¯­é“ï¼š" â†’ æ—ç™½
  * "é€ åŒ–ï¼" â†’ ç™½éª¨ç²¾

è¾“å‡ºè¦æ±‚ï¼š
- å¿…é¡»åŒ…å«åŸæ–‡çš„æ¯ä¸ªå­—ç¬¦ï¼ˆé™¤äº†æ ‡ç‚¹ç¬¦å·çš„è°ƒæ•´ï¼‰
- segmentæ•°é‡åº”è¯¥ä¸åŸæ–‡å¥å­æ•°é‡åŸºæœ¬å¯¹åº”
- ä¸èƒ½è·³è¿‡ä»»ä½•å†…å®¹æ®µè½

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
  "segments": [
    {{"order": 1, "text": "å®Œæ•´çš„å¥å­å†…å®¹", "speaker": "è¯´è¯è€…", "text_type": "dialogue/narration/inner_monologue", "confidence": 0.9}}
  ],
  "characters": [
    {{"name": "è§’è‰²å", "frequency": å‡ºç°æ¬¡æ•°, "gender": "male/female/neutral", "personality": "calm/brave/gentle", "personality_description": "æ€§æ ¼æè¿°", "is_main_character": true/false, "confidence": 0.8}}
  ]
}}

**å†æ¬¡å¼ºè°ƒï¼šä¸èƒ½é—æ¼ä»»ä½•æ–‡æœ¬å†…å®¹ï¼æ¯ä¸ªå¥å­éƒ½å¿…é¡»åœ¨segmentsä¸­ä½“ç°ï¼**"""
        
        return prompt

    def _detect_novel_type(self, text: str) -> str:
        """ğŸ†• æ£€æµ‹å°è¯´ç±»å‹ï¼Œä¸ºåç»­åˆ†ææä¾›ä¸Šä¸‹æ–‡"""
        # å–æ–‡æœ¬å‰1000å­—ç¬¦è¿›è¡Œç±»å‹åˆ†æ
        sample_text = text[:1000] if len(text) > 1000 else text
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡å°è¯´ç±»å‹è¯†åˆ«ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å°è¯´æ–‡æœ¬ç‰‡æ®µï¼Œåˆ¤æ–­å…¶æ‰€å±ç±»å‹ã€‚

æ–‡æœ¬ç‰‡æ®µï¼š
{sample_text}

è¯·æ ¹æ®ä»¥ä¸‹ç‰¹å¾åˆ¤æ–­å°è¯´ç±»å‹ï¼š

**å¤ä»£/å¤è£…å°è¯´**ï¼š
- æ—¶ä»£èƒŒæ™¯ï¼šå¤ä»£ä¸­å›½ã€æœä»£ã€çš‡å¸ã€å®˜å‘˜
- è¯­è¨€ç‰¹è‰²ï¼šæ–‡è¨€æ–‡è‰²å½©ã€å¤ä»£ç§°è°“
- å…³é”®è¯ï¼šé™›ä¸‹ã€çš‡ä¸Šã€å…¬ä¸»ã€ç‹çˆ·ã€å¤§äººã€è‡£ã€å¥´å©¢ã€åºœé‚¸ã€æœå»·

**ç°ä»£/éƒ½å¸‚å°è¯´**ï¼š
- æ—¶ä»£èƒŒæ™¯ï¼šç°ä»£ç¤¾ä¼šã€åŸå¸‚ç”Ÿæ´»
- ç§‘æŠ€å…ƒç´ ï¼šæ‰‹æœºã€ç”µè„‘ã€ç½‘ç»œã€æ±½è½¦ã€å…¬å¸ã€åšç‰©é¦†
- å…³é”®è¯ï¼šè€æ¿ã€ç»ç†ã€åŒäº‹ã€æ‰‹æœºã€çŸ­ä¿¡ã€ç”µè¯ã€ç½‘ç»œã€å…¬å¸ã€åšç‰©é¦†ã€å¯¼å¸ˆ

**æ­¦ä¾ /ä»™ä¾ å°è¯´**ï¼š
- æ­¦åŠŸå…ƒç´ ï¼šå†…åŠ›ã€å‰‘æ³•ã€è½»åŠŸã€ä¿®ç‚¼
- æ±Ÿæ¹–èƒŒæ™¯ï¼šé—¨æ´¾ã€æŒé—¨ã€å¼Ÿå­ã€æ±Ÿæ¹–
- å…³é”®è¯ï¼šå¸ˆçˆ¶ã€æŒé—¨ã€å¼Ÿå­ã€å†…åŠ›ã€çœŸæ°”ã€å‰‘æ°”ã€é—¨æ´¾

**ç„å¹»/å¥‡å¹»å°è¯´**ï¼š
- é­”æ³•å…ƒç´ ï¼šæ³•æœ¯ã€é­”æ³•ã€å¼‚èƒ½ã€é­”å…½
- å¼‚ä¸–ç•Œï¼šå¤§é™†ã€ç‹å›½ã€é­”æ³•å¸ˆã€æˆ˜å£«
- å…³é”®è¯ï¼šé­”æ³•ã€æ³•æœ¯ã€é­”å…½ã€å¤§é™†ã€ç‹å›½ã€å¼‚èƒ½
- æ³¨æ„ï¼šå¦‚æœåŒæ—¶åŒ…å«ç°ä»£å…ƒç´ ï¼ˆæ‰‹æœºã€åšç‰©é¦†ã€å¯¼å¸ˆï¼‰ä¼˜å…ˆåˆ¤æ–­ä¸ºç°ä»£å°è¯´

**ç§‘å¹»å°è¯´**ï¼š
- ç§‘æŠ€å…ƒç´ ï¼šæœªæ¥ç§‘æŠ€ã€æœºå™¨äººã€å¤ªç©ºã€æ—¶é—´æ—…è¡Œ
- å…³é”®è¯ï¼šæœºå™¨äººã€å¤–æ˜Ÿäººã€å¤ªç©ºã€æœªæ¥ã€ç§‘æŠ€ã€å®éªŒå®¤

**å†›äº‹/å†å²å°è¯´**ï¼š
- å†›äº‹å…ƒç´ ï¼šæˆ˜äº‰ã€å†›é˜Ÿã€å°†å†›ã€å£«å…µ
- å…³é”®è¯ï¼šå°†å†›ã€å£«å…µã€æˆ˜äº‰ã€å†›é˜Ÿã€æˆ˜åœºã€ä½œæˆ˜

åªéœ€è¦è¾“å‡ºç±»å‹åç§°ï¼Œä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š
- ancientï¼ˆå¤ä»£/å¤è£…ï¼‰
- modernï¼ˆç°ä»£/éƒ½å¸‚ï¼‰
- wuxiaï¼ˆæ­¦ä¾ /ä»™ä¾ ï¼‰
- fantasyï¼ˆç„å¹»/å¥‡å¹»ï¼‰
- scifiï¼ˆç§‘å¹»ï¼‰
- militaryï¼ˆå†›äº‹/å†å²ï¼‰
- unknownï¼ˆæ— æ³•ç¡®å®šï¼‰

åªè¾“å‡ºç±»å‹åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š"""

        try:
            response = self._call_ollama(prompt)
            if response:
                # æå–ç±»å‹åç§°
                novel_type = response.strip().lower()
                if novel_type in ['ancient', 'modern', 'wuxia', 'fantasy', 'scifi', 'military']:
                    self.logger.info(f"æ£€æµ‹åˆ°å°è¯´ç±»å‹: {novel_type}")
                    return novel_type
                else:
                    self.logger.warning(f"æœªè¯†åˆ«çš„å°è¯´ç±»å‹: {novel_type}ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹")
                    return 'unknown'
            else:
                self.logger.error("å°è¯´ç±»å‹æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹")
                return 'unknown'
        except Exception as e:
            self.logger.error(f"å°è¯´ç±»å‹æ£€æµ‹å¼‚å¸¸: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹")
            return 'unknown'

    def _build_type_aware_analysis_prompt(self, text: str, novel_type: str) -> str:
        """ğŸ†• åŸºäºå°è¯´ç±»å‹æ„å»ºä¸“é—¨çš„åˆ†ææç¤ºè¯ - ç®€åŒ–ç‰ˆé€‚é…14Bæ¨¡å‹"""
        
        prompt = f"""ä½ æ˜¯ä¸­æ–‡å°è¯´æ–‡æœ¬åˆ†æä¸“å®¶ï¼Œä½¿ç”¨qwen2.5:14bæ¨¡å‹ã€‚è¯·åˆ†æä»¥ä¸‹å°è¯´æ–‡æœ¬ã€‚

æ–‡æœ¬ï¼š{text[:4000] if len(text) > 4000 else text}

æ ¸å¿ƒä»»åŠ¡ï¼š
1. æŒ‰å¥å­åˆ†æ®µï¼Œè¯†åˆ«æ¯æ®µçš„è¯´è¯è€…
2. åŒºåˆ†å¯¹è¯ã€æ—ç™½ã€å¿ƒç†ç‹¬ç™½
3. ä¿æŒè§’è‰²åç§°ä¸€è‡´æ€§

å…³é”®åŸåˆ™ï¼š
- å¼•å·å†…å®¹ = è§’è‰²å¯¹è¯
- æè¿°åŠ¨ä½œ = æ—ç™½
- "è§’è‰²è¯´ï¼š'è¯è¯­'" = åˆ†ä¸ºä¸¤æ®µï¼šåŠ¨ä½œ(æ—ç™½) + è¯è¯­(è§’è‰²)

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "segments": [
    {{"order": 1, "text": "æ–‡æœ¬å†…å®¹", "speaker": "è¯´è¯è€…", "text_type": "dialogue/narration/inner_monologue", "confidence": 0.9}}
  ],
  "characters": [
    {{"name": "è§’è‰²å", "frequency": å‡ºç°æ¬¡æ•°, "gender": "male/female/neutral", "personality": "calm/brave/gentle", "is_main_character": true/false, "confidence": 0.8}}
  ]
}}

åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
        
        return prompt

    def _build_comprehensive_analysis_prompt(self, text: str) -> str:
        """æ„å»ºå¹³è¡¡çš„åˆ†ææç¤ºè¯ - ç¡®ä¿å®Œæ•´æ€§"""
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ä¸­æ–‡å°è¯´æ–‡æœ¬åˆ†æä¸“å®¶ã€‚è¯·å®Œæ•´åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œç¡®ä¿ä¸é—æ¼ä»»ä½•å†…å®¹ã€‚

**âš ï¸ é‡è¦ï¼šå¿…é¡»å¤„ç†æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼Œä¸èƒ½é—æ¼ä»»ä½•å¥å­ï¼**

æ ¸å¿ƒä»»åŠ¡ï¼š
1. æŒ‰å¥å­åˆ†æ®µï¼Œè¯†åˆ«æ¯æ®µçš„è¯´è¯è€…
2. åˆ†ç¦»æ··åˆæ ¼å¼ï¼š"è§’è‰²è¯´ï¼š'å¯¹è¯'" â†’ ä¸¤æ®µï¼šåŠ¨ä½œ(æ—ç™½) + å¯¹è¯(è§’è‰²)  
3. å¼•å·å†…å®¹=è§’è‰²å¯¹è¯ï¼Œæè¿°åŠ¨ä½œ=æ—ç™½
4. ç¡®ä¿æ‰€æœ‰æ–‡æœ¬éƒ½åœ¨segmentsä¸­ä½“ç°

å…³é”®è§„åˆ™ï¼š
- æ¯ä¸ªå®Œæ•´å¥å­éƒ½è¦æˆä¸ºä¸€ä¸ªsegment
- "XXè¯´ï¼š"ç­‰åŠ¨ä½œæè¿° â†’ æ—ç™½
- å¼•å·å†…çš„å®é™…è¯è¯­ â†’ å¯¹åº”è§’è‰²
- çº¯æè¿°æ€§æ–‡å­— â†’ æ—ç™½

æ–‡æœ¬ï¼š
{text}

**å®Œæ•´æ€§è¦æ±‚ï¼š**
- segmentsæ€»å­—æ•°åº”æ¥è¿‘åŸæ–‡å­—æ•°
- æ¯ä¸ªå¥å­éƒ½å¿…é¡»åŒ…å«åœ¨æŸä¸ªsegmentä¸­
- ä¸èƒ½è·³è¿‡ä»»ä½•å†…å®¹æ®µè½

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "segments": [
    {{"order": 1, "text": "å®Œæ•´æ–‡æœ¬å†…å®¹", "speaker": "è¯´è¯è€…", "text_type": "dialogue/narration", "confidence": 0.9}}
  ],
  "characters": [
    {{"name": "è§’è‰²å", "frequency": å‡ºç°æ¬¡æ•°, "gender": "male/female/neutral", "is_main_character": true/false, "confidence": 0.8}}
  ]
}}

åªè¾“å‡ºJSONï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰æ–‡æœ¬å†…å®¹ã€‚"""
        
        return prompt

    def _call_ollama(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Ollama API"""
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": self._get_model_options()
            }
            
            timeout = self.settings.get("analysisTimeout", 60)
            response = requests.post(
                self.api_url,
                json=payload,
                timeout=timeout  # ä»ç³»ç»Ÿè®¾ç½®è¯»å–è¶…æ—¶æ—¶é—´
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                logger.error(f"Ollama APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return None
                
        except requests.exceptions.Timeout:
            logger.error("Ollama APIè°ƒç”¨è¶…æ—¶")
            return None
        except Exception as e:
            logger.error(f"Ollama APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return None
    
    def _parse_comprehensive_response(self, response: str) -> Dict:
        """è§£æOllamaè¿”å›çš„ç»¼åˆåˆ†æç»“æœ"""
        try:
            # æ£€æŸ¥responseæ˜¯å¦ä¸ºNoneæˆ–ç©º
            if not response or response.strip() == '':
                logger.error("Ollamaå“åº”ä¸ºç©ºæˆ–None")
                return {
                    'segments': [],
                    'detected_characters': [],
                    'analysis_summary': {'total_segments': 0, 'total_characters': 0}
                }
            
            # è®°å½•åŸå§‹å“åº”ç”¨äºè°ƒè¯•
            logger.info(f"OllamaåŸå§‹å“åº”: {response[:500]}...")
            
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                data = json.loads(json_str)
                
                logger.info(f"è§£æçš„JSONæ•°æ®: {json.dumps(data, ensure_ascii=False, indent=2)}")
                
                # å¤„ç†segments
                segments = []
                for i, seg_data in enumerate(data.get('segments', [])):
                    # æ”¯æŒæ–°çš„text_type: inner_monologue
                    text_type = seg_data.get('text_type', 'narration')
                    if text_type not in ['dialogue', 'narration', 'inner_monologue']:
                        text_type = 'narration'  # é»˜è®¤ä¸ºæ—ç™½
                        
                    # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†ç©ºçš„speakerå­—æ®µ
                    speaker = seg_data.get('speaker', '') or ''  # ç¡®ä¿ä¸æ˜¯None
                    if isinstance(speaker, str):
                        speaker = speaker.strip()
                    else:
                        speaker = ''
                    
                    if not speaker:  # å¤„ç†ç©ºå­—ç¬¦ä¸²ã€Noneã€æˆ–åªæœ‰ç©ºæ ¼çš„æƒ…å†µ
                        if text_type in ['narration', 'inner_monologue']:
                            speaker = 'æ—ç™½'
                        else:
                            speaker = 'æœªçŸ¥è§’è‰²'
                    
                    segments.append({
                        'order': seg_data.get('order', i + 1),
                        'text': seg_data.get('text', ''),
                        'speaker': speaker,
                        'confidence': seg_data.get('confidence', 0.8),
                        'detection_rule': 'ollama_ai',
                        'text_type': text_type
                    })
                
                # å¤„ç†characters
                characters = []
                for char_data in data.get('characters', []):
                    if isinstance(char_data, dict) and 'name' in char_data:
                        name = char_data.get('name', '')
                        if name and len(name) >= 2:
                            characters.append({
                                'name': name,
                                'frequency': char_data.get('frequency', 1),
                                'character_trait': {
                                    'trait': char_data.get('personality', 'calm'),
                                    'confidence': char_data.get('confidence', 0.8),
                                    'description': char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ')
                                },
                                'first_appearance': 1,
                                'is_main_character': char_data.get('is_main_character', False),
                                'recommended_config': {
                                    'gender': self._infer_gender_smart(name, char_data.get('gender', 'unknown')),
                                    'personality': char_data.get('personality', 'calm'),
                                    'personality_description': char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ'),
                                    'personality_confidence': char_data.get('confidence', 0.8),
                                    'description': f"{name}ï¼Œ{self._infer_gender_smart(name, char_data.get('gender', 'unknown'))}è§’è‰²ï¼Œ{char_data.get('personality_description', 'æ€§æ ¼ç‰¹å¾å¾…åˆ†æ')}ï¼Œåœ¨æ–‡æœ¬ä¸­å‡ºç°{char_data.get('frequency', 1)}æ¬¡ã€‚",
                                    'recommended_tts_params': self._get_tts_params(char_data.get('personality', 'calm')),
                                    'voice_type': f"{self._infer_gender_smart(name, char_data.get('gender', 'unknown'))}_{char_data.get('personality', 'calm')}",
                                    'color': self._get_character_color(char_data.get('personality', 'calm'))
                                }
                            })
                
                result = {
                    'segments': segments,
                    'characters': characters
                }
                
                # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥è§£æç»“æœ
                logger.debug(f"è§£æå®Œæˆ - segmentsæ•°é‡: {len(segments)}")
                logger.debug(f"è§£æå®Œæˆ - charactersæ•°é‡: {len(characters)}")
                if len(segments) > 0:
                    logger.debug(f"è§£æå®Œæˆ - ç¬¬ä¸€ä¸ªsegment: {segments[0]}")
                
                return result
            
            else:
                logger.error("æ— æ³•ä»Ollamaå“åº”ä¸­æå–JSONæ•°æ®")
                return {'segments': [], 'characters': []}
                
        except json.JSONDecodeError as e:
            logger.error(f"è§£æOllama JSONå“åº”å¤±è´¥: {str(e)}")
            logger.error(f"åŸå§‹å“åº”: {response}")
            return {'segments': [], 'characters': []}
        except Exception as e:
            logger.error(f"å¤„ç†Ollamaå“åº”å¼‚å¸¸: {str(e)}")
            return {'segments': [], 'characters': []}

    def _get_tts_params(self, personality: str) -> Dict:
        """æ ¹æ®æ€§æ ¼è·å–TTSå‚æ•°"""
        params_map = {
            'gentle': {'time_step': 35, 'p_w': 1.2, 't_w': 2.8},
            'fierce': {'time_step': 28, 'p_w': 1.6, 't_w': 3.2},
            'calm': {'time_step': 32, 'p_w': 1.4, 't_w': 3.0},
            'lively': {'time_step': 30, 'p_w': 1.3, 't_w': 2.9},
            'wise': {'time_step': 34, 'p_w': 1.3, 't_w': 3.1},
            'brave': {'time_step': 29, 'p_w': 1.5, 't_w': 3.1}
        }
        return params_map.get(personality, {'time_step': 32, 'p_w': 1.4, 't_w': 3.0})
    
    def _get_character_color(self, personality: str) -> str:
        """æ ¹æ®æ€§æ ¼è·å–è§’è‰²é¢œè‰²"""
        color_map = {
            'gentle': '#FFB6C1',  # æµ…ç²‰è‰²
            'fierce': '#FF6347',  # ç•ªèŒ„çº¢
            'calm': '#06b6d4',   # é’è‰²
            'lively': '#32CD32', # ç»¿è‰²
            'wise': '#9370DB',   # ç´«è‰²
            'brave': '#FF8C00'   # æ©™è‰²
        }
        return color_map.get(personality, '#06b6d4')
    
    def _infer_gender_smart(self, name: str, ai_gender: str) -> str:
        """æ™ºèƒ½æ¨æ–­è§’è‰²æ€§åˆ« - å®Œå…¨ä¾èµ–AIåˆ¤æ–­ï¼Œç§»é™¤ç¡¬ç¼–ç """
        # å¦‚æœAIå·²ç»æ­£ç¡®è¯†åˆ«äº†æ€§åˆ«ï¼Œç›´æ¥ä½¿ç”¨
        if ai_gender and ai_gender in ['male', 'female', 'neutral']:
            return ai_gender
        
        # å¦‚æœAIæ²¡æœ‰è¿”å›æ€§åˆ«ä¿¡æ¯ï¼Œè°ƒç”¨ä¸“é—¨çš„æ€§åˆ«è¯†åˆ«AI
        try:
            gender = self._ai_infer_gender(name)
            if gender in ['male', 'female', 'neutral']:
                logger.info(f"AIæ¨æ–­è§’è‰² '{name}' æ€§åˆ«: {gender}")
                return gender
        except Exception as e:
            logger.warning(f"AIæ€§åˆ«æ¨æ–­å¤±è´¥: {str(e)}")
        
        # é»˜è®¤è¿”å›unknownï¼Œè®©ç”¨æˆ·æ‰‹åŠ¨é€‰æ‹©
        logger.warning(f"æ— æ³•æ¨æ–­è§’è‰² '{name}' çš„æ€§åˆ«")
        return 'unknown'
    
    def _ai_infer_gender(self, character_name: str) -> str:
        """ä½¿ç”¨AIæ¨æ–­è§’è‰²æ€§åˆ«"""
        try:
            prompt = f"""è¯·åˆ¤æ–­è§’è‰² "{character_name}" çš„æ€§åˆ«ã€‚

åˆ¤æ–­è§„åˆ™ï¼š
1. åŸºäºä¸­æ–‡å§“åçš„å¸¸è§ç‰¹å¾
2. åŸºäºæ–‡å­¦ä½œå“ä¸­çš„è§’è‰²è®¾å®š
3. åŸºäºç§°è°“ã€å¤´è¡”çš„è¯­ä¹‰å«ä¹‰

è¿”å›æ ¼å¼ï¼ˆåªè¿”å›ä¸€ä¸ªè¯ï¼‰ï¼š
- maleï¼ˆç”·æ€§ï¼‰
- femaleï¼ˆå¥³æ€§ï¼‰  
- neutralï¼ˆä¸­æ€§ï¼Œå¦‚æ—ç™½ã€å™è¿°è€…ï¼‰

è§’è‰²åï¼š{character_name}
æ€§åˆ«ï¼š"""

            response = self._call_ollama(prompt)
            if response:
                # æå–æ€§åˆ«åˆ¤æ–­
                gender = response.strip().lower()
                if 'male' in gender and 'female' not in gender:
                    return 'male'
                elif 'female' in gender:
                    return 'female'
                elif 'neutral' in gender:
                    return 'neutral'
            
            return 'unknown'
            
        except Exception as e:
            logger.error(f"AIæ€§åˆ«æ¨æ–­å¼‚å¸¸: {str(e)}")
            return 'unknown' 