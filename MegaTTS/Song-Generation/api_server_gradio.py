#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºå®˜æ–¹Gradioé€»è¾‘çš„SongGeneration FastAPIæœåŠ¡å™¨
"""

import os
import sys
import time
import uuid
import json
import yaml
import re
import torch
import torchaudio
from datetime import datetime
from typing import Optional, Dict, Any
from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.responses import FileResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append('codeclm/tokenizer/')
sys.path.append('codeclm/tokenizer/Flow1dVAE/')

# ç›´æ¥å¯¼å…¥LeVoInferenceçš„å®ç°
import torch
import numpy as np
from omegaconf import OmegaConf
from codeclm.trainer.codec_song_pl import CodecLM_PL
from codeclm.models import CodecLM
from codeclm.models import builders
from separator import Separator

class LeVoInference(torch.nn.Module):
    def __init__(self, ckpt_path):
        super().__init__()
        torch.backends.cudnn.enabled = False 
        OmegaConf.register_new_resolver("eval", lambda x: eval(x))
        OmegaConf.register_new_resolver("concat", lambda *x: [xxx for xx in x for xxx in xx])
        OmegaConf.register_new_resolver("get_fname", lambda: 'default')
        OmegaConf.register_new_resolver("load_yaml", lambda x: list(OmegaConf.load(x)))

        cfg_path = os.path.join(ckpt_path, 'config.yaml')
        self.pt_path = os.path.join(ckpt_path, 'model.pt')
        self.cfg = OmegaConf.load(cfg_path)
        self.cfg.mode = 'inference'
        self.max_duration = self.cfg.max_dur
        self.default_params = dict(
            top_p = 0.0,
            record_tokens = True,
            record_window = 50,
            extend_stride = 5,
            duration = self.max_duration,
        )

    def forward(self, lyric: str, description: str = None, prompt_audio_path: os.PathLike = None, genre: str = None, auto_prompt_path: os.PathLike = None, params = dict()):
        if prompt_audio_path is not None and os.path.exists(prompt_audio_path):
            separator = Separator()
            audio_tokenizer = builders.get_audio_tokenizer_model(self.cfg.audio_tokenizer_checkpoint, self.cfg)
            audio_tokenizer = audio_tokenizer.eval().cuda()
            seperate_tokenizer = builders.get_audio_tokenizer_model(self.cfg.audio_tokenizer_checkpoint_sep, self.cfg)
            seperate_tokenizer = seperate_tokenizer.eval().cuda()
            pmt_wav, vocal_wav, bgm_wav = separator.run(prompt_audio_path)
            pmt_wav = pmt_wav.cuda()
            vocal_wav = vocal_wav.cuda()
            bgm_wav = bgm_wav.cuda()
            pmt_wav, _ = audio_tokenizer.encode(pmt_wav)
            vocal_wav, bgm_wav = seperate_tokenizer.encode(vocal_wav, bgm_wav)
            melody_is_wav = False
            del audio_tokenizer
            del seperate_tokenizer
            del separator
        elif genre is not None and auto_prompt_path is not None:
            auto_prompt = torch.load(auto_prompt_path)
            merge_prompt = [item for sublist in auto_prompt.values() for item in sublist]
            if genre == "Auto": 
                prompt_token = merge_prompt[np.random.randint(0, len(merge_prompt))]
            else:
                prompt_token = auto_prompt[genre][np.random.randint(0, len(auto_prompt[genre]))]
            pmt_wav = prompt_token[:,[0],:]
            vocal_wav = prompt_token[:,[1],:]
            bgm_wav = prompt_token[:,[2],:]
            melody_is_wav = False
        else:
            pmt_wav = None
            vocal_wav = None
            bgm_wav = None
            melody_is_wav = True

        model_light = CodecLM_PL(self.cfg, self.pt_path)
        model_light = model_light.eval()
        model_light.audiolm.cfg = self.cfg
        model = CodecLM(name = "tmp",
            lm = model_light.audiolm,
            audiotokenizer = None,
            max_duration = self.max_duration,
            seperate_tokenizer = None,
        )
        del model_light
        model.lm = model.lm.cuda().to(torch.float16)
        params = {**self.default_params, **params}
        model.set_generation_params(**params)

        generate_inp = {
            'lyrics': [lyric.replace("  ", " ")],
            'descriptions': [description],
            'melody_wavs': pmt_wav,
            'vocal_wavs': vocal_wav,
            'bgm_wavs': bgm_wav,
            'melody_is_wav': melody_is_wav,
        }

        with torch.autocast(device_type="cuda", dtype=torch.float16):
            tokens = model.generate(**generate_inp, return_tokens=True)

        del model
        torch.cuda.empty_cache()

        seperate_tokenizer = builders.get_audio_tokenizer_model(self.cfg.audio_tokenizer_checkpoint_sep, self.cfg)
        seperate_tokenizer = seperate_tokenizer.eval().cuda()
        model = CodecLM(name = "tmp",
            lm = None,
            audiotokenizer = None,
            max_duration = self.max_duration,
            seperate_tokenizer = seperate_tokenizer,
        )

        if tokens.shape[-1] > 3000:
            tokens = tokens[..., :3000]
            
        with torch.no_grad():
            if melody_is_wav:
                wav_seperate = model.generate_audio(tokens, pmt_wav, vocal_wav, bgm_wav)
            else:
                wav_seperate = model.generate_audio(tokens)

        del seperate_tokenizer
        del model
        torch.cuda.empty_cache()
        return wav_seperate[0]

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['USER'] = 'root'
os.environ['PYTHONDONTWRITEBYTECODE'] = '1'
os.environ['TRANSFORMERS_CACHE'] = f"{os.getcwd()}/third_party/hub"
os.environ['NCCL_HOME'] = '/usr/local/tccl'
os.environ['PYTHONPATH'] = f"{os.getcwd()}/codeclm/tokenizer/:{os.getcwd()}:{os.getcwd()}/codeclm/tokenizer/Flow1dVAE/:{os.getcwd()}/codeclm/tokenizer/:{os.environ.get('PYTHONPATH', '')}"

# FastAPIåº”ç”¨
app = FastAPI(
    title="SongGeneration API Server", 
    description="åŸºäºå®˜æ–¹Gradioé€»è¾‘çš„éŸ³ä¹ç”ŸæˆAPIæœåŠ¡",
    version="2.0.0"
)

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
app.mount("/static", StaticFiles(directory="templates"), name="static")

# å…¨å±€å˜é‡
MODEL = None
STRUCTS = None
AUTO_PROMPT = None

# æ”¯æŒçš„éŸ³ä¹é£æ ¼
SUPPORTED_GENRES = ["Pop", "R&B", "Dance", "Jazz", "Folk", "Rock", "Chinese Style", "Chinese Tradition", "Metal", "Reggae", "Chinese Opera", "Auto"]

# é»˜è®¤æ­Œè¯ç¤ºä¾‹
EXAMPLE_LYRICS = """
[intro-short]

[verse]
å¤œæ™šçš„è¡—ç¯é—ªçƒ
æˆ‘æ¼«æ­¥åœ¨ç†Ÿæ‚‰çš„è§’è½
å›å¿†åƒæ½®æ°´èˆ¬æ¶Œæ¥
ä½ çš„ç¬‘å®¹å¦‚æ­¤æ¸…æ™°

[chorus]
å›å¿†çš„æ¸©åº¦è¿˜åœ¨
ä½ å´å·²ä¸åœ¨
æˆ‘çš„å¿ƒè¢«çˆ±å¡«æ»¡
å´åˆè¢«æ€å¿µåˆºç—›

[outro-short]
""".strip()

class SongRequest(BaseModel):
    """æ­Œæ›²ç”Ÿæˆè¯·æ±‚"""
    lyrics: str
    description: Optional[str] = None
    genre: Optional[str] = None
    cfg_coef: Optional[float] = 1.5
    temperature: Optional[float] = 0.9
    top_k: Optional[int] = 50

class SongResponse(BaseModel):
    """æ­Œæ›²ç”Ÿæˆå“åº”"""
    success: bool
    message: str
    file_id: Optional[str] = None
    file_path: Optional[str] = None
    generation_time: Optional[float] = None
    sample_rate: Optional[int] = None
    input_config: Optional[Dict[str, Any]] = None

def format_lyrics(lyric: str) -> tuple[str, Optional[str]]:
    """æ ¼å¼åŒ–æ­Œè¯ï¼Œè¿”å›(formatted_lyric, error_message)"""
    global STRUCTS
    
    # æ ¼å¼åŒ–ç»“æ„æ ‡ç­¾
    lyric = lyric.replace("[intro]", "[intro-short]").replace("[inst]", "[inst-short]").replace("[outro]", "[outro-short]")
    paragraphs = [p.strip() for p in lyric.strip().split('\n\n') if p.strip()]
    
    if len(paragraphs) < 1:
        return None, "æ­Œè¯ä¸èƒ½ä¸ºç©º"
    
    paragraphs_norm = []
    vocal_flag = False
    vocal_structs = ['[verse]', '[chorus]', '[bridge]']
    
    for para in paragraphs:
        lines = para.splitlines()
        struct_tag = lines[0].strip().lower()
        
        if struct_tag not in STRUCTS:
            return None, f"æ®µè½å¿…é¡»ä»¥ç»“æ„æ ‡ç­¾å¼€å§‹ï¼Œæ”¯æŒçš„æ ‡ç­¾: {list(STRUCTS.keys())}"
        
        if struct_tag in vocal_structs:
            vocal_flag = True
            if len(lines) < 2 or not [line.strip() for line in lines[1:] if line.strip()]:
                return None, f"ä»¥ä¸‹æ®µè½éœ€è¦æ­Œè¯: {vocal_structs}"
            else:
                new_para_list = []
                for line in lines[1:]:
                    new_para_list.append(re.sub(r"[^\w\s\[\]\-\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af\u00c0-\u017f]", "", line))
                new_para_str = f"{struct_tag} {'.'.join(new_para_list)}"
        else:
            if len(lines) > 1:
                return None, f"ä»¥ä¸‹æ®µè½ä¸åº”åŒ…å«æ­Œè¯: [intro], [intro-short], [intro-medium], [inst], [inst-short], [inst-medium], [outro], [outro-short], [outro-medium]"
            else:
                new_para_str = struct_tag
        paragraphs_norm.append(new_para_str)
    
    if not vocal_flag:
        return None, f"æ­Œè¯å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªäººå£°æ®µè½: {vocal_structs}"
    
    lyric_norm = " ; ".join(paragraphs_norm)
    return lyric_norm, None

def initialize_model(ckpt_path: str):
    """åˆå§‹åŒ–æ¨¡å‹"""
    global MODEL, STRUCTS, AUTO_PROMPT
    
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–SongGenerationæ¨¡å‹...")
    
    # åˆå§‹åŒ–LeVoInference
    MODEL = LeVoInference(ckpt_path)
    
    # åŠ è½½ç»“æ„é…ç½®
    with open('conf/vocab.yaml', 'r', encoding='utf-8') as file:
        STRUCTS = yaml.safe_load(file)
    
    # åŠ è½½è‡ªåŠ¨æç¤º
    AUTO_PROMPT = torch.load('ckpt/ckpt/prompt.pt')
    
    print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ!")

async def cleanup_file(file_path: str, delay: int = 0):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    if delay > 0:
        import asyncio
        await asyncio.sleep(delay)
    
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"ğŸ—‘ï¸ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†æ–‡ä»¶å¤±è´¥: {e}")

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–æ¨¡å‹"""
    ckpt_path = sys.argv[1] if len(sys.argv) > 1 else "ckpt/songgeneration_base"
    initialize_model(ckpt_path)

@app.get("/")
async def root():
    """APIæ ¹è·¯å¾„"""
    return {
        "message": "SongGeneration API Server (Gradioç‰ˆæœ¬)", 
        "status": "running", 
        "version": "2.0.0",
        "based_on": "Official Gradio Implementation",
        "demo_page": "/demo",
        "api_docs": "/docs"
    }

@app.get("/demo", response_class=HTMLResponse)
async def demo_page():
    """Demoæ¼”ç¤ºé¡µé¢"""
    try:
        with open("templates/demo.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="Demoé¡µé¢æœªæ‰¾åˆ°")

@app.get("/ping")
async def ping():
    """ç®€å•å­˜æ´»æ£€æŸ¥"""
    return {"status": "pong", "timestamp": time.time()}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    model_status = MODEL is not None
    
    # æ£€æŸ¥GPUçŠ¶æ€
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "available": True,
            "device_count": torch.cuda.device_count(),
            "current_device": torch.cuda.current_device(),
            "device_name": torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else "Unknown",
            "memory_allocated": f"{torch.cuda.memory_allocated(0) / 1024**3:.2f} GB" if torch.cuda.device_count() > 0 else "0 GB",
            "memory_reserved": f"{torch.cuda.memory_reserved(0) / 1024**3:.2f} GB" if torch.cuda.device_count() > 0 else "0 GB"
        }
    else:
        gpu_info = {"available": False}
    
    # æ•´ä½“çŠ¶æ€
    overall_status = "healthy" if model_status and torch.cuda.is_available() else "degraded"
    
    return {
        "status": overall_status,
        "timestamp": time.time(),
        "model": {
            "loaded": model_status,
            "ready": model_status,
            "implementation": "Official LeVoInference"
        },
        "gpu": gpu_info,
        "api_version": "2.0.0",
        "endpoints": {
            "generate": "/generate",
            "download": "/download/{file_id}",
            "supported_genres": "/supported_genres",
            "example_lyrics": "/example_lyrics"
        }
    }

@app.get("/supported_genres")
async def get_supported_genres():
    """è·å–æ”¯æŒçš„éŸ³ä¹é£æ ¼"""
    return {"genres": SUPPORTED_GENRES}

@app.get("/example_lyrics")
async def get_example_lyrics():
    """è·å–ç¤ºä¾‹æ­Œè¯"""
    return {"example_lyrics": EXAMPLE_LYRICS, "structs": STRUCTS if STRUCTS else []}

@app.post("/generate", response_model=SongResponse)
async def generate_song(request: SongRequest, background_tasks: BackgroundTasks):
    """ç”Ÿæˆæ­Œæ›²"""
    if MODEL is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåˆå§‹åŒ–")
    
    try:
        start_time = time.time()
        
        # éªŒè¯è¾“å…¥
        if not request.lyrics.strip():
            raise HTTPException(status_code=400, detail="æ­Œè¯ä¸èƒ½ä¸ºç©º")
        
        if request.genre and request.genre not in SUPPORTED_GENRES:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„éŸ³ä¹é£æ ¼: {request.genre}")
        
        # æ ¼å¼åŒ–æ­Œè¯
        lyric_norm, error_msg = format_lyrics(request.lyrics)
        if error_msg:
            raise HTTPException(status_code=400, detail=error_msg)
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶ID
        file_id = str(uuid.uuid4())
        output_dir = "output/api_generated"
        os.makedirs(output_dir, exist_ok=True)
        target_wav_path = f"{output_dir}/{file_id}.flac"
        
        # å‡†å¤‡ç”Ÿæˆå‚æ•°
        params = {
            'cfg_coef': request.cfg_coef,
            'temperature': request.temperature,
            'top_k': request.top_k
        }
        params = {k: v for k, v in params.items() if v is not None}
        
        print(f"ğŸµ å¼€å§‹ç”Ÿæˆæ­Œæ›² (ID: {file_id})...")
        print(f"ğŸ“ æ­Œè¯: {lyric_norm[:100]}...")
        
        # è°ƒç”¨å®˜æ–¹æ¨ç†å¼•æ“
        audio_data = MODEL(
            lyric=lyric_norm,
            description=request.description,
            prompt_audio_path=None,
            genre=request.genre,
            auto_prompt_path='ckpt/ckpt/prompt.pt',
            params=params
        )
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        sample_rate = MODEL.cfg.sample_rate
        
        # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯æ­£ç¡®çš„2Dæ ¼å¼ [channels, samples]
        audio_tensor = audio_data.cpu()
        print(f"ğŸ” åŸå§‹éŸ³é¢‘shape: {audio_tensor.shape}")
        
        # å¤„ç†ä¸åŒçš„éŸ³é¢‘å¼ é‡æ ¼å¼
        if audio_tensor.dim() == 1:
            # 1D -> 2D [1, samples]
            audio_tensor = audio_tensor.unsqueeze(0)
        elif audio_tensor.dim() == 3:
            # 3D -> 2D, å‡è®¾æ ¼å¼ä¸º [batch, channels, samples]
            audio_tensor = audio_tensor.squeeze(0)
        elif audio_tensor.dim() > 3:
            # é«˜ç»´åº¦ï¼Œå–ç¬¬ä¸€ä¸ªbatchå’Œå‰ä¸¤ä¸ªç»´åº¦
            audio_tensor = audio_tensor[0]
            if audio_tensor.dim() > 2:
                audio_tensor = audio_tensor.view(audio_tensor.shape[0], -1)
        
        print(f"ğŸ” å¤„ç†åéŸ³é¢‘shape: {audio_tensor.shape}")
        
        # ç¡®ä¿æ˜¯2Då¼ é‡
        if audio_tensor.dim() != 2:
            # æœ€åçš„ä¿é™©æªæ–½ï¼šflattenç„¶åreshape
            audio_tensor = audio_tensor.flatten().unsqueeze(0)
            print(f"ğŸ” æœ€ç»ˆéŸ³é¢‘shape: {audio_tensor.shape}")
        
        torchaudio.save(target_wav_path, audio_tensor, sample_rate)
        
        generation_time = time.time() - start_time
        print(f"âœ… æ­Œæ›²ç”Ÿæˆå®Œæˆ (ID: {file_id}), è€—æ—¶: {generation_time:.2f}ç§’")
        
        # åˆ›å»ºè¾“å…¥é…ç½®è®°å½•
        input_config = {
            "lyric": lyric_norm,
            "genre": request.genre,
            "description": request.description,
            "params": params,
            "inference_duration": generation_time,
            "timestamp": datetime.now().isoformat(),
        }
        
        # æ·»åŠ åå°ä»»åŠ¡æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆ1å°æ—¶åï¼‰
        background_tasks.add_task(cleanup_file, target_wav_path, delay=3600)
        
        return SongResponse(
            success=True,
            message="æ­Œæ›²ç”ŸæˆæˆåŠŸ",
            file_id=file_id,
            file_path=target_wav_path,
            generation_time=generation_time,
            sample_rate=sample_rate,
            input_config=input_config
        )
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ­Œæ›²æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.get("/download/{file_id}")
async def download_song(file_id: str):
    """ä¸‹è½½ç”Ÿæˆçš„æ­Œæ›²æ–‡ä»¶"""
    file_path = f"output/api_generated/{file_id}.flac"
    
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")
    
    return FileResponse(
        path=file_path,
        media_type="audio/flac",
        filename=f"song_{file_id}.flac"
    )

if __name__ == "__main__":
    print("ğŸµ SongGeneration API Server (Gradioç‰ˆæœ¬) å¯åŠ¨ä¸­...")
    print("ğŸ“ æ¨¡å‹è·¯å¾„:", sys.argv[1] if len(sys.argv) > 1 else "ckpt/songgeneration_base")
    print("ğŸŒ ç«¯å£:", sys.argv[2] if len(sys.argv) > 2 else "7862")
    
    port = int(sys.argv[2]) if len(sys.argv) > 2 else 7862
    uvicorn.run(app, host="0.0.0.0", port=port) 